{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/common/users/ppk31/CS543_DL_Proj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from pytorch_model_summary import summary\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "from utils import (load_data, save_img_results, load_from_checkpoint)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### set cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"using GPU: {torch.cuda.is_available()}\")\n",
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print(f\"GPU ids: {gpus}\")\n",
    "\n",
    "# torch.random.seed()\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.set_device(gpus[1])\n",
    "cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(text_encoder):\n",
    "    print(f\"using {text_encoder} as text encoder\")\n",
    "    if text_encoder == \"distilbert-base-uncased\":\n",
    "        return DistilBertTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "        return CLIPTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"text-cnn-rnn\":\n",
    "        with open(\"train_test_split/text-cnn-rnn/train_embeddings.pkl\", \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, pretrained=True):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if text_encoder == \"distilbert-base-uncased\":\n",
    "            self.encoder = DistilBertModel.from_pretrained(text_encoder)\n",
    "        elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            self.encoder = CLIPModel.from_pretrained(text_encoder)\n",
    "        self.retrieve_token_index = 0\n",
    "    \n",
    "    def forward(self, input_tokens, attention_mask):\n",
    "        if self.text_encoder == \"distilbert-base-uncased\":\n",
    "            out = self.encoder(input_ids = input_tokens, attention_mask = attention_mask)\n",
    "            last_hidden_states = out.last_hidden_state\n",
    "            embeddings = last_hidden_states[:, self.retrieve_token_index, :]    # output_dimensions = 768\n",
    "        elif self.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            embeddings = self.encoder.get_text_features(input_ids = input_tokens, attention_mask = attention_mask) # output_dimensions = 512\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmented Projection Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmented_Projection(nn.Module):\n",
    "    def __init__(self, stage, gen_channels, gen_dim):\n",
    "        super(Augmented_Projection, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.t_dim = config.text_dim\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.z_dim = config.z_dim\n",
    "        self.gen_in = gen_channels #config.generator_dim * gen_dim\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stage == 1:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(self.c_dim + self.z_dim, self.gen_in * gen_dim * gen_dim, bias=False), # bias=False, # 768 -> 192*8*8*8\n",
    "                nn.BatchNorm1d(self.gen_in * gen_dim * gen_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def augment(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        eps = Variable(torch.randn(std.size()).float().cuda())\n",
    "        return mu + (std * eps)\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        if noise is None and self.stage==1:\n",
    "            noise = torch.randn((text_embedding.shape[0], self.z_dim)).float().cuda()\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        c_code = self.augment(mu, logvar)\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            c_code = torch.cat((c_code, noise), dim=1)\n",
    "            c_code = self.project(c_code)\n",
    "        \n",
    "        return c_code, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling and Upsampling Block for Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, kernel_size=4, stride=2, padding=1, batch_norm=True, activation=True, use_conv=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, stride=1, padding=1, batch_norm=True, activation=True, bias=False, use_deconv=False, dropout=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.use_deconv = use_deconv\n",
    "\n",
    "        if use_deconv:\n",
    "            self.deconv = nn.ConvTranspose2d(self.channels, self.out_channels, kernel_size=4, stride=2, padding=padding, bias=bias) # use when not using interpolate\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=stride, padding=padding, bias = bias)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.ReLU()\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.use_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Layer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param in_channels: the number of input channels.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        stride = 1,\n",
    "        padding = 1\n",
    "    ):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        if in_channels == out_channels:\n",
    "                self.x_residual = nn.Identity()\n",
    "        else:\n",
    "            self.x_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.x_residual(x)\n",
    "        h = x + g\n",
    "        return self.relu(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-I Generator Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims # 4\n",
    "        self.in_channels = config.generator_dim * 8 # 192*8\n",
    "        self.channel_mul = config.channel_mul\n",
    "        self.num_resblocks = config.n_resblocks\n",
    "        self.use_deconv=config.use_deconv \n",
    "        self.dropout=config.dropout\n",
    "        ch = self.in_channels\n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding, noise)\n",
    "        x = proj_x.view(-1, self.in_channels, self.in_dims, self.in_dims)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stage-II Generator Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims * config.in_dims # 16\n",
    "        self.in_channels = config.generator_dim # 192\n",
    "        self.channel_mul = config.channel_mul_stage2\n",
    "        self.num_resblocks = config.n_resblocks_stage2\n",
    "        self.use_deconv=config.use_deconv2 \n",
    "        self.dropout=config.dropout2\n",
    "        ch = self.in_channels * 4 \n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "        \n",
    "        self.downblocks= nn.Sequential(\n",
    "            Downsample(3, self.in_channels, kernel_size=3, stride=1, padding=1, batch_norm=False),\n",
    "            Downsample(self.in_channels, self.in_channels*2),\n",
    "            Downsample(self.in_channels*2, self.in_channels*4)\n",
    "        )\n",
    "        self.combined = nn.Sequential(\n",
    "            Downsample(self.in_channels*4 + self.c_dim, self.in_channels*4, kernel_size=3, stride=1, padding=1) # 768 x 16 x 16\n",
    "        )\n",
    "            \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout if layer<2 else False))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_embedding, stage1_out):\n",
    "        enc_img = self.downblocks(stage1_out)\n",
    "        \n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding)\n",
    "        x = proj_x.view(-1, self.c_dim, 1, 1)\n",
    "        x = x.repeat(1, 1, self.in_dims, self.in_dims)\n",
    "        x = torch.cat([enc_img, x], dim=1)\n",
    "        x = self.combined(x)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve text embeddings for given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(prompt, tokenizer, encoder):\n",
    "    # captions_dict = {'input_ids': [list of captions vector], 'attention_mask': [list of attention_mask]}\n",
    "    captions_dict = tokenizer(prompt, padding='max_length', truncation=True, max_length=77, return_tensors=\"pt\")\n",
    "    text_encoder = TextEncoder(encoder, pretrained=True)\n",
    "    text_encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(captions_dict['input_ids'], captions_dict['attention_mask'])\n",
    "    return [text_embeddings.squeeze(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments to the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Generate Text to Image arguments\")\n",
    "parser.add_argument('--text_encoder', required=True, type=str, default=None, help=\"Which text encoder to use, distilbert-base-uncased or openai/clip-vit-base-patch32 or text-cnn-rnn\")\n",
    "parser.add_argument('--g1', type=str, required=True, default=None, help=\"Generator 1 Path\")\n",
    "parser.add_argument('--g2', type=str, required=True, default=None, help=\"Generator 2 Path\")\n",
    "parser.add_argument('--prompt', type=str, default=\"this is a large dark grey bird with a large beak.\")\n",
    "parser.add_argument('--n_images', type=int, default=1, help=\"number of images to generate for given prompt, max_allowed=6\")\n",
    "parser.add_argument('--test_dataset', action=\"store_true\", help=\"in case of text-cnn-rnn, provide this flag\")\n",
    "parser.add_argument('--out_path', type=str, default=\"valid_results\", help=\"in case of text-cnn-rnn, provide this flag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if using char-CNN-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-cnn-rnn\n",
    "args = parser.parse_args(['--text_encoder', 'text-cnn-rnn',\n",
    "                          '--g1', 'text-cnn-rnn/out_I/checkpoint_s1_ls/netG1_epoch_400.pth',\n",
    "                          '--g2', 'text-cnn-rnn/out_I/checkpoint_s2_ls/netG2_epoch_80.pth',\n",
    "                          '--test_dataset'])\n",
    "os.makedirs(args.out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "args = parser.parse_args(['--text_encoder', 'distilbert-base-uncased',\n",
    "                          '--g1', 'bert/bert_out_V/checkpoint_s1_bert_ls/netG1_epoch_500.pth',\n",
    "                          '--g2', 'bert/bert_out_V/checkpoint_s2_bert_ls/netG2_epoch_100.pth',\n",
    "                          '--prompt', 'the bird has a small orange bill that has a black tip.'])\n",
    "os.makedirs(args.out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### if using CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip\n",
    "args = parser.parse_args(['--text_encoder', 'openai/clip-vit-base-patch32',\n",
    "                          '--g1', 'clip/clip_out_V/checkpoint_s1_clip_ls/netG1_epoch_460.pth',\n",
    "                          '--g2', 'clip/clip_out_V/checkpoint_s2_clip_onels_1/netG2_epoch_80.pth',\n",
    "                          '--prompt', 'the bird has a small orange bill that has a black tip.'])\n",
    "os.makedirs(args.out_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load text encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.text_encoder == \"distilbert-base-uncased\":\n",
    "    from configs import config\n",
    "    from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "elif args.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "    from configs import config2 as config\n",
    "    from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor\n",
    "else:\n",
    "    from configs import config3 as config\n",
    "    with open(\"train_test_split/text-cnn-rnn/test_captions.pkl\", 'rb') as f:\n",
    "        captions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(args.text_encoder)\n",
    "\n",
    "prompt = None\n",
    "text_embeddings = []\n",
    "\n",
    "# get text_embeddings\n",
    "if args.text_encoder == \"text-cnn-rnn\":\n",
    "    caption_idx = np.random.randint(0, len(captions), args.n_images)\n",
    "    prompt = []\n",
    "    for i, index in enumerate(caption_idx):\n",
    "        caption_list = captions[index]\n",
    "        idx = random.randint(0, len(caption_list)-1)\n",
    "        caption = caption_list[idx]\n",
    "        embedding_list = tokenizer[index]\n",
    "        embedding = embedding_list[idx]\n",
    "        prompt.append(caption)\n",
    "        text_embeddings.append(torch.tensor(embedding))\n",
    "else:\n",
    "    prompt = [args.prompt] * args.n_images\n",
    "    text_embeddings = get_text_embeddings(args.prompt, tokenizer, args.text_encoder)\n",
    "    text_embeddings = text_embeddings * args.n_images\n",
    "\n",
    "text_embeddings = torch.stack(text_embeddings, dim=0).float().cuda()\n",
    "print(text_embeddings.shape)\n",
    "assert args.n_images == text_embeddings.size(0), f\"No. of text embeddings: {text_embeddings.size(0)} different from number of images: {args.n_images} to be generated\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load Stage-I and Stage-II Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen1 = load_from_checkpoint(Generator1(stage=1), args.g1)\n",
    "gen2 = load_from_checkpoint(Generator2(stage=2), args.g2)\n",
    "gen1.float().cuda()\n",
    "gen2.float().cuda()\n",
    "gen1.eval()\n",
    "gen2.eval()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise = torch.randn(args.n_images, 100).float().cuda()\n",
    "    low_res, _, _ = gen1(text_embeddings, noise)\n",
    "    out, _, _ = gen2(text_embeddings, low_res)\n",
    "\n",
    "low_res = low_res.cpu().data\n",
    "out = out.cpu().data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_counter = len(os.listdir(args.out_path))\n",
    "vutils.save_image(low_res, '%s/generated_sample_s1_%03d.png' % (args.out_path, next_counter), normalize=True)\n",
    "vutils.save_image(out, '%s/generated_sample_s2_%03d.png' % (args.out_path, next_counter), normalize=True)\n",
    "print(f\"Image saved at: {args.out_path}, counter: {next_counter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### open generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen2_image = Image.open('%s/generated_sample_s2_%03d.png' % (args.out_path, next_counter))\n",
    "gen2_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen1_image = Image.open('%s/generated_sample_s1_%03d.png' % (args.out_path, next_counter))\n",
    "gen1_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba9bc282ea7dd8acf6b93a88ab047ea17bba2d98cff2c21ca6cffa26ac4d8f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
