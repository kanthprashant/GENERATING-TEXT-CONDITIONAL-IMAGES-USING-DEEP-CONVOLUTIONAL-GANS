{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/common/users/ppk31/CS543_DL_Proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from configs import config\n",
    "from pytorch_model_summary import summary\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import (weights_init, make_train_test_split, load_data, compute_discriminator_loss, \n",
    "                   compute_generator_loss, KL_loss, L1_loss, save_img_results, save_model, load_from_checkpoint)\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from dataset import Text2ImgDataset, Text2ImgDataset_reformed\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DATASET = '/freespace/local/ppk31_cs543/Project/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.text_encoder == \"distilbert-base-uncased\":\n",
    "    from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "elif config.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "    from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: True\n",
      "GPU ids: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "print(f\"using GPU: {torch.cuda.is_available()}\")\n",
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print(f\"GPU ids: {gpus}\")\n",
    "\n",
    "torch.random.seed()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.set_device(gpus[0])\n",
    "cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(text_encoder):\n",
    "    print(f\"using {text_encoder} as text encoder\")\n",
    "    if text_encoder == \"distilbert-base-uncased\":\n",
    "        return DistilBertTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "        return CLIPTokenizer.from_pretrained(text_encoder)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, pretrained=True):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if text_encoder == \"distilbert-base-uncased\":\n",
    "            self.encoder = DistilBertModel.from_pretrained(text_encoder)\n",
    "        elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            self.encoder = CLIPModel.from_pretrained(text_encoder)\n",
    "        # self.text_embedding = 768\n",
    "        # self.projection = ProjectionHead('text_projector', self.text_embedding, project_dim)\n",
    "        self.retrieve_token_index = 0\n",
    "    \n",
    "    def forward(self, input_tokens, attention_mask):\n",
    "        if self.text_encoder == \"distilbert-base-uncased\":\n",
    "            out = self.encoder(input_ids = input_tokens, attention_mask = attention_mask)\n",
    "            last_hidden_states = out.last_hidden_state\n",
    "            embeddings = last_hidden_states[:, self.retrieve_token_index, :]    # output_dimensions = 768\n",
    "        elif self.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            embeddings = self.encoder.get_text_features(input_ids = input_tokens, attention_mask = attention_mask) # output_dimensions = 512\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmented_Projection(nn.Module):\n",
    "    def __init__(self, stage, gen_channels, gen_dim):\n",
    "        super(Augmented_Projection, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.t_dim = config.text_dim\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.z_dim = config.z_dim\n",
    "        self.gen_in = gen_channels #config.generator_dim * gen_dim\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stage == 1:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(self.c_dim + self.z_dim, self.gen_in * gen_dim * gen_dim, bias=False), # bias=False, # 768 -> 192*8*8*8\n",
    "                nn.BatchNorm1d(self.gen_in * gen_dim * gen_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def augment(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        # if config.cuda_is_available:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_().cuda()\n",
    "        # else:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(torch.randn(std.size()).float().cuda())\n",
    "        # eps = Variable(eps)\n",
    "        # eps.mul(std).add(mu)\n",
    "        return mu + (std * eps)\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        if noise is None and self.stage==1:\n",
    "            noise = torch.randn((text_embedding.shape[0], self.z_dim)).float().cuda()\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        c_code = self.augment(mu, logvar)\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            c_code = torch.cat((c_code, noise), dim=1)\n",
    "            c_code = self.project(c_code)\n",
    "        \n",
    "        return c_code, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, kernel_size=4, stride=2, padding=1, batch_norm=True, activation=True, use_conv=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, stride=1, padding=1, batch_norm=True, activation=True, bias=False, use_deconv=False, dropout=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.use_deconv = use_deconv\n",
    "\n",
    "        if use_deconv:\n",
    "            self.deconv = nn.ConvTranspose2d(self.channels, self.out_channels, kernel_size=4, stride=2, padding=padding, bias=bias) # use when not using interpolate\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=stride, padding=padding, bias = bias)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.ReLU()\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.use_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param in_channels: the number of input channels.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        stride = 1,\n",
    "        padding = 1\n",
    "    ):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        if in_channels == out_channels:\n",
    "                self.x_residual = nn.Identity()\n",
    "        else:\n",
    "            self.x_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.x_residual(x)\n",
    "        h = x + g\n",
    "        return self.relu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    :param channels: is the number of channels in the feature map\n",
    "    :param n_heads: is the number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, n_heads=1, cond_channels=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        assert (\n",
    "            channels % n_heads == 0\n",
    "        ), f\"q,k,v channels {channels//n_heads} cannot be constructed for {n_heads} heads, input channels: {channels}\"\n",
    "        self.n_heads = n_heads\n",
    "        # self.norm1 = nn.GroupNorm(num_groups=16, num_channels=channels, eps=1e-6, affine=True) # num_groups=32\n",
    "        # self.norm1 = nn.BatchNorm2d(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels*3, kernel_size=1)\n",
    "        self.attention = QKVAttention(self.n_heads)\n",
    "        if cond_channels is not None:\n",
    "            # self.norm2 = nn.GroupNorm(num_groups=16, num_channels=cond_channels, eps=1e-6, affine=True) # num_groups=32\n",
    "            # self.norm2 = nn.BatchNorm2d(cond_channels)\n",
    "            self.cond_kv = nn.Conv2d(cond_channels, channels*2, kernel_size=1)\n",
    "        # self.proj_out = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "    def forward(self, x, cond_out = None):\n",
    "        b, c, *spatial = x.shape\n",
    "        h, w = spatial\n",
    "        # qkv = self.qkv(self.norm1(x).view(b, c, -1)) # b, c*3, h*w\n",
    "        qkv = self.qkv(x).view(b, -1, h*w) # b, c*3, h*w\n",
    "        # qkv = self.qkv(x.view(b, c, -1)) # b, c*3, h*w\n",
    "        if cond_out is not None:\n",
    "            _, cc, *hw = cond_out.shape\n",
    "            hh, ww = hw\n",
    "            # cond_out = self.cond_kv(self.norm2(cond_out).view(b, cc, -1))\n",
    "            cond_out = self.cond_kv(cond_out).view(b, -1, hh*ww)\n",
    "            h = self.attention(qkv, cond_out)\n",
    "        else:\n",
    "            h = self.attention(qkv)\n",
    "        # h = self.proj_out(h)\n",
    "        return x + h.reshape(b, c, *spatial)\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "    def forward(self, qkv, cond_kv=None):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads) # no. of channels for q,k,v for each head\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        if cond_kv is not None:\n",
    "            assert cond_kv.shape[1] == self.n_heads * ch * 2\n",
    "            ek, ev = cond_kv.reshape(bs * self.n_heads, ch * 2, -1).split(ch, dim=1)\n",
    "            k = torch.cat([ek, k], dim=-1)\n",
    "            v = torch.cat([ev, v], dim=-1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims # 4\n",
    "        self.in_channels = config.generator_dim * 8 # 192*8\n",
    "        self.channel_mul = config.channel_mul\n",
    "        self.num_resblocks = config.n_resblocks\n",
    "        self.use_deconv=config.use_deconv \n",
    "        self.dropout=config.dropout\n",
    "        ch = self.in_channels\n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding, noise)\n",
    "        x = proj_x.view(-1, self.in_channels, self.in_dims, self.in_dims)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims * config.in_dims # 16\n",
    "        self.in_channels = config.generator_dim # 192\n",
    "        self.channel_mul = config.channel_mul_stage2\n",
    "        self.num_resblocks = config.n_resblocks_stage2\n",
    "        self.use_deconv=config.use_deconv2 \n",
    "        self.dropout=config.dropout2\n",
    "        ch = self.in_channels * 4 \n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "        \n",
    "        self.downblocks= nn.Sequential(\n",
    "            Downsample(3, self.in_channels, kernel_size=3, stride=1, padding=1, batch_norm=False),\n",
    "            Downsample(self.in_channels, self.in_channels*2),\n",
    "            Downsample(self.in_channels*2, self.in_channels*4)\n",
    "        )\n",
    "        self.combined = nn.Sequential(\n",
    "            Downsample(self.in_channels*4 + self.c_dim, self.in_channels*4, kernel_size=3, stride=1, padding=1) # 768 x 16 x 16\n",
    "        )\n",
    "            \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout if layer<2 else False))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        print(\"Initialized stage2 Generator\")\n",
    "        \n",
    "    def forward(self, text_embedding, stage1_out):\n",
    "        enc_img = self.downblocks(stage1_out)\n",
    "        \n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding)\n",
    "        x = proj_x.view(-1, self.c_dim, 1, 1)\n",
    "        x = x.repeat(1, 1, self.in_dims, self.in_dims)\n",
    "        x = torch.cat([enc_img, x], dim=1)\n",
    "        x = self.combined(x)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, d_ch, c_dim, txt_dim, condition):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.condition = condition\n",
    "        self.d_ch = d_ch\n",
    "        self.c_dim = c_dim\n",
    "        self.txt_dim = txt_dim\n",
    "\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8+self.c_dim, n_heads=2, cond_channels=self.c_dim)\n",
    "        # self.conv1 = Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8, n_heads=1)\n",
    "\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Linear(self.txt_dim, self.c_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if condition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                # Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, feat, cond_out=None):\n",
    "        if self.condition:\n",
    "            ### compress text_embeddings using a linear layer\n",
    "            cond_out = self.compress(cond_out)\n",
    "            ### reshape\n",
    "            cond_out = cond_out.view(-1, self.c_dim, 1, 1)\n",
    "            cond_out = cond_out.repeat(1, 1, 4, 4) # (1, 1, 8, 8) (1,1,config.in_dims,config.in_dims)\n",
    "            x = torch.cat((feat, cond_out), 1)\n",
    "        else:\n",
    "            x = feat\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.attention(feat, cond_out)\n",
    "        # x = self.attention(x)\n",
    "        out = self.outlogits(x)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.d_ch = config.discriminator_channel\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.txt_dim = config.text_dim\n",
    "        # self.att_logits = config.att_logits\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            Downsample(3, self.d_ch, batch_norm=False),\n",
    "            Downsample(self.d_ch, self.d_ch*2),\n",
    "            Downsample(self.d_ch*2, self.d_ch*4),\n",
    "            Downsample(self.d_ch*4, self.d_ch*8),\n",
    "        )\n",
    "\n",
    "        if stage == 2:\n",
    "            self.encode_further = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, self.d_ch*16),\n",
    "                Downsample(self.d_ch*16, self.d_ch*32),\n",
    "                Downsample(self.d_ch*32, self.d_ch*16, kernel_size=3, stride=1, padding=1),\n",
    "                Downsample(self.d_ch*16, self.d_ch*8, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        self.cond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, self.txt_dim, condition=True)\n",
    "        self.uncond_discriminator_logits = None\n",
    "        # if self.stage == 2:\n",
    "        #     self.uncond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, condition=False)\n",
    "        print(\"Initialized, stage {} discriminator\".format(stage))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        if self.stage == 2:\n",
    "            x = self.encode_further(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(stage, batch_size, random_captions=True):\n",
    "        imageSize = None\n",
    "        if stage == 1:\n",
    "                imageSize = config.imageSize # 64\n",
    "        else:\n",
    "                imageSize = config.imageSize * 4  # 64*4 = 256\n",
    "\n",
    "        print(f\"Genearting Dataset with image size: {imageSize}\")\n",
    "\n",
    "        tokenizer = get_tokenizer(config.text_encoder)\n",
    "\n",
    "        imageFolder = os.path.join(DATASET, config.dataset, config.imageFolder) # check these params in config before running\n",
    "\n",
    "        if random_captions:\n",
    "                train_images, train_captions = load_data(imageListPath=config.trainImageListPath, captionsListPath=config.trainCaptionsListPath)\n",
    "                train_dataset = Text2ImgDataset_reformed(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage=False)\n",
    "        else:\n",
    "                imageListPath = os.path.join(DATASET, config.dataset, config.imageListPath) # check these params in config before running\n",
    "                captionsListPath = os.path.join(DATASET, config.dataset, config.captionsListPath) # check these params in config before running\n",
    "                train_images, train_captions, test_images, test_captions = make_train_test_split(imageListPath, captionsListPath, config.test_size)\n",
    "                train_dataset = Text2ImgDataset(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage = False) # change based on stage -imagesize\n",
    "        \n",
    "        print(\"Dataset created:\\n\\\n",
    "                length of train dataset: {}\\n\".format(len(train_dataset)))\n",
    "\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fddd23ded90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.tb_dir, exist_ok=True)       # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.model_out, exist_ok=True)    # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.out_img, exist_ok=True)      # change these in config when training stage1 and stage2 accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_controllers(netD, netG):\n",
    "    d_lr = config.d_lr\n",
    "    g_lr = config.g_lr\n",
    "    # We create the optimizer object of the discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = d_lr, betas = (0.5, 0.999))\n",
    "    scheduler_D = MultiStepLR(optimizerD, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True) \n",
    "    # We create the optimizer object of the generator.\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = g_lr, betas = (0.5, 0.999)) \n",
    "    scheduler_G = MultiStepLR(optimizerG, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    L1Loss = nn.L1Loss()\n",
    "\n",
    "    return optimizerD, scheduler_D, optimizerG, scheduler_G, criterion, L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "def train(stage, batch_size, trainloader):\n",
    "\n",
    "    noise_dim = config.z_dim\n",
    "    noise = Variable(torch.FloatTensor(batch_size, noise_dim).float().cuda())\n",
    "    real_labels = Variable(torch.ones(batch_size).float().cuda())\n",
    "    fake_labels = Variable(torch.zeros(batch_size).float().cuda())\n",
    "\n",
    "    assert batch_size == real_labels.shape[0], \"batch_size and target size do not match in real_labels\"\n",
    "    assert batch_size == fake_labels.shape[0], \"batch_size and target size do not match in fake_labels\"\n",
    "\n",
    "    text_encoder = TextEncoder(config.text_encoder, pretrained=True)\n",
    "    text_encoder.eval()\n",
    "\n",
    "    # stage 1 training, only stage 1 g and stage1 d\n",
    "    if stage == 1:\n",
    "        netG = Generator1(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    # stage 2 training, stage1 g output is fed to stage2 g, stage2 d\n",
    "    else:\n",
    "        stage1_G = Generator1(1)\n",
    "        stage1_G = load_from_checkpoint(stage1_G, config.gen1_ckpt)\n",
    "        stage1_G.float().cuda()\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        stage1_G.eval()\n",
    "        netG = Generator2(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    recovered_epoch = 0\n",
    "    if config.load_checkpoint:\n",
    "        if stage == 1:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen1_ckpt, netD, config.d1_ckpt)\n",
    "        else:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen2_ckpt, netD, config.d2_ckpt)\n",
    "    else:\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "    netG.float().cuda()\n",
    "    netD.float().cuda()\n",
    "\n",
    "    optimizerD, schedulerD, optimizerG, schedulerG, criterion, L1Loss = get_controllers(netD, netG)\n",
    "\n",
    "    tb = 'stage' + str(stage) + '_b' + str(batch_size) + '_d' + (str(config.imageSize) if stage==1 else str(config.imageSize*4)) + '_' + str(recovered_epoch)\n",
    "    summary = SummaryWriter(os.path.join(config.tb_dir, tb))\n",
    "   \n",
    "    running_count = 0\n",
    "    # KL_coeff = torch.linspace(0., config.KL_COEFF, 30)\n",
    "    # alpha_l1, _ = torch.linspace(0., config.alpha_L1, 30).sort(descending=True)\n",
    "    KL_coeff = config.KL_COEFF\n",
    "    alpha_L1 = config.alpha_L1\n",
    "\n",
    "    print(f\"Traininig Stage: {stage}, outputs at: {config.tb_dir}, {config.out_img}, {config.model_out}\")\n",
    "\n",
    "    for epoch in range(recovered_epoch+1, config.max_epoch+1):\n",
    "        D_loss = 0\n",
    "        D_real_loss = 0\n",
    "        D_fake_loss = 0\n",
    "        D_wrong_loss = 0\n",
    "        G_loss = 0\n",
    "        KL_l = 0\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = text_encoder(batch['input_ids'], batch['attention_mask'])\n",
    "            text_embeddings = text_embeddings.float().cuda()\n",
    "            real_images = batch['image']\n",
    "            caption = None\n",
    "            if 'caption' in batch:\n",
    "                caption = batch['caption']\n",
    "\n",
    "            noise.data.normal_(0,1)\n",
    "            # noise = torch.randn((batch_size, noise_dim)).float().cuda()\n",
    "            \n",
    "            low_res = None\n",
    "            if stage == 1:\n",
    "                # Generate fake image\n",
    "                inputs = (text_embeddings, noise)\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 64, f\"Image size {fake_images.shape[-1]} differs from 64\"\n",
    "            \n",
    "            else:\n",
    "                # Generate fake image\n",
    "                s1_inputs = (text_embeddings, noise)\n",
    "                with torch.no_grad():\n",
    "                    low_res, _, _ = nn.parallel.data_parallel(stage1_G, s1_inputs, gpus)\n",
    "                # pass stage 1 output to generator\n",
    "                s2_inputs = (text_embeddings, low_res.detach())\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, s2_inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 256, f\"Image size {fake_images.shape[-1]} differs from 256\"\n",
    "\n",
    "            rlabels = real_labels.clone()\n",
    "            flabels = fake_labels.clone()\n",
    "            # label smoothing\n",
    "            if config.label_smoothening:\n",
    "                r = np.random.rand(1)[0]\n",
    "                if r <= 0.5:\n",
    "                    smoothening = np.random.choice(a=np.linspace(0., 0.20, num=5), replace=True, size=batch_size)\n",
    "                    smoothening = torch.tensor(smoothening).float().cuda()\n",
    "                    rlabels -= smoothening\n",
    "                    flabels += smoothening\n",
    "                # occasionally flip labels\n",
    "                else:\n",
    "                    rlabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.05,0.95])\n",
    "                    flabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.95,0.05])\n",
    "                    rlabels = torch.tensor(rlabels).float().cuda()\n",
    "                    flabels = torch.tensor(flabels).float().cuda()\n",
    "\n",
    "            # Update discriminator network\n",
    "            netD.zero_grad()\n",
    "            errD, errD_real, errD_wrong, errD_fake = compute_discriminator_loss(netD, criterion, real_images, fake_images,\n",
    "                                                                                rlabels, flabels, text_embeddings, gpus)\n",
    "            errD.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netD.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerD.step()\n",
    "            D_loss += errD.item()\n",
    "            D_real_loss += errD_real\n",
    "            D_fake_loss += errD_fake\n",
    "            D_wrong_loss += errD_wrong\n",
    "\n",
    "            # Update generator network\n",
    "            netG.zero_grad()\n",
    "            errG_fake = compute_generator_loss(netD, criterion, fake_images, real_images, \n",
    "                                               rlabels, text_embeddings, gpus)\n",
    "            kl_loss = KL_loss(mu, logvar)\n",
    "            errG_total = errG_fake +  (KL_coeff * kl_loss)\n",
    "            if alpha_L1 > 0:\n",
    "                errG_L1 = L1_loss(L1Loss, fake_images, real_images)\n",
    "                errG_total += (alpha_L1 * errG_L1)\n",
    "            \n",
    "            # annealing KL_coeff and L1_loss\n",
    "            # if epoch -1 < 30:\n",
    "            #     kld_coeff = KL_coeff[epoch-1].item()\n",
    "            #     l1_coeff = alpha_l1[epoch-1].item()\n",
    "            # else:\n",
    "            #     kld_coeff = KL_coeff[-1].item()\n",
    "            #     l1_coeff = alpha_l1[-1].item()\n",
    "\n",
    "            errG_total.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netG.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerG.step()\n",
    "            G_loss += errG_total.item()\n",
    "            KL_l += kl_loss.item()\n",
    "\n",
    "            running_count += 1\n",
    "            if i%100 == 0:\n",
    "                print('[%d/%d] [%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, i, len(trainloader), errD.item(), errG_total.item(), kl_loss.item()))\n",
    "                save_img_results(real_images, fake_images, low_res,  caption, epoch, config.out_img)\n",
    "\n",
    "        summary.add_scalars('Discriminator', {'DLoss':D_loss/len(trainloader), \n",
    "                                              'RealLoss':D_real_loss/len(trainloader), \n",
    "                                              'FakeLoss':D_fake_loss/len(trainloader), \n",
    "                                              'WrongLoss':D_wrong_loss/len(trainloader)}, epoch)\n",
    "        summary.add_scalars('Generator', {'GLoss':G_loss/len(trainloader), \n",
    "                                          'KL_Loss':KL_l/len(trainloader)}, epoch)\n",
    "        # summary.add_scalars('Grad_Norm', {'D':np.mean(d_grad_norm),\n",
    "        #                                   'G':np.mean(g_grad_norm)}, epoch) \n",
    "        print('[%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, D_loss/len(trainloader), G_loss/len(trainloader), KL_l/len(trainloader)))\n",
    "        \n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        if epoch % config.save_snapshot == 0:\n",
    "            save_model(netG, netD, epoch, config.model_out, stage=stage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# call model.eval() before feeding the data, as this will change the behavior of the BatchNorm layer \n",
    "# to use the running estimates instead of calculating them for the current batch\n",
    "# \"\"\"\n",
    "# netG1.eval()\n",
    "# netD1.eval()\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 64\n",
      "using distilbert-base-uncased as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 11199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=1\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 256\n",
      "using distilbert-base-uncased as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 11199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=2\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loaded from bert/bert_out_V/checkpoint_s1_bert_ls/netG1_epoch_460.pth, starting at epoch: 460\n",
      "Initialized stage2 Generator\n",
      "Initialized, stage 2 discriminator\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Traininig Stage: 2, outputs at: bert/bert_out_V/tensorboard_s2_bert_onels_1, bert/bert_out_V/results_s2_bert_onels_1, bert/bert_out_V/checkpoint_s2_bert_onels_1\n",
      "[1/500] [0/87] Loss_D: 1.70920, Loss_G: 18.15248, Loss_KL: 0.02996\n",
      "[1/500] Loss_D: 1.75388, Loss_G: 1.54486, Loss_KL: 0.00320\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[2/500] [0/87] Loss_D: 1.35201, Loss_G: 1.35865, Loss_KL: 0.00036\n",
      "[2/500] Loss_D: 1.35734, Loss_G: 1.28451, Loss_KL: 0.00015\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[3/500] [0/87] Loss_D: 1.26096, Loss_G: 1.47950, Loss_KL: 0.00004\n",
      "[3/500] Loss_D: 1.34956, Loss_G: 1.20125, Loss_KL: 0.00004\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[4/500] [0/87] Loss_D: 1.47239, Loss_G: 1.06331, Loss_KL: 0.00003\n",
      "[4/500] Loss_D: 1.36331, Loss_G: 1.05961, Loss_KL: 0.00002\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[5/500] [0/87] Loss_D: 1.38660, Loss_G: 1.22143, Loss_KL: 0.00007\n",
      "[5/500] Loss_D: 1.36139, Loss_G: 1.05896, Loss_KL: 0.00001\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[6/500] [0/87] Loss_D: 1.39091, Loss_G: 0.95011, Loss_KL: 0.00000\n",
      "[6/500] Loss_D: 1.36590, Loss_G: 1.00754, Loss_KL: 0.00004\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[7/500] [0/87] Loss_D: 1.35300, Loss_G: 1.04624, Loss_KL: 0.00000\n",
      "[7/500] Loss_D: 1.37442, Loss_G: 0.91943, Loss_KL: 0.00000\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[8/500] [0/87] Loss_D: 1.40005, Loss_G: 0.97152, Loss_KL: 0.00000\n",
      "[8/500] Loss_D: 1.36137, Loss_G: 0.95825, Loss_KL: 0.00000\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[9/500] [0/87] Loss_D: 1.39462, Loss_G: 1.02150, Loss_KL: 0.00000\n",
      "[9/500] Loss_D: 1.35813, Loss_G: 0.97459, Loss_KL: 0.00001\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[10/500] [0/87] Loss_D: 1.33632, Loss_G: 0.99560, Loss_KL: 0.00000\n",
      "[10/500] Loss_D: 1.35450, Loss_G: 0.94715, Loss_KL: 0.00001\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[11/500] [0/87] Loss_D: 1.33947, Loss_G: 1.04956, Loss_KL: 0.00003\n",
      "[11/500] Loss_D: 1.34742, Loss_G: 0.99387, Loss_KL: 0.00001\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[12/500] [0/87] Loss_D: 1.33852, Loss_G: 0.92953, Loss_KL: 0.00000\n",
      "[12/500] Loss_D: 1.33108, Loss_G: 0.97247, Loss_KL: 0.00000\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[13/500] [0/87] Loss_D: 1.30555, Loss_G: 0.82662, Loss_KL: 0.00001\n",
      "[13/500] Loss_D: 1.32305, Loss_G: 0.97122, Loss_KL: 0.00003\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[14/500] [0/87] Loss_D: 1.30592, Loss_G: 0.94345, Loss_KL: 0.00006\n",
      "[14/500] Loss_D: 1.31069, Loss_G: 1.05750, Loss_KL: 0.00016\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[15/500] [0/87] Loss_D: 1.20535, Loss_G: 1.30152, Loss_KL: 0.00038\n",
      "[15/500] Loss_D: 1.30762, Loss_G: 1.01928, Loss_KL: 0.00054\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[16/500] [0/87] Loss_D: 1.27545, Loss_G: 1.31606, Loss_KL: 0.00099\n",
      "[16/500] Loss_D: 1.29761, Loss_G: 1.04768, Loss_KL: 0.00130\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[17/500] [0/87] Loss_D: 1.28887, Loss_G: 1.21090, Loss_KL: 0.00178\n",
      "[17/500] Loss_D: 1.26403, Loss_G: 1.16257, Loss_KL: 0.00291\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[18/500] [0/87] Loss_D: 1.15571, Loss_G: 1.27209, Loss_KL: 0.00480\n",
      "[18/500] Loss_D: 1.29300, Loss_G: 1.14755, Loss_KL: 0.00448\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[19/500] [0/87] Loss_D: 1.23278, Loss_G: 1.09901, Loss_KL: 0.00463\n",
      "[19/500] Loss_D: 1.22226, Loss_G: 1.37386, Loss_KL: 0.00464\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[20/500] [0/87] Loss_D: 1.11135, Loss_G: 1.66953, Loss_KL: 0.00460\n",
      "[20/500] Loss_D: 1.17295, Loss_G: 1.55863, Loss_KL: 0.00582\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[21/500] [0/87] Loss_D: 1.23289, Loss_G: 2.02065, Loss_KL: 0.00814\n",
      "[21/500] Loss_D: 1.20292, Loss_G: 1.52483, Loss_KL: 0.00650\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[22/500] [0/87] Loss_D: 1.01685, Loss_G: 1.58038, Loss_KL: 0.00690\n",
      "[22/500] Loss_D: 1.15608, Loss_G: 1.70279, Loss_KL: 0.00692\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[23/500] [0/87] Loss_D: 1.27070, Loss_G: 1.48988, Loss_KL: 0.00907\n",
      "[23/500] Loss_D: 1.14842, Loss_G: 1.78868, Loss_KL: 0.00752\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[24/500] [0/87] Loss_D: 1.11744, Loss_G: 1.02041, Loss_KL: 0.00563\n",
      "[24/500] Loss_D: 1.16846, Loss_G: 1.70112, Loss_KL: 0.00759\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[25/500] [0/87] Loss_D: 1.30477, Loss_G: 0.93271, Loss_KL: 0.00924\n",
      "[25/500] Loss_D: 1.18102, Loss_G: 1.57543, Loss_KL: 0.00773\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[26/500] [0/87] Loss_D: 1.15843, Loss_G: 1.90999, Loss_KL: 0.00532\n",
      "[26/500] Loss_D: 1.15484, Loss_G: 1.50132, Loss_KL: 0.00685\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[27/500] [0/87] Loss_D: 1.21612, Loss_G: 1.63359, Loss_KL: 0.00616\n",
      "[27/500] Loss_D: 1.15617, Loss_G: 1.55499, Loss_KL: 0.00699\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[28/500] [0/87] Loss_D: 1.22174, Loss_G: 1.60808, Loss_KL: 0.00692\n",
      "[28/500] Loss_D: 1.12000, Loss_G: 1.60325, Loss_KL: 0.00746\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[29/500] [0/87] Loss_D: 1.36622, Loss_G: 2.83330, Loss_KL: 0.00640\n",
      "[29/500] Loss_D: 1.11724, Loss_G: 1.71782, Loss_KL: 0.00672\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[30/500] [0/87] Loss_D: 1.13852, Loss_G: 1.43328, Loss_KL: 0.00692\n",
      "[30/500] Loss_D: 1.06420, Loss_G: 1.77759, Loss_KL: 0.00771\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[31/500] [0/87] Loss_D: 1.17519, Loss_G: 2.48175, Loss_KL: 0.00752\n",
      "[31/500] Loss_D: 1.10876, Loss_G: 1.69826, Loss_KL: 0.00806\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[32/500] [0/87] Loss_D: 0.95753, Loss_G: 1.57943, Loss_KL: 0.00754\n",
      "[32/500] Loss_D: 1.10196, Loss_G: 1.76687, Loss_KL: 0.00832\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[33/500] [0/87] Loss_D: 1.04915, Loss_G: 1.51113, Loss_KL: 0.00839\n",
      "[33/500] Loss_D: 1.02675, Loss_G: 2.01885, Loss_KL: 0.00912\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[34/500] [0/87] Loss_D: 1.20475, Loss_G: 2.03671, Loss_KL: 0.00895\n",
      "[34/500] Loss_D: 1.01305, Loss_G: 1.93131, Loss_KL: 0.01001\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[35/500] [0/87] Loss_D: 0.92671, Loss_G: 1.52527, Loss_KL: 0.01413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/500] Loss_D: 0.99490, Loss_G: 2.00821, Loss_KL: 0.00930\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[36/500] [0/87] Loss_D: 1.35035, Loss_G: 3.50216, Loss_KL: 0.00900\n",
      "[36/500] Loss_D: 0.98379, Loss_G: 2.08975, Loss_KL: 0.00994\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[37/500] [0/87] Loss_D: 0.74733, Loss_G: 2.14979, Loss_KL: 0.00823\n",
      "[37/500] Loss_D: 1.00088, Loss_G: 2.13612, Loss_KL: 0.00929\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[38/500] [0/87] Loss_D: 0.86628, Loss_G: 2.21131, Loss_KL: 0.00798\n",
      "[38/500] Loss_D: 0.99022, Loss_G: 1.93322, Loss_KL: 0.00882\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[39/500] [0/87] Loss_D: 1.11819, Loss_G: 3.25699, Loss_KL: 0.00747\n",
      "[39/500] Loss_D: 1.03315, Loss_G: 2.09806, Loss_KL: 0.00873\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[40/500] [0/87] Loss_D: 1.23363, Loss_G: 2.59498, Loss_KL: 0.01028\n",
      "[40/500] Loss_D: 0.94501, Loss_G: 2.28706, Loss_KL: 0.00940\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[41/500] [0/87] Loss_D: 1.03396, Loss_G: 1.53893, Loss_KL: 0.01101\n",
      "[41/500] Loss_D: 0.99939, Loss_G: 2.16385, Loss_KL: 0.00882\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[42/500] [0/87] Loss_D: 0.83511, Loss_G: 2.32995, Loss_KL: 0.01040\n",
      "[42/500] Loss_D: 0.94267, Loss_G: 2.33960, Loss_KL: 0.00989\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[43/500] [0/87] Loss_D: 0.81373, Loss_G: 3.13864, Loss_KL: 0.00956\n",
      "[43/500] Loss_D: 0.95714, Loss_G: 2.25075, Loss_KL: 0.01073\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[44/500] [0/87] Loss_D: 1.06739, Loss_G: 2.70139, Loss_KL: 0.00911\n",
      "[44/500] Loss_D: 0.93997, Loss_G: 2.27006, Loss_KL: 0.00830\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[45/500] [0/87] Loss_D: 0.83339, Loss_G: 2.93370, Loss_KL: 0.00671\n",
      "[45/500] Loss_D: 0.96446, Loss_G: 2.25398, Loss_KL: 0.00948\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[46/500] [0/87] Loss_D: 0.95401, Loss_G: 1.27434, Loss_KL: 0.00741\n",
      "[46/500] Loss_D: 0.90871, Loss_G: 2.32774, Loss_KL: 0.00912\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[47/500] [0/87] Loss_D: 0.70217, Loss_G: 1.84715, Loss_KL: 0.01130\n",
      "[47/500] Loss_D: 0.90241, Loss_G: 2.40885, Loss_KL: 0.01057\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[48/500] [0/87] Loss_D: 0.95571, Loss_G: 4.24629, Loss_KL: 0.00918\n",
      "[48/500] Loss_D: 0.85711, Loss_G: 2.56708, Loss_KL: 0.00935\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[49/500] [0/87] Loss_D: 0.75514, Loss_G: 1.33514, Loss_KL: 0.00740\n",
      "[49/500] Loss_D: 0.88585, Loss_G: 2.44875, Loss_KL: 0.01181\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[50/500] [0/87] Loss_D: 0.93477, Loss_G: 3.41350, Loss_KL: 0.01181\n",
      "[50/500] Loss_D: 0.91657, Loss_G: 2.41077, Loss_KL: 0.00971\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[51/500] [0/87] Loss_D: 1.02627, Loss_G: 1.82158, Loss_KL: 0.01027\n",
      "[51/500] Loss_D: 0.81388, Loss_G: 2.14925, Loss_KL: 0.01164\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[52/500] [0/87] Loss_D: 0.75899, Loss_G: 2.17370, Loss_KL: 0.01506\n",
      "[52/500] Loss_D: 0.80029, Loss_G: 1.97200, Loss_KL: 0.01101\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[53/500] [0/87] Loss_D: 0.79373, Loss_G: 1.82517, Loss_KL: 0.01117\n",
      "[53/500] Loss_D: 0.81706, Loss_G: 2.18474, Loss_KL: 0.00979\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[54/500] [0/87] Loss_D: 0.80202, Loss_G: 1.99002, Loss_KL: 0.00441\n",
      "[54/500] Loss_D: 0.76174, Loss_G: 2.38472, Loss_KL: 0.00631\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[55/500] [0/87] Loss_D: 0.72403, Loss_G: 1.97532, Loss_KL: 0.00897\n",
      "[55/500] Loss_D: 0.82049, Loss_G: 2.22480, Loss_KL: 0.00759\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[56/500] [0/87] Loss_D: 0.81416, Loss_G: 1.89668, Loss_KL: 0.00344\n",
      "[56/500] Loss_D: 0.78996, Loss_G: 2.35306, Loss_KL: 0.00433\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[57/500] [0/87] Loss_D: 0.76352, Loss_G: 4.09573, Loss_KL: 0.00557\n",
      "[57/500] Loss_D: 0.75447, Loss_G: 2.48204, Loss_KL: 0.00807\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[58/500] [0/87] Loss_D: 0.70911, Loss_G: 2.09149, Loss_KL: 0.01230\n",
      "[58/500] Loss_D: 0.76337, Loss_G: 2.53715, Loss_KL: 0.01124\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[59/500] [0/87] Loss_D: 0.52803, Loss_G: 2.28634, Loss_KL: 0.01078\n",
      "[59/500] Loss_D: 0.72807, Loss_G: 2.69401, Loss_KL: 0.01093\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[60/500] [0/87] Loss_D: 0.71332, Loss_G: 3.07279, Loss_KL: 0.00848\n",
      "[60/500] Loss_D: 0.74129, Loss_G: 2.70027, Loss_KL: 0.00835\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[61/500] [0/87] Loss_D: 0.75197, Loss_G: 2.15581, Loss_KL: 0.01021\n",
      "[61/500] Loss_D: 0.76059, Loss_G: 2.37524, Loss_KL: 0.00889\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[62/500] [0/87] Loss_D: 0.61131, Loss_G: 3.05278, Loss_KL: 0.00911\n",
      "[62/500] Loss_D: 0.76378, Loss_G: 2.55900, Loss_KL: 0.00911\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[63/500] [0/87] Loss_D: 0.54787, Loss_G: 3.11945, Loss_KL: 0.00605\n",
      "[63/500] Loss_D: 0.72850, Loss_G: 2.63335, Loss_KL: 0.00720\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[64/500] [0/87] Loss_D: 0.81512, Loss_G: 1.51843, Loss_KL: 0.00811\n",
      "[64/500] Loss_D: 0.73173, Loss_G: 2.61095, Loss_KL: 0.00671\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[65/500] [0/87] Loss_D: 1.04506, Loss_G: 4.34901, Loss_KL: 0.00639\n",
      "[65/500] Loss_D: 0.74922, Loss_G: 2.65875, Loss_KL: 0.00889\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[66/500] [0/87] Loss_D: 0.77442, Loss_G: 2.63419, Loss_KL: 0.01301\n",
      "[66/500] Loss_D: 0.69391, Loss_G: 2.66679, Loss_KL: 0.01020\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[67/500] [0/87] Loss_D: 0.47618, Loss_G: 3.26765, Loss_KL: 0.00993\n",
      "[67/500] Loss_D: 0.71586, Loss_G: 2.73720, Loss_KL: 0.00882\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[68/500] [0/87] Loss_D: 1.14910, Loss_G: 3.69931, Loss_KL: 0.00884\n",
      "[68/500] Loss_D: 0.73983, Loss_G: 2.59097, Loss_KL: 0.00789\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[69/500] [0/87] Loss_D: 1.04138, Loss_G: 6.30387, Loss_KL: 0.00994\n",
      "[69/500] Loss_D: 0.72698, Loss_G: 2.77298, Loss_KL: 0.00711\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[70/500] [0/87] Loss_D: 0.71062, Loss_G: 2.62615, Loss_KL: 0.00579\n",
      "[70/500] Loss_D: 0.68735, Loss_G: 2.76232, Loss_KL: 0.00589\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[71/500] [0/87] Loss_D: 0.89344, Loss_G: 4.59209, Loss_KL: 0.00515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/500] Loss_D: 0.73587, Loss_G: 2.75545, Loss_KL: 0.00560\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[72/500] [0/87] Loss_D: 0.76798, Loss_G: 2.40745, Loss_KL: 0.00616\n",
      "[72/500] Loss_D: 0.73024, Loss_G: 2.71282, Loss_KL: 0.00687\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[73/500] [0/87] Loss_D: 0.84207, Loss_G: 1.71882, Loss_KL: 0.00739\n",
      "[73/500] Loss_D: 0.69911, Loss_G: 2.66710, Loss_KL: 0.00695\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[74/500] [0/87] Loss_D: 0.45406, Loss_G: 4.61262, Loss_KL: 0.00831\n",
      "[74/500] Loss_D: 0.75196, Loss_G: 2.64524, Loss_KL: 0.00919\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[75/500] [0/87] Loss_D: 0.80249, Loss_G: 3.73707, Loss_KL: 0.00679\n",
      "[75/500] Loss_D: 0.71793, Loss_G: 2.65105, Loss_KL: 0.00838\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[76/500] [0/87] Loss_D: 0.56829, Loss_G: 2.13590, Loss_KL: 0.01195\n",
      "[76/500] Loss_D: 0.72830, Loss_G: 2.61742, Loss_KL: 0.01049\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[77/500] [0/87] Loss_D: 0.73999, Loss_G: 3.31941, Loss_KL: 0.00852\n",
      "[77/500] Loss_D: 0.73369, Loss_G: 2.58191, Loss_KL: 0.00969\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[78/500] [0/87] Loss_D: 0.99747, Loss_G: 3.07153, Loss_KL: 0.00858\n",
      "[78/500] Loss_D: 0.70643, Loss_G: 2.63729, Loss_KL: 0.00831\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[79/500] [0/87] Loss_D: 0.80363, Loss_G: 3.78392, Loss_KL: 0.00788\n",
      "[79/500] Loss_D: 0.72855, Loss_G: 2.71999, Loss_KL: 0.00807\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[80/500] [0/87] Loss_D: 0.64368, Loss_G: 3.07986, Loss_KL: 0.00742\n",
      "[80/500] Loss_D: 0.67852, Loss_G: 2.74053, Loss_KL: 0.00854\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[81/500] [0/87] Loss_D: 0.50624, Loss_G: 3.51222, Loss_KL: 0.00979\n",
      "[81/500] Loss_D: 0.68763, Loss_G: 2.73863, Loss_KL: 0.00695\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[82/500] [0/87] Loss_D: 0.77899, Loss_G: 2.30688, Loss_KL: 0.00718\n",
      "[82/500] Loss_D: 0.69767, Loss_G: 2.74050, Loss_KL: 0.00750\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[83/500] [0/87] Loss_D: 0.83068, Loss_G: 2.89141, Loss_KL: 0.00637\n",
      "[83/500] Loss_D: 0.69124, Loss_G: 2.73386, Loss_KL: 0.00647\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[84/500] [0/87] Loss_D: 0.57330, Loss_G: 4.06962, Loss_KL: 0.00784\n",
      "[84/500] Loss_D: 0.68693, Loss_G: 2.82160, Loss_KL: 0.00819\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[85/500] [0/87] Loss_D: 0.57116, Loss_G: 2.26193, Loss_KL: 0.00970\n",
      "[85/500] Loss_D: 0.65549, Loss_G: 2.77328, Loss_KL: 0.00929\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[86/500] [0/87] Loss_D: 0.57345, Loss_G: 2.07594, Loss_KL: 0.01078\n",
      "[86/500] Loss_D: 0.68350, Loss_G: 2.83442, Loss_KL: 0.00951\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[87/500] [0/87] Loss_D: 0.32551, Loss_G: 2.47326, Loss_KL: 0.01074\n",
      "[87/500] Loss_D: 0.67431, Loss_G: 2.86096, Loss_KL: 0.00827\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[88/500] [0/87] Loss_D: 0.80359, Loss_G: 1.55057, Loss_KL: 0.00792\n",
      "[88/500] Loss_D: 0.68300, Loss_G: 2.87452, Loss_KL: 0.00859\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[89/500] [0/87] Loss_D: 0.81154, Loss_G: 3.18593, Loss_KL: 0.00958\n",
      "[89/500] Loss_D: 0.64931, Loss_G: 2.97553, Loss_KL: 0.00845\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[90/500] [0/87] Loss_D: 0.83701, Loss_G: 2.22167, Loss_KL: 0.00741\n",
      "[90/500] Loss_D: 0.68410, Loss_G: 2.88417, Loss_KL: 0.01002\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[91/500] [0/87] Loss_D: 0.38265, Loss_G: 3.75212, Loss_KL: 0.01121\n",
      "[91/500] Loss_D: 0.69964, Loss_G: 2.92032, Loss_KL: 0.01011\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[92/500] [0/87] Loss_D: 0.85221, Loss_G: 3.03758, Loss_KL: 0.01050\n",
      "[92/500] Loss_D: 0.66192, Loss_G: 2.92232, Loss_KL: 0.01080\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[93/500] [0/87] Loss_D: 0.58028, Loss_G: 4.51699, Loss_KL: 0.01154\n",
      "[93/500] Loss_D: 0.65646, Loss_G: 2.94122, Loss_KL: 0.01190\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[94/500] [0/87] Loss_D: 0.52312, Loss_G: 1.89226, Loss_KL: 0.01213\n",
      "[94/500] Loss_D: 0.68810, Loss_G: 2.82321, Loss_KL: 0.01143\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[95/500] [0/87] Loss_D: 0.78414, Loss_G: 2.11920, Loss_KL: 0.00982\n",
      "[95/500] Loss_D: 0.68713, Loss_G: 2.89267, Loss_KL: 0.00869\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[96/500] [0/87] Loss_D: 0.74311, Loss_G: 2.52439, Loss_KL: 0.00755\n",
      "[96/500] Loss_D: 0.67658, Loss_G: 2.89057, Loss_KL: 0.00952\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[97/500] [0/87] Loss_D: 0.83519, Loss_G: 2.57600, Loss_KL: 0.01000\n",
      "[97/500] Loss_D: 0.65300, Loss_G: 2.85667, Loss_KL: 0.01001\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[98/500] [0/87] Loss_D: 0.42761, Loss_G: 2.68423, Loss_KL: 0.01093\n",
      "[98/500] Loss_D: 0.65073, Loss_G: 2.95678, Loss_KL: 0.01163\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[99/500] [0/87] Loss_D: 0.57449, Loss_G: 2.97517, Loss_KL: 0.01060\n",
      "[99/500] Loss_D: 0.66002, Loss_G: 2.92843, Loss_KL: 0.01162\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[100/500] [0/87] Loss_D: 0.78286, Loss_G: 5.05342, Loss_KL: 0.00830\n",
      "[100/500] Loss_D: 0.68455, Loss_G: 3.02547, Loss_KL: 0.00944\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[101/500] [0/87] Loss_D: 1.05667, Loss_G: 1.73122, Loss_KL: 0.01118\n",
      "[101/500] Loss_D: 0.65208, Loss_G: 2.29317, Loss_KL: 0.01004\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[102/500] [0/87] Loss_D: 0.52332, Loss_G: 2.41716, Loss_KL: 0.01043\n",
      "[102/500] Loss_D: 0.64533, Loss_G: 2.40886, Loss_KL: 0.01024\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[103/500] [0/87] Loss_D: 0.70809, Loss_G: 2.91336, Loss_KL: 0.00994\n",
      "[103/500] Loss_D: 0.62739, Loss_G: 2.55174, Loss_KL: 0.00964\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[104/500] [0/87] Loss_D: 0.42721, Loss_G: 2.83901, Loss_KL: 0.00953\n",
      "[104/500] Loss_D: 0.65160, Loss_G: 2.50419, Loss_KL: 0.01033\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[105/500] [0/87] Loss_D: 0.62951, Loss_G: 2.38755, Loss_KL: 0.01007\n",
      "[105/500] Loss_D: 0.63452, Loss_G: 2.47834, Loss_KL: 0.01067\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[106/500] [0/87] Loss_D: 0.57569, Loss_G: 2.75654, Loss_KL: 0.00968\n",
      "[106/500] Loss_D: 0.64767, Loss_G: 2.46204, Loss_KL: 0.01159\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[107/500] [0/87] Loss_D: 0.72297, Loss_G: 2.34465, Loss_KL: 0.01034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107/500] Loss_D: 0.62563, Loss_G: 2.65804, Loss_KL: 0.01058\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[108/500] [0/87] Loss_D: 0.72710, Loss_G: 2.64887, Loss_KL: 0.00974\n",
      "[108/500] Loss_D: 0.64271, Loss_G: 2.67250, Loss_KL: 0.01006\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[109/500] [0/87] Loss_D: 0.63682, Loss_G: 2.71998, Loss_KL: 0.00972\n",
      "[109/500] Loss_D: 0.67631, Loss_G: 2.50307, Loss_KL: 0.01034\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[110/500] [0/87] Loss_D: 0.80198, Loss_G: 1.67615, Loss_KL: 0.01048\n",
      "[110/500] Loss_D: 0.61078, Loss_G: 2.61491, Loss_KL: 0.01129\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[111/500] [0/87] Loss_D: 0.83957, Loss_G: 3.23447, Loss_KL: 0.01034\n",
      "[111/500] Loss_D: 0.60422, Loss_G: 2.67654, Loss_KL: 0.01004\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[112/500] [0/87] Loss_D: 0.67802, Loss_G: 2.57553, Loss_KL: 0.01121\n",
      "[112/500] Loss_D: 0.64156, Loss_G: 2.59122, Loss_KL: 0.01080\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[113/500] [0/87] Loss_D: 0.39448, Loss_G: 3.25056, Loss_KL: 0.00983\n",
      "[113/500] Loss_D: 0.62370, Loss_G: 2.63293, Loss_KL: 0.01033\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[114/500] [0/87] Loss_D: 0.54713, Loss_G: 3.77230, Loss_KL: 0.01135\n",
      "[114/500] Loss_D: 0.62303, Loss_G: 2.65363, Loss_KL: 0.01089\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[115/500] [0/87] Loss_D: 0.74947, Loss_G: 2.35080, Loss_KL: 0.00984\n",
      "[115/500] Loss_D: 0.60601, Loss_G: 2.66494, Loss_KL: 0.00965\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[116/500] [0/87] Loss_D: 0.76441, Loss_G: 2.05226, Loss_KL: 0.00948\n",
      "[116/500] Loss_D: 0.63986, Loss_G: 2.57718, Loss_KL: 0.00959\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[117/500] [0/87] Loss_D: 0.49784, Loss_G: 2.15093, Loss_KL: 0.01124\n",
      "[117/500] Loss_D: 0.63899, Loss_G: 2.56142, Loss_KL: 0.01018\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[118/500] [0/87] Loss_D: 0.72726, Loss_G: 2.40918, Loss_KL: 0.01077\n",
      "[118/500] Loss_D: 0.63450, Loss_G: 2.71561, Loss_KL: 0.01002\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[119/500] [0/87] Loss_D: 0.80106, Loss_G: 2.57774, Loss_KL: 0.00937\n",
      "[119/500] Loss_D: 0.62202, Loss_G: 2.63977, Loss_KL: 0.00951\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[120/500] [0/87] Loss_D: 0.76241, Loss_G: 2.32191, Loss_KL: 0.01113\n",
      "[120/500] Loss_D: 0.65593, Loss_G: 2.65330, Loss_KL: 0.01165\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[121/500] [0/87] Loss_D: 0.70646, Loss_G: 2.60483, Loss_KL: 0.01164\n",
      "[121/500] Loss_D: 0.61636, Loss_G: 2.63212, Loss_KL: 0.01166\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[122/500] [0/87] Loss_D: 0.42898, Loss_G: 3.80428, Loss_KL: 0.01171\n",
      "[122/500] Loss_D: 0.61252, Loss_G: 2.75996, Loss_KL: 0.01207\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[123/500] [0/87] Loss_D: 0.44088, Loss_G: 3.45033, Loss_KL: 0.01262\n",
      "[123/500] Loss_D: 0.60155, Loss_G: 2.80139, Loss_KL: 0.01263\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[124/500] [0/87] Loss_D: 0.56117, Loss_G: 2.55162, Loss_KL: 0.01277\n",
      "[124/500] Loss_D: 0.62523, Loss_G: 2.84045, Loss_KL: 0.01187\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[125/500] [0/87] Loss_D: 0.67694, Loss_G: 2.57267, Loss_KL: 0.01168\n",
      "[125/500] Loss_D: 0.63180, Loss_G: 2.72591, Loss_KL: 0.01060\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[126/500] [0/87] Loss_D: 0.70793, Loss_G: 2.29377, Loss_KL: 0.01047\n",
      "[126/500] Loss_D: 0.63663, Loss_G: 2.82213, Loss_KL: 0.01183\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[127/500] [0/87] Loss_D: 0.52744, Loss_G: 3.49455, Loss_KL: 0.01187\n",
      "[127/500] Loss_D: 0.66331, Loss_G: 3.04132, Loss_KL: 0.01347\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[128/500] [0/87] Loss_D: 0.56268, Loss_G: 3.68793, Loss_KL: 0.01325\n",
      "[128/500] Loss_D: 0.60742, Loss_G: 2.98510, Loss_KL: 0.01456\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[129/500] [0/87] Loss_D: 0.46984, Loss_G: 5.03983, Loss_KL: 0.01294\n",
      "[129/500] Loss_D: 0.63180, Loss_G: 3.05398, Loss_KL: 0.01238\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[130/500] [0/87] Loss_D: 0.72105, Loss_G: 2.81308, Loss_KL: 0.01061\n",
      "[130/500] Loss_D: 0.65263, Loss_G: 2.96525, Loss_KL: 0.01299\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[131/500] [0/87] Loss_D: 0.86721, Loss_G: 1.89044, Loss_KL: 0.01241\n",
      "[131/500] Loss_D: 0.62554, Loss_G: 3.15902, Loss_KL: 0.01269\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[132/500] [0/87] Loss_D: 0.55976, Loss_G: 3.14418, Loss_KL: 0.01391\n",
      "[132/500] Loss_D: 0.65509, Loss_G: 2.89275, Loss_KL: 0.01337\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[133/500] [0/87] Loss_D: 0.66257, Loss_G: 2.88281, Loss_KL: 0.01243\n",
      "[133/500] Loss_D: 0.64922, Loss_G: 2.96904, Loss_KL: 0.01161\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[134/500] [0/87] Loss_D: 0.76488, Loss_G: 2.72844, Loss_KL: 0.01100\n",
      "[134/500] Loss_D: 0.64096, Loss_G: 2.87821, Loss_KL: 0.01220\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[135/500] [0/87] Loss_D: 0.50703, Loss_G: 2.39714, Loss_KL: 0.01387\n",
      "[135/500] Loss_D: 0.63458, Loss_G: 2.98997, Loss_KL: 0.01285\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[136/500] [0/87] Loss_D: 0.69351, Loss_G: 2.91978, Loss_KL: 0.01066\n",
      "[136/500] Loss_D: 0.60099, Loss_G: 3.00083, Loss_KL: 0.01388\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[137/500] [0/87] Loss_D: 0.55342, Loss_G: 2.92821, Loss_KL: 0.01622\n",
      "[137/500] Loss_D: 0.64259, Loss_G: 3.06431, Loss_KL: 0.01286\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[138/500] [0/87] Loss_D: 0.71979, Loss_G: 2.95208, Loss_KL: 0.01284\n",
      "[138/500] Loss_D: 0.62560, Loss_G: 3.08162, Loss_KL: 0.01216\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[139/500] [0/87] Loss_D: 0.47576, Loss_G: 2.70174, Loss_KL: 0.01244\n",
      "[139/500] Loss_D: 0.62322, Loss_G: 3.04722, Loss_KL: 0.01354\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[140/500] [0/87] Loss_D: 0.75331, Loss_G: 2.61431, Loss_KL: 0.01377\n",
      "[140/500] Loss_D: 0.61457, Loss_G: 2.92546, Loss_KL: 0.01396\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[141/500] [0/87] Loss_D: 0.47739, Loss_G: 3.65142, Loss_KL: 0.01057\n",
      "[141/500] Loss_D: 0.63499, Loss_G: 3.06276, Loss_KL: 0.01388\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[142/500] [0/87] Loss_D: 0.45767, Loss_G: 4.02122, Loss_KL: 0.01608\n",
      "[142/500] Loss_D: 0.61311, Loss_G: 2.96419, Loss_KL: 0.01555\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[143/500] [0/87] Loss_D: 0.47835, Loss_G: 2.93332, Loss_KL: 0.01448\n",
      "[143/500] Loss_D: 0.60968, Loss_G: 2.90272, Loss_KL: 0.01508\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[144/500] [0/87] Loss_D: 0.71275, Loss_G: 2.82138, Loss_KL: 0.01558\n",
      "[144/500] Loss_D: 0.60090, Loss_G: 3.13558, Loss_KL: 0.01510\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[145/500] [0/87] Loss_D: 0.35676, Loss_G: 3.97383, Loss_KL: 0.01415\n",
      "[145/500] Loss_D: 0.63233, Loss_G: 3.14263, Loss_KL: 0.01394\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[146/500] [0/87] Loss_D: 0.68945, Loss_G: 2.95983, Loss_KL: 0.01382\n",
      "[146/500] Loss_D: 0.61991, Loss_G: 3.10139, Loss_KL: 0.01408\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[147/500] [0/87] Loss_D: 0.58395, Loss_G: 2.62056, Loss_KL: 0.01558\n",
      "[147/500] Loss_D: 0.59918, Loss_G: 2.98944, Loss_KL: 0.01536\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[148/500] [0/87] Loss_D: 0.72837, Loss_G: 2.68313, Loss_KL: 0.01521\n",
      "[148/500] Loss_D: 0.65871, Loss_G: 2.83859, Loss_KL: 0.01429\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[149/500] [0/87] Loss_D: 0.71136, Loss_G: 2.58110, Loss_KL: 0.01349\n",
      "[149/500] Loss_D: 0.59345, Loss_G: 2.95071, Loss_KL: 0.01503\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[150/500] [0/87] Loss_D: 0.72354, Loss_G: 3.28787, Loss_KL: 0.01394\n",
      "[150/500] Loss_D: 0.68548, Loss_G: 2.99774, Loss_KL: 0.01404\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[151/500] [0/87] Loss_D: 0.48290, Loss_G: 3.73868, Loss_KL: 0.01338\n",
      "[151/500] Loss_D: 0.60495, Loss_G: 2.53109, Loss_KL: 0.01415\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[152/500] [0/87] Loss_D: 0.70063, Loss_G: 2.46503, Loss_KL: 0.01564\n",
      "[152/500] Loss_D: 0.61962, Loss_G: 2.54074, Loss_KL: 0.01439\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[153/500] [0/87] Loss_D: 0.67947, Loss_G: 2.45293, Loss_KL: 0.01378\n",
      "[153/500] Loss_D: 0.61256, Loss_G: 2.56776, Loss_KL: 0.01317\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[154/500] [0/87] Loss_D: 0.71663, Loss_G: 2.35798, Loss_KL: 0.01101\n",
      "[154/500] Loss_D: 0.63223, Loss_G: 2.58665, Loss_KL: 0.01334\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[155/500] [0/87] Loss_D: 0.74096, Loss_G: 2.02347, Loss_KL: 0.01289\n",
      "[155/500] Loss_D: 0.62647, Loss_G: 2.57521, Loss_KL: 0.01268\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[156/500] [0/87] Loss_D: 0.69512, Loss_G: 2.40436, Loss_KL: 0.01227\n",
      "[156/500] Loss_D: 0.61001, Loss_G: 2.55772, Loss_KL: 0.01287\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[157/500] [0/87] Loss_D: 0.68974, Loss_G: 2.92899, Loss_KL: 0.01372\n",
      "[157/500] Loss_D: 0.63538, Loss_G: 2.53284, Loss_KL: 0.01271\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[158/500] [0/87] Loss_D: 0.59730, Loss_G: 3.26179, Loss_KL: 0.01182\n",
      "[158/500] Loss_D: 0.62794, Loss_G: 2.62702, Loss_KL: 0.01363\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[159/500] [0/87] Loss_D: 0.70807, Loss_G: 2.37343, Loss_KL: 0.01468\n",
      "[159/500] Loss_D: 0.59909, Loss_G: 2.56775, Loss_KL: 0.01368\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[160/500] [0/87] Loss_D: 0.71561, Loss_G: 2.56568, Loss_KL: 0.01389\n",
      "[160/500] Loss_D: 0.60296, Loss_G: 2.70705, Loss_KL: 0.01365\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[161/500] [0/87] Loss_D: 0.70299, Loss_G: 2.47358, Loss_KL: 0.01685\n",
      "[161/500] Loss_D: 0.62347, Loss_G: 2.61293, Loss_KL: 0.01444\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[162/500] [0/87] Loss_D: 0.49163, Loss_G: 3.30133, Loss_KL: 0.01436\n",
      "[162/500] Loss_D: 0.59340, Loss_G: 2.77980, Loss_KL: 0.01484\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[163/500] [0/87] Loss_D: 0.67982, Loss_G: 2.56194, Loss_KL: 0.01692\n",
      "[163/500] Loss_D: 0.60509, Loss_G: 2.78718, Loss_KL: 0.01523\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[164/500] [0/87] Loss_D: 0.78571, Loss_G: 2.36888, Loss_KL: 0.01538\n",
      "[164/500] Loss_D: 0.62602, Loss_G: 2.66229, Loss_KL: 0.01607\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[165/500] [0/87] Loss_D: 0.50155, Loss_G: 2.62168, Loss_KL: 0.01703\n",
      "[165/500] Loss_D: 0.62953, Loss_G: 2.69269, Loss_KL: 0.01615\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[166/500] [0/87] Loss_D: 0.40940, Loss_G: 2.99357, Loss_KL: 0.01391\n",
      "[166/500] Loss_D: 0.58114, Loss_G: 2.89625, Loss_KL: 0.01524\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[167/500] [0/87] Loss_D: 0.67792, Loss_G: 3.18535, Loss_KL: 0.01467\n",
      "[167/500] Loss_D: 0.61946, Loss_G: 2.62601, Loss_KL: 0.01585\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[168/500] [0/87] Loss_D: 0.72390, Loss_G: 3.05348, Loss_KL: 0.01473\n"
     ]
    }
   ],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba9bc282ea7dd8acf6b93a88ab047ea17bba2d98cff2c21ca6cffa26ac4d8f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
