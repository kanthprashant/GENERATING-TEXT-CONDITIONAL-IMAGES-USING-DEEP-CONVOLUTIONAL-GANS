{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/common/users/ppk31/CS543_DL_Proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from configs import config\n",
    "from pytorch_model_summary import summary\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import (weights_init, make_train_test_split, load_data, compute_discriminator_loss, \n",
    "                   compute_generator_loss, KL_loss, L1_loss, save_img_results, save_model, load_from_checkpoint)\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from dataset import Text2ImgDataset, Text2ImgDataset_reformed\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DATASET = '/freespace/local/ppk31_cs543/Project/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.text_encoder == \"distilbert-base-uncased\":\n",
    "    from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "elif config.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "    from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"using GPU: {torch.cuda.is_available()}\")\n",
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print(f\"GPU ids: {gpus}\")\n",
    "\n",
    "torch.random.seed()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.set_device(gpus[0])\n",
    "cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(text_encoder):\n",
    "    print(f\"using {text_encoder} as text encoder\")\n",
    "    if text_encoder == \"distilbert-base-uncased\":\n",
    "        return DistilBertTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "        return CLIPTokenizer.from_pretrained(text_encoder)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, pretrained=True):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if text_encoder == \"distilbert-base-uncased\":\n",
    "            self.encoder = DistilBertModel.from_pretrained(text_encoder)\n",
    "        elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            self.encoder = CLIPModel.from_pretrained(text_encoder)\n",
    "        # self.text_embedding = 768\n",
    "        # self.projection = ProjectionHead('text_projector', self.text_embedding, project_dim)\n",
    "        self.retrieve_token_index = 0\n",
    "    \n",
    "    def forward(self, input_tokens, attention_mask):\n",
    "        if self.text_encoder == \"distilbert-base-uncased\":\n",
    "            out = self.encoder(input_ids = input_tokens, attention_mask = attention_mask)\n",
    "            last_hidden_states = out.last_hidden_state\n",
    "            embeddings = last_hidden_states[:, self.retrieve_token_index, :]    # output_dimensions = 768\n",
    "        elif self.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            embeddings = self.encoder.get_text_features(input_ids = input_tokens, attention_mask = attention_mask) # output_dimensions = 512\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmented_Projection(nn.Module):\n",
    "    def __init__(self, stage, gen_channels, gen_dim):\n",
    "        super(Augmented_Projection, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.t_dim = config.text_dim\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.z_dim = config.z_dim\n",
    "        self.gen_in = gen_channels #config.generator_dim * gen_dim\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stage == 1:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(self.c_dim + self.z_dim, self.gen_in * gen_dim * gen_dim, bias=False), # bias=False, # 768 -> 192*8*8*8\n",
    "                nn.BatchNorm1d(self.gen_in * gen_dim * gen_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def augment(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        # if config.cuda_is_available:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_().cuda()\n",
    "        # else:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(torch.randn(std.size()).float().cuda())\n",
    "        # eps = Variable(eps)\n",
    "        # eps.mul(std).add(mu)\n",
    "        return mu + (std * eps)\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        if noise is None and self.stage==1:\n",
    "            noise = torch.randn((text_embedding.shape[0], self.z_dim)).float().cuda()\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        c_code = self.augment(mu, logvar)\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            c_code = torch.cat((c_code, noise), dim=1)\n",
    "            c_code = self.project(c_code)\n",
    "        \n",
    "        return c_code, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, kernel_size=4, stride=2, padding=1, batch_norm=True, activation=True, use_conv=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, stride=1, padding=1, batch_norm=True, activation=True, bias=False, use_deconv=False, dropout=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.use_deconv = use_deconv\n",
    "\n",
    "        if use_deconv:\n",
    "            self.deconv = nn.ConvTranspose2d(self.channels, self.out_channels, kernel_size=4, stride=2, padding=padding, bias=bias) # use when not using interpolate\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=stride, padding=padding, bias = bias)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.ReLU()\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.use_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param in_channels: the number of input channels.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        stride = 1,\n",
    "        padding = 1\n",
    "    ):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        if in_channels == out_channels:\n",
    "                self.x_residual = nn.Identity()\n",
    "        else:\n",
    "            self.x_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.x_residual(x)\n",
    "        h = x + g\n",
    "        return self.relu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    :param channels: is the number of channels in the feature map\n",
    "    :param n_heads: is the number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, n_heads=1, cond_channels=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        assert (\n",
    "            channels % n_heads == 0\n",
    "        ), f\"q,k,v channels {channels//n_heads} cannot be constructed for {n_heads} heads, input channels: {channels}\"\n",
    "        self.n_heads = n_heads\n",
    "        # self.norm1 = nn.GroupNorm(num_groups=16, num_channels=channels, eps=1e-6, affine=True) # num_groups=32\n",
    "        # self.norm1 = nn.BatchNorm2d(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels*3, kernel_size=1)\n",
    "        self.attention = QKVAttention(self.n_heads)\n",
    "        if cond_channels is not None:\n",
    "            # self.norm2 = nn.GroupNorm(num_groups=16, num_channels=cond_channels, eps=1e-6, affine=True) # num_groups=32\n",
    "            # self.norm2 = nn.BatchNorm2d(cond_channels)\n",
    "            self.cond_kv = nn.Conv2d(cond_channels, channels*2, kernel_size=1)\n",
    "        # self.proj_out = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "    def forward(self, x, cond_out = None):\n",
    "        b, c, *spatial = x.shape\n",
    "        h, w = spatial\n",
    "        # qkv = self.qkv(self.norm1(x).view(b, c, -1)) # b, c*3, h*w\n",
    "        qkv = self.qkv(x).view(b, -1, h*w) # b, c*3, h*w\n",
    "        # qkv = self.qkv(x.view(b, c, -1)) # b, c*3, h*w\n",
    "        if cond_out is not None:\n",
    "            _, cc, *hw = cond_out.shape\n",
    "            hh, ww = hw\n",
    "            # cond_out = self.cond_kv(self.norm2(cond_out).view(b, cc, -1))\n",
    "            cond_out = self.cond_kv(cond_out).view(b, -1, hh*ww)\n",
    "            h = self.attention(qkv, cond_out)\n",
    "        else:\n",
    "            h = self.attention(qkv)\n",
    "        # h = self.proj_out(h)\n",
    "        return x + h.reshape(b, c, *spatial)\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "    def forward(self, qkv, cond_kv=None):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads) # no. of channels for q,k,v for each head\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        if cond_kv is not None:\n",
    "            assert cond_kv.shape[1] == self.n_heads * ch * 2\n",
    "            ek, ev = cond_kv.reshape(bs * self.n_heads, ch * 2, -1).split(ch, dim=1)\n",
    "            k = torch.cat([ek, k], dim=-1)\n",
    "            v = torch.cat([ev, v], dim=-1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims # 4\n",
    "        self.in_channels = config.generator_dim * 8 # 192*8\n",
    "        self.channel_mul = config.channel_mul\n",
    "        self.num_resblocks = config.n_resblocks\n",
    "        self.use_deconv=config.use_deconv \n",
    "        self.dropout=config.dropout\n",
    "        ch = self.in_channels\n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding, noise)\n",
    "        x = proj_x.view(-1, self.in_channels, self.in_dims, self.in_dims)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims * config.in_dims # 16\n",
    "        self.in_channels = config.generator_dim # 192\n",
    "        self.channel_mul = config.channel_mul_stage2\n",
    "        self.num_resblocks = config.n_resblocks_stage2\n",
    "        self.use_deconv=config.use_deconv2 \n",
    "        self.dropout=config.dropout2\n",
    "        ch = self.in_channels * 4 \n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "        \n",
    "        self.downblocks= nn.Sequential(\n",
    "            Downsample(3, self.in_channels, kernel_size=3, stride=1, padding=1, batch_norm=False),\n",
    "            Downsample(self.in_channels, self.in_channels*2),\n",
    "            Downsample(self.in_channels*2, self.in_channels*4)\n",
    "        )\n",
    "        self.combined = nn.Sequential(\n",
    "            Downsample(self.in_channels*4 + self.c_dim, self.in_channels*4, kernel_size=3, stride=1, padding=1) # 768 x 16 x 16\n",
    "        )\n",
    "            \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout if layer<2 else False))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        print(\"Initialized stage2 Generator\")\n",
    "        \n",
    "    def forward(self, text_embedding, stage1_out):\n",
    "        enc_img = self.downblocks(stage1_out)\n",
    "        \n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding)\n",
    "        x = proj_x.view(-1, self.c_dim, 1, 1)\n",
    "        x = x.repeat(1, 1, self.in_dims, self.in_dims)\n",
    "        x = torch.cat([enc_img, x], dim=1)\n",
    "        x = self.combined(x)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, d_ch, c_dim, txt_dim, condition):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.condition = condition\n",
    "        self.d_ch = d_ch\n",
    "        self.c_dim = c_dim\n",
    "        self.txt_dim = txt_dim\n",
    "\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8+self.c_dim, n_heads=2, cond_channels=self.c_dim)\n",
    "        # self.conv1 = Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8, n_heads=1)\n",
    "\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Linear(self.txt_dim, self.c_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if condition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                # Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, feat, cond_out=None):\n",
    "        if self.condition:\n",
    "            ### compress text_embeddings using a linear layer\n",
    "            cond_out = self.compress(cond_out)\n",
    "            ### reshape\n",
    "            cond_out = cond_out.view(-1, self.c_dim, 1, 1)\n",
    "            cond_out = cond_out.repeat(1, 1, 4, 4) # (1, 1, 8, 8) (1,1,config.in_dims,config.in_dims)\n",
    "            x = torch.cat((feat, cond_out), 1)\n",
    "        else:\n",
    "            x = feat\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.attention(feat, cond_out)\n",
    "        # x = self.attention(x)\n",
    "        out = self.outlogits(x)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.d_ch = config.discriminator_channel\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.txt_dim = config.text_dim\n",
    "        # self.att_logits = config.att_logits\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            Downsample(3, self.d_ch, batch_norm=False),\n",
    "            Downsample(self.d_ch, self.d_ch*2),\n",
    "            Downsample(self.d_ch*2, self.d_ch*4),\n",
    "            Downsample(self.d_ch*4, self.d_ch*8),\n",
    "        )\n",
    "\n",
    "        if stage == 2:\n",
    "            self.encode_further = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, self.d_ch*16),\n",
    "                Downsample(self.d_ch*16, self.d_ch*32),\n",
    "                Downsample(self.d_ch*32, self.d_ch*16, kernel_size=3, stride=1, padding=1),\n",
    "                Downsample(self.d_ch*16, self.d_ch*8, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        self.cond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, self.txt_dim, condition=True)\n",
    "        self.uncond_discriminator_logits = None\n",
    "        # if self.stage == 2:\n",
    "        #     self.uncond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, condition=False)\n",
    "        print(\"Initialized, stage {} discriminator\".format(stage))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        if self.stage == 2:\n",
    "            x = self.encode_further(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(stage, batch_size, random_captions=True):\n",
    "        imageSize = None\n",
    "        if stage == 1:\n",
    "                imageSize = config.imageSize # 64\n",
    "        else:\n",
    "                imageSize = config.imageSize * 4  # 64*4 = 256\n",
    "\n",
    "        print(f\"Genearting Dataset with image size: {imageSize}\")\n",
    "\n",
    "        tokenizer = get_tokenizer(config.text_encoder)\n",
    "\n",
    "        imageFolder = os.path.join(DATASET, config.dataset, config.imageFolder) # check these params in config before running\n",
    "\n",
    "        if random_captions:\n",
    "                train_images, train_captions = load_data(imageListPath=config.trainImageListPath, captionsListPath=config.trainCaptionsListPath)\n",
    "                train_dataset = Text2ImgDataset_reformed(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage=False)\n",
    "        else:\n",
    "                imageListPath = os.path.join(DATASET, config.dataset, config.imageListPath) # check these params in config before running\n",
    "                captionsListPath = os.path.join(DATASET, config.dataset, config.captionsListPath) # check these params in config before running\n",
    "                train_images, train_captions, test_images, test_captions = make_train_test_split(imageListPath, captionsListPath, config.test_size)\n",
    "                train_dataset = Text2ImgDataset(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage = False) # change based on stage -imagesize\n",
    "        \n",
    "        print(\"Dataset created:\\n\\\n",
    "                length of train dataset: {}\\n\".format(len(train_dataset)))\n",
    "\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.tb_dir, exist_ok=True)       # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.model_out, exist_ok=True)    # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.out_img, exist_ok=True)      # change these in config when training stage1 and stage2 accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_controllers(netD, netG):\n",
    "    d_lr = config.d_lr\n",
    "    g_lr = config.g_lr\n",
    "    # We create the optimizer object of the discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = d_lr, betas = (0.5, 0.999))\n",
    "    scheduler_D = MultiStepLR(optimizerD, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True) \n",
    "    # We create the optimizer object of the generator.\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = g_lr, betas = (0.5, 0.999)) \n",
    "    scheduler_G = MultiStepLR(optimizerG, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    L1Loss = nn.L1Loss()\n",
    "\n",
    "    return optimizerD, scheduler_D, optimizerG, scheduler_G, criterion, L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "def train(stage, batch_size, trainloader):\n",
    "\n",
    "    noise_dim = config.z_dim\n",
    "    noise = Variable(torch.FloatTensor(batch_size, noise_dim).float().cuda())\n",
    "    real_labels = Variable(torch.ones(batch_size).float().cuda())\n",
    "    fake_labels = Variable(torch.zeros(batch_size).float().cuda())\n",
    "\n",
    "    assert batch_size == real_labels.shape[0], \"batch_size and target size do not match in real_labels\"\n",
    "    assert batch_size == fake_labels.shape[0], \"batch_size and target size do not match in fake_labels\"\n",
    "\n",
    "    text_encoder = TextEncoder(config.text_encoder, pretrained=True)\n",
    "    text_encoder.eval()\n",
    "\n",
    "    # stage 1 training, only stage 1 g and stage1 d\n",
    "    if stage == 1:\n",
    "        netG = Generator1(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    # stage 2 training, stage1 g output is fed to stage2 g, stage2 d\n",
    "    else:\n",
    "        stage1_G = Generator1(1)\n",
    "        stage1_G = load_from_checkpoint(stage1_G, config.gen1_ckpt)\n",
    "        stage1_G.float().cuda()\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        stage1_G.eval()\n",
    "        netG = Generator2(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    recovered_epoch = 0\n",
    "    if config.load_checkpoint:\n",
    "        if stage == 1:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen1_ckpt, netD, config.d1_ckpt)\n",
    "        else:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen2_ckpt, netD, config.d2_ckpt)\n",
    "    else:\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "    netG.float().cuda()\n",
    "    netD.float().cuda()\n",
    "\n",
    "    optimizerD, schedulerD, optimizerG, schedulerG, criterion, L1Loss = get_controllers(netD, netG)\n",
    "\n",
    "    tb = 'stage' + str(stage) + '_b' + str(batch_size) + '_d' + (str(config.imageSize) if stage==1 else str(config.imageSize*4)) + '_' + str(recovered_epoch)\n",
    "    summary = SummaryWriter(os.path.join(config.tb_dir, tb))\n",
    "   \n",
    "    running_count = 0\n",
    "    # KL_coeff = torch.linspace(0., config.KL_COEFF, 30)\n",
    "    # alpha_l1, _ = torch.linspace(0., config.alpha_L1, 30).sort(descending=True)\n",
    "    KL_coeff = config.KL_COEFF\n",
    "    alpha_L1 = config.alpha_L1\n",
    "\n",
    "    print(f\"Traininig Stage: {stage}, outputs at: {config.tb_dir}, {config.out_img}, {config.model_out}\")\n",
    "\n",
    "    for epoch in range(recovered_epoch+1, config.max_epoch+1):\n",
    "        D_loss = 0\n",
    "        D_real_loss = 0\n",
    "        D_fake_loss = 0\n",
    "        D_wrong_loss = 0\n",
    "        G_loss = 0\n",
    "        KL_l = 0\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = text_encoder(batch['input_ids'], batch['attention_mask'])\n",
    "            text_embeddings = text_embeddings.float().cuda()\n",
    "            real_images = batch['image']\n",
    "            caption = None\n",
    "            if 'caption' in batch:\n",
    "                caption = batch['caption']\n",
    "\n",
    "            noise.data.normal_(0,1)\n",
    "            # noise = torch.randn((batch_size, noise_dim)).float().cuda()\n",
    "            \n",
    "            low_res = None\n",
    "            if stage == 1:\n",
    "                # Generate fake image\n",
    "                inputs = (text_embeddings, noise)\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 64, f\"Image size {fake_images.shape[-1]} differs from 64\"\n",
    "            \n",
    "            else:\n",
    "                # Generate fake image\n",
    "                s1_inputs = (text_embeddings, noise)\n",
    "                with torch.no_grad():\n",
    "                    low_res, _, _ = nn.parallel.data_parallel(stage1_G, s1_inputs, gpus)\n",
    "                # pass stage 1 output to generator\n",
    "                s2_inputs = (text_embeddings, low_res.detach())\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, s2_inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 256, f\"Image size {fake_images.shape[-1]} differs from 256\"\n",
    "\n",
    "            rlabels = real_labels.clone()\n",
    "            flabels = fake_labels.clone()\n",
    "            # label smoothing\n",
    "            if config.label_smoothening:\n",
    "                r = np.random.rand(1)[0]\n",
    "                if r <= 0.5:\n",
    "                    smoothening = np.random.choice(a=np.linspace(0., 0.20, num=5), replace=True, size=batch_size)\n",
    "                    smoothening = torch.tensor(smoothening).float().cuda()\n",
    "                    rlabels -= smoothening\n",
    "                    flabels += smoothening\n",
    "                # occasionally flip labels\n",
    "                else:\n",
    "                    rlabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.05,0.95])\n",
    "                    flabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.95,0.05])\n",
    "                    rlabels = torch.tensor(rlabels).float().cuda()\n",
    "                    flabels = torch.tensor(flabels).float().cuda()\n",
    "\n",
    "            # Update discriminator network\n",
    "            netD.zero_grad()\n",
    "            errD, errD_real, errD_wrong, errD_fake = compute_discriminator_loss(netD, criterion, real_images, fake_images,\n",
    "                                                                                rlabels, flabels, text_embeddings, gpus)\n",
    "            errD.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netD.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerD.step()\n",
    "            D_loss += errD.item()\n",
    "            D_real_loss += errD_real\n",
    "            D_fake_loss += errD_fake\n",
    "            D_wrong_loss += errD_wrong\n",
    "\n",
    "            # Update generator network\n",
    "            netG.zero_grad()\n",
    "            errG_fake = compute_generator_loss(netD, criterion, fake_images, real_images, \n",
    "                                               rlabels, text_embeddings, gpus)\n",
    "            kl_loss = KL_loss(mu, logvar)\n",
    "            errG_total = errG_fake +  (KL_coeff * kl_loss)\n",
    "            if alpha_L1 > 0:\n",
    "                errG_L1 = L1_loss(L1Loss, fake_images, real_images)\n",
    "                errG_total += (alpha_L1 * errG_L1)\n",
    "            \n",
    "            # annealing KL_coeff and L1_loss\n",
    "            # if epoch -1 < 30:\n",
    "            #     kld_coeff = KL_coeff[epoch-1].item()\n",
    "            #     l1_coeff = alpha_l1[epoch-1].item()\n",
    "            # else:\n",
    "            #     kld_coeff = KL_coeff[-1].item()\n",
    "            #     l1_coeff = alpha_l1[-1].item()\n",
    "\n",
    "            errG_total.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netG.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerG.step()\n",
    "            G_loss += errG_total.item()\n",
    "            KL_l += kl_loss.item()\n",
    "\n",
    "            running_count += 1\n",
    "            if i%100 == 0:\n",
    "                print('[%d/%d] [%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, i, len(trainloader), errD.item(), errG_total.item(), kl_loss.item()))\n",
    "                save_img_results(real_images, fake_images, low_res,  caption, epoch, config.out_img)\n",
    "\n",
    "        summary.add_scalars('Discriminator', {'DLoss':D_loss/len(trainloader), \n",
    "                                              'RealLoss':D_real_loss/len(trainloader), \n",
    "                                              'FakeLoss':D_fake_loss/len(trainloader), \n",
    "                                              'WrongLoss':D_wrong_loss/len(trainloader)}, epoch)\n",
    "        summary.add_scalars('Generator', {'GLoss':G_loss/len(trainloader), \n",
    "                                          'KL_Loss':KL_l/len(trainloader)}, epoch)\n",
    "        # summary.add_scalars('Grad_Norm', {'D':np.mean(d_grad_norm),\n",
    "        #                                   'G':np.mean(g_grad_norm)}, epoch) \n",
    "        print('[%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, D_loss/len(trainloader), G_loss/len(trainloader), KL_l/len(trainloader)))\n",
    "        \n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        if epoch % config.save_snapshot == 0:\n",
    "            save_model(netG, netD, epoch, config.model_out, stage=stage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# call model.eval() before feeding the data, as this will change the behavior of the BatchNorm layer \n",
    "# to use the running estimates instead of calculating them for the current batch\n",
    "# \"\"\"\n",
    "# netG1.eval()\n",
    "# netD1.eval()\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage=1\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage=2\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba9bc282ea7dd8acf6b93a88ab047ea17bba2d98cff2c21ca6cffa26ac4d8f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
