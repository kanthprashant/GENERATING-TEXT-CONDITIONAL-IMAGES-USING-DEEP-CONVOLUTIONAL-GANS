import torch

class config:
    cuda_is_available = torch.cuda.is_available()
    generator_dim = 192
    in_dims = 4 #8 # BxCx8x8
    n_resblocks = [0,0,0,0,0] # [0,0,1,0,0,0]
    channel_mul = [1, 2, 4, 8, 16] # [1, 2, 4, 8, 16, 32]
    n_resblocks_stage2 = [4,0,0,0,0]
    channel_mul_stage2 = [1, 2, 4, 8, 16]
    dropout = False
    dropout2 = False
    use_deconv = False
    use_deconv2 = False
    attention_resolutions = [] # [32]
    att_logits = False  # if used in discriminator
    attention_heads = 2
    discriminator_channel = 64 #96
    text_dim= 768 # if bert 768 elif clip 512
    condition_dim = 128 # 512
    z_dim = 100 # 256

    clip_grad = False
    label_smoothening = True
    g_lr = 2e-4 #2e-4
    d_lr = 2e-4 #2e-4
    lr_decay_epoch = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]
    lr_gamma = 0.5
    batch_size = 32 #64
    max_epoch = 500 #200
    imageSize = 64 #256
    trainImageListPath = "train_test_split/train_images.pkl"
    trainCaptionsListPath = "train_test_split/train_captions.pkl"
    imageListPath = "imageList.pkl"
    captionsListPath = "captionList.pkl"
    test_size = 0.1
    text_encoder = "distilbert-base-uncased"    # "distilbert-base-uncased" or "openai/clip-vit-base-patch32"
    dataset = "caltech-ucsd-bird/CUB_200_2011"
    imageFolder = "images"
    alpha_L1 = 0
    KL_COEFF = 2.0
    VIS_COUNT = 16
    out_img = "bert/bert_out_V/results_s2_bert_onels_1"
    save_snapshot = 20
    model_out = "bert/bert_out_V/checkpoint_s2_bert_onels_1"
    tb_dir = "bert/bert_out_V/tensorboard_s2_bert_onels_1" # "bert/bert_out_III/tensorboard_s1_bert_con"
    load_checkpoint = False
    gen1_ckpt = "bert/bert_out_V/checkpoint_s1_bert_ls/netG1_epoch_460.pth"
    d1_ckpt = ""
    gen2_ckpt = ""
    d2_ckpt = ""

class config2:
    cuda_is_available = torch.cuda.is_available()
    generator_dim = 192
    in_dims = 4 #8 # BxCx8x8
    n_resblocks = [0,0,0,0,0] # [0,0,1,0,0,0]
    channel_mul = [1, 2, 4, 8, 16] # [1, 2, 4, 8, 16, 32]
    n_resblocks_stage2 = [4,0,0,0,0]
    channel_mul_stage2 = [1, 2, 4, 8, 16]
    dropout = False
    dropout2 = False
    use_deconv = False
    use_deconv2 = False
    attention_resolutions = [] # [32]
    att_logits = False  # if used in discriminator
    attention_heads = 2
    discriminator_channel = 64 #96
    text_dim= 512 # if bert 768 elif clip 512
    condition_dim = 128 # 512
    z_dim = 100 # 256

    clip_grad = False
    label_smoothening = True
    g_lr = 2e-4
    d_lr = 2e-4
    lr_decay_epoch = [100, 200, 300, 400]
    lr_gamma = 0.5
    batch_size = 16
    max_epoch = 400 #200
    imageSize = 64 #256
    trainImageListPath = "train_test_split/train_images.pkl"
    trainCaptionsListPath = "train_test_split/train_captions.pkl"
    imageListPath = "imageList.pkl"
    captionsListPath = "captionList.pkl"
    test_size = 0.1
    text_encoder = "openai/clip-vit-base-patch32"    # "distilbert-base-uncased" or "openai/clip-vit-base-patch32"
    dataset = "caltech-ucsd-bird/CUB_200_2011"
    imageFolder = "images"
    alpha_L1 = 0
    KL_COEFF = 1.0
    VIS_COUNT = 16
    out_img = "clip/clip_out_VI/results_s2_clip_onels_1biasF"
    save_snapshot = 20
    model_out = "clip/clip_out_VI/checkpoint_s2_clip_onels_1biasF"
    tb_dir = "clip/clip_out_VI/tensorboard_s2_clip_onels_1biasF"
    load_checkpoint = False
    gen1_ckpt = "clip/clip_out_VI/checkpoint_s1_clip_onels_1biasF/netG1_epoch_400.pth"
    d1_ckpt = ""
    gen2_ckpt = ""
    d2_ckpt = ""

class config3:
    cuda_is_available = torch.cuda.is_available()
    generator_dim = 192
    in_dims = 4 #8 # BxCx8x8
    n_resblocks = [0,0,0,0,0] # [0,0,1,0,0,0]
    channel_mul = [1, 2, 4, 8, 16] # [1, 2, 4, 8, 16, 32]
    n_resblocks_stage2 = [4,0,0,0,0]
    channel_mul_stage2 = [1, 2, 4, 8, 16]
    dropout = False
    dropout2 = False
    use_deconv = False
    use_deconv2 = False
    attention_resolutions = [] # [32]
    att_logits = False  # if used in discriminator
    attention_heads = 2
    discriminator_channel = 64 #96
    text_dim= 1024 # if bert 768 elif clip 512 else text-cnn-rnn 1024
    condition_dim = 128 # 512
    z_dim = 100 # 256

    clip_grad = False
    label_smoothening = True
    g_lr = 2e-4
    d_lr = 2e-4
    lr_decay_epoch = [100, 200, 300, 400, 500]
    lr_gamma = 0.5
    batch_size = 16
    max_epoch = 600 #200
    imageSize = 64 #256
    trainImageListPath = "train_test_split/text-cnn-rnn/train_images.pkl"
    trainCaptionsListPath = "train_test_split/text-cnn-rnn/train_captions.pkl"
    trainEmbeddingsListPath = "train_test_split/text-cnn-rnn/train_embeddings.pkl"
    imageListPath = "imageList.pkl"
    captionsListPath = "captionList.pkl"
    test_size = 0.1
    text_encoder = "text-cnn-rnn"    # "distilbert-base-uncased" or "openai/clip-vit-base-patch32"
    dataset = "caltech-ucsd-bird/CUB_200_2011"
    imageFolder = "images"
    alpha_L1 = 0
    KL_COEFF = 1.0
    VIS_COUNT = 16
    out_img = "text-cnn-rnn/out_II/results_s2_ls_biasF_mu"
    save_snapshot = 20
    model_out = "text-cnn-rnn/out_II/checkpoint_s2_ls_biasF_mu"
    tb_dir = "text-cnn-rnn/out_II/tensorboard_s2_ls_biasF_mu"
    load_checkpoint = False
    gen1_ckpt = "text-cnn-rnn/out_II/checkpoint_s1_ls_biasF/netG1_epoch_600.pth"
    d1_ckpt = ""
    gen2_ckpt = ""
    d2_ckpt = ""

## Wassersteinn GAN
class config4:
    cuda_is_available = torch.cuda.is_available()
    generator_dim = 192
    in_dims = 4 #8 # BxCx8x8
    n_resblocks = [0,0,0,0,0] # [0,0,1,0,0,0]
    channel_mul = [1, 2, 4, 8, 16] # [1, 2, 4, 8, 16, 32]
    n_resblocks_stage2 = [4,0,0,0,0]
    channel_mul_stage2 = [1, 2, 4, 8, 16]
    dropout = False
    dropout2 = False
    use_deconv = False
    use_deconv2 = False
    attention_resolutions = [] # [32]
    att_logits = False  # if used in discriminator
    attention_heads = 2
    discriminator_channel = 64 #96
    text_dim= 1024 # if bert 768 elif clip 512 else text-cnn-rnn 1024
    condition_dim = 128 # 512
    z_dim = 100 # 256

    n_critic = 5
    lamda = 10
    g_lr = 2e-4
    d_lr = 2e-4
    batch_size = 32
    max_epoch = 600 #200
    imageSize = 64 #256
    trainImageListPath = "train_test_split/text-cnn-rnn/train_images.pkl"
    trainCaptionsListPath = "train_test_split/text-cnn-rnn/train_captions.pkl"
    trainEmbeddingsListPath = "train_test_split/text-cnn-rnn/train_embeddings.pkl"
    imageListPath = "imageList.pkl"
    captionsListPath = "captionList.pkl"
    test_size = 0.1
    text_encoder = "text-cnn-rnn"    # "distilbert-base-uncased" or "openai/clip-vit-base-patch32"
    dataset = "caltech-ucsd-bird/CUB_200_2011"
    imageFolder = "images"
    VIS_COUNT = 16
    out_img = "text-cnn-rnn/out_III/results_s2_wGAN"
    save_snapshot = 20
    model_out = "text-cnn-rnn/out_III/checkpoint_s2_wGAN"
    tb_dir = "text-cnn-rnn/out_III/tensorboard_s2_wGAN"
    load_checkpoint = False
    gen1_ckpt = "text-cnn-rnn/out_III/checkpoint_s1_wGAN/netG1_epoch_1000.pth"
    d1_ckpt = ""
    gen2_ckpt = ""
    d2_ckpt = ""