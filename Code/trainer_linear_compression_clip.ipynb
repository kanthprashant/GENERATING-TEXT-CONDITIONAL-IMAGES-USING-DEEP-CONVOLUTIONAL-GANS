{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/common/users/ppk31/CS543_DL_Proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from configs import config2 as config\n",
    "from pytorch_model_summary import summary\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import (weights_init, make_train_test_split, load_data, compute_discriminator_loss, \n",
    "                   compute_generator_loss, KL_loss, L1_loss, save_img_results, save_model, load_from_checkpoint)\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from dataset import Text2ImgDataset, Text2ImgDataset_reformed\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "import traceback\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DATASET = '/freespace/local/ppk31_cs543/Project/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.text_encoder == \"distilbert-base-uncased\":\n",
    "    from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "elif config.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "    from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: True\n",
      "GPU ids: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"using GPU: {torch.cuda.is_available()}\")\n",
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print(f\"GPU ids: {gpus}\")\n",
    "\n",
    "torch.random.seed()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.set_device(gpus[0])\n",
    "cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(text_encoder):\n",
    "    print(f\"using {text_encoder} as text encoder\")\n",
    "    if text_encoder == \"distilbert-base-uncased\":\n",
    "        return DistilBertTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "        return CLIPTokenizer.from_pretrained(text_encoder)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, pretrained=True):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if text_encoder == \"distilbert-base-uncased\":\n",
    "            self.encoder = DistilBertModel.from_pretrained(text_encoder)\n",
    "        elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            self.encoder = CLIPModel.from_pretrained(text_encoder)\n",
    "        # self.text_embedding = 768\n",
    "        # self.projection = ProjectionHead('text_projector', self.text_embedding, project_dim)\n",
    "        self.retrieve_token_index = 0\n",
    "    \n",
    "    def forward(self, input_tokens, attention_mask):\n",
    "        if self.text_encoder == \"distilbert-base-uncased\":\n",
    "            out = self.encoder(input_ids = input_tokens, attention_mask = attention_mask)\n",
    "            last_hidden_states = out.last_hidden_state\n",
    "            embeddings = last_hidden_states[:, self.retrieve_token_index, :]    # output_dimensions = 768\n",
    "        elif self.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            embeddings = self.encoder.get_text_features(input_ids = input_tokens, attention_mask = attention_mask) # output_dimensions = 512\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmented_Projection(nn.Module):\n",
    "    def __init__(self, stage, gen_channels, gen_dim):\n",
    "        super(Augmented_Projection, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.t_dim = config.text_dim\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.z_dim = config.z_dim\n",
    "        self.gen_in = gen_channels #config.generator_dim * gen_dim\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stage == 1:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(self.c_dim + self.z_dim, self.gen_in * gen_dim * gen_dim, bias=False), # bias=False, # 768 -> 192*8*8*8\n",
    "                nn.BatchNorm1d(self.gen_in * gen_dim * gen_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def augment(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        # if config.cuda_is_available:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_().cuda()\n",
    "        # else:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(torch.randn(std.size()).float().cuda())\n",
    "        # eps = Variable(eps)\n",
    "        # eps.mul(std).add(mu)\n",
    "        return mu + (std * eps)\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        if noise is None and self.stage==1:\n",
    "            noise = torch.randn((text_embedding.shape[0], self.z_dim)).float().cuda()\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        c_code = self.augment(mu, logvar)\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            c_code = torch.cat((c_code, noise), dim=1)\n",
    "            c_code = self.project(c_code)\n",
    "        \n",
    "        return c_code, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, kernel_size=4, stride=2, padding=1, batch_norm=True, activation=True, use_conv=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, stride=1, padding=1, batch_norm=True, activation=True, bias=False, use_deconv=False, dropout=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.use_deconv = use_deconv\n",
    "\n",
    "        if use_deconv:\n",
    "            self.deconv = nn.ConvTranspose2d(self.channels, self.out_channels, kernel_size=4, stride=2, padding=padding, bias=bias) # use when not using interpolate\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=stride, padding=padding, bias = bias)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.ReLU()\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.use_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param in_channels: the number of input channels.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        stride = 1,\n",
    "        padding = 1\n",
    "    ):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        if in_channels == out_channels:\n",
    "                self.x_residual = nn.Identity()\n",
    "        else:\n",
    "            self.x_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.x_residual(x)\n",
    "        h = x + g\n",
    "        return self.relu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    :param channels: is the number of channels in the feature map\n",
    "    :param n_heads: is the number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, n_heads=1, cond_channels=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        assert (\n",
    "            channels % n_heads == 0\n",
    "        ), f\"q,k,v channels {channels//n_heads} cannot be constructed for {n_heads} heads, input channels: {channels}\"\n",
    "        self.n_heads = n_heads\n",
    "        # self.norm1 = nn.GroupNorm(num_groups=16, num_channels=channels, eps=1e-6, affine=True) # num_groups=32\n",
    "        # self.norm1 = nn.BatchNorm2d(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels*3, kernel_size=1)\n",
    "        self.attention = QKVAttention(self.n_heads)\n",
    "        if cond_channels is not None:\n",
    "            # self.norm2 = nn.GroupNorm(num_groups=16, num_channels=cond_channels, eps=1e-6, affine=True) # num_groups=32\n",
    "            # self.norm2 = nn.BatchNorm2d(cond_channels)\n",
    "            self.cond_kv = nn.Conv2d(cond_channels, channels*2, kernel_size=1)\n",
    "        # self.proj_out = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "    def forward(self, x, cond_out = None):\n",
    "        b, c, *spatial = x.shape\n",
    "        h, w = spatial\n",
    "        # qkv = self.qkv(self.norm1(x).view(b, c, -1)) # b, c*3, h*w\n",
    "        qkv = self.qkv(x).view(b, -1, h*w) # b, c*3, h*w\n",
    "        # qkv = self.qkv(x.view(b, c, -1)) # b, c*3, h*w\n",
    "        if cond_out is not None:\n",
    "            _, cc, *hw = cond_out.shape\n",
    "            hh, ww = hw\n",
    "            # cond_out = self.cond_kv(self.norm2(cond_out).view(b, cc, -1))\n",
    "            cond_out = self.cond_kv(cond_out).view(b, -1, hh*ww)\n",
    "            h = self.attention(qkv, cond_out)\n",
    "        else:\n",
    "            h = self.attention(qkv)\n",
    "        # h = self.proj_out(h)\n",
    "        return x + h.reshape(b, c, *spatial)\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "    def forward(self, qkv, cond_kv=None):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads) # no. of channels for q,k,v for each head\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        if cond_kv is not None:\n",
    "            assert cond_kv.shape[1] == self.n_heads * ch * 2\n",
    "            ek, ev = cond_kv.reshape(bs * self.n_heads, ch * 2, -1).split(ch, dim=1)\n",
    "            k = torch.cat([ek, k], dim=-1)\n",
    "            v = torch.cat([ev, v], dim=-1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims # 4\n",
    "        self.in_channels = config.generator_dim * 8 # 192*8\n",
    "        self.channel_mul = config.channel_mul\n",
    "        self.num_resblocks = config.n_resblocks\n",
    "        self.use_deconv=config.use_deconv \n",
    "        self.dropout=config.dropout\n",
    "        ch = self.in_channels\n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding, noise)\n",
    "        x = proj_x.view(-1, self.in_channels, self.in_dims, self.in_dims)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims * config.in_dims # 16\n",
    "        self.in_channels = config.generator_dim # 192\n",
    "        self.channel_mul = config.channel_mul_stage2\n",
    "        self.num_resblocks = config.n_resblocks_stage2\n",
    "        self.use_deconv=config.use_deconv2 \n",
    "        self.dropout=config.dropout2\n",
    "        ch = self.in_channels * 4 \n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "        \n",
    "        self.downblocks= nn.Sequential(\n",
    "            Downsample(3, self.in_channels, kernel_size=3, stride=1, padding=1, batch_norm=False),\n",
    "            Downsample(self.in_channels, self.in_channels*2),\n",
    "            Downsample(self.in_channels*2, self.in_channels*4)\n",
    "        )\n",
    "        self.combined = nn.Sequential(\n",
    "            Downsample(self.in_channels*4 + self.c_dim, self.in_channels*4, kernel_size=3, stride=1, padding=1) # 768 x 16 x 16\n",
    "        )\n",
    "            \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout if layer<2 else False))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        print(\"Initialized stage2 Generator\")\n",
    "        \n",
    "    def forward(self, text_embedding, stage1_out):\n",
    "        enc_img = self.downblocks(stage1_out)\n",
    "        \n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding)\n",
    "        x = proj_x.view(-1, self.c_dim, 1, 1)\n",
    "        x = x.repeat(1, 1, self.in_dims, self.in_dims)\n",
    "        x = torch.cat([enc_img, x], dim=1)\n",
    "        x = self.combined(x)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, d_ch, c_dim, txt_dim, condition):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.condition = condition\n",
    "        self.d_ch = d_ch\n",
    "        self.c_dim = c_dim\n",
    "        self.txt_dim = txt_dim\n",
    "\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8+self.c_dim, n_heads=2, cond_channels=self.c_dim)\n",
    "        # self.conv1 = Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8, n_heads=1)\n",
    "\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Linear(self.txt_dim, self.c_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if condition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                # Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, feat, cond_out=None):\n",
    "        if self.condition:\n",
    "            ### compress text_embeddings using a linear layer\n",
    "            cond_out = self.compress(cond_out)\n",
    "            ### reshape\n",
    "            cond_out = cond_out.view(-1, self.c_dim, 1, 1)\n",
    "            cond_out = cond_out.repeat(1, 1, 4, 4) # (1, 1, 8, 8) (1,1,config.in_dims,config.in_dims)\n",
    "            x = torch.cat((feat, cond_out), 1)\n",
    "        else:\n",
    "            x = feat\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.attention(feat, cond_out)\n",
    "        # x = self.attention(x)\n",
    "        out = self.outlogits(x)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.d_ch = config.discriminator_channel\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.txt_dim = config.text_dim\n",
    "        # self.att_logits = config.att_logits\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            Downsample(3, self.d_ch, batch_norm=False),\n",
    "            Downsample(self.d_ch, self.d_ch*2),\n",
    "            Downsample(self.d_ch*2, self.d_ch*4),\n",
    "            Downsample(self.d_ch*4, self.d_ch*8),\n",
    "        )\n",
    "\n",
    "        if stage == 2:\n",
    "            self.encode_further = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, self.d_ch*16),\n",
    "                Downsample(self.d_ch*16, self.d_ch*32),\n",
    "                Downsample(self.d_ch*32, self.d_ch*16, kernel_size=3, stride=1, padding=1),\n",
    "                Downsample(self.d_ch*16, self.d_ch*8, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        self.cond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, self.txt_dim, condition=True)\n",
    "        self.uncond_discriminator_logits = None\n",
    "        # if self.stage == 2:\n",
    "        #     self.uncond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, condition=False)\n",
    "        print(\"Initialized, stage {} discriminator\".format(stage))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        if self.stage == 2:\n",
    "            x = self.encode_further(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(stage, batch_size, random_captions=True):\n",
    "        imageSize = None\n",
    "        if stage == 1:\n",
    "                imageSize = config.imageSize # 64\n",
    "        else:\n",
    "                imageSize = config.imageSize * 4  # 64*4 = 256\n",
    "\n",
    "        print(f\"Genearting Dataset with image size: {imageSize}\")\n",
    "\n",
    "        tokenizer = get_tokenizer(config.text_encoder)\n",
    "\n",
    "        imageFolder = os.path.join(DATASET, config.dataset, config.imageFolder) # check these params in config before running\n",
    "\n",
    "        if random_captions:\n",
    "                train_images, train_captions = load_data(imageListPath=config.trainImageListPath, captionsListPath=config.trainCaptionsListPath)\n",
    "                train_dataset = Text2ImgDataset_reformed(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage=False)\n",
    "        else:\n",
    "                imageListPath = os.path.join(DATASET, config.dataset, config.imageListPath) # check these params in config before running\n",
    "                captionsListPath = os.path.join(DATASET, config.dataset, config.captionsListPath) # check these params in config before running\n",
    "                train_images, train_captions, test_images, test_captions = make_train_test_split(imageListPath, captionsListPath, config.test_size)\n",
    "                train_dataset = Text2ImgDataset(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage = False) # change based on stage -imagesize\n",
    "        \n",
    "        print(\"Dataset created:\\n\\\n",
    "                length of train dataset: {}\\n\".format(len(train_dataset)))\n",
    "\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f31b37f5940>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.tb_dir, exist_ok=True)       # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.model_out, exist_ok=True)    # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.out_img, exist_ok=True)      # change these in config when training stage1 and stage2 accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_controllers(netD, netG):\n",
    "    d_lr = config.d_lr\n",
    "    g_lr = config.g_lr\n",
    "    # We create the optimizer object of the discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = d_lr, betas = (0.5, 0.999))\n",
    "    scheduler_D = MultiStepLR(optimizerD, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True) \n",
    "    # We create the optimizer object of the generator.\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = g_lr, betas = (0.5, 0.999)) \n",
    "    scheduler_G = MultiStepLR(optimizerG, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    L1Loss = nn.L1Loss()\n",
    "\n",
    "    return optimizerD, scheduler_D, optimizerG, scheduler_G, criterion, L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "def train(stage, batch_size, trainloader):\n",
    "\n",
    "    noise_dim = config.z_dim\n",
    "    noise = Variable(torch.FloatTensor(batch_size, noise_dim).float().cuda())\n",
    "    real_labels = Variable(torch.ones(batch_size).float().cuda())\n",
    "    fake_labels = Variable(torch.zeros(batch_size).float().cuda())\n",
    "\n",
    "    assert batch_size == real_labels.shape[0], \"batch_size and target size do not match in real_labels\"\n",
    "    assert batch_size == fake_labels.shape[0], \"batch_size and target size do not match in fake_labels\"\n",
    "\n",
    "    text_encoder = TextEncoder(config.text_encoder, pretrained=True)\n",
    "    text_encoder.eval()\n",
    "\n",
    "    # stage 1 training, only stage 1 g and stage1 d\n",
    "    if stage == 1:\n",
    "        netG = Generator1(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    # stage 2 training, stage1 g output is fed to stage2 g, stage2 d\n",
    "    else:\n",
    "        stage1_G = Generator1(1)\n",
    "        stage1_G = load_from_checkpoint(stage1_G, config.gen1_ckpt)\n",
    "        stage1_G.float().cuda()\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        stage1_G.eval()\n",
    "        netG = Generator2(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    recovered_epoch = 0\n",
    "    if config.load_checkpoint:\n",
    "        if stage == 1:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen1_ckpt, netD, config.d1_ckpt)\n",
    "        else:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen2_ckpt, netD, config.d2_ckpt)\n",
    "    else:\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "    netG.float().cuda()\n",
    "    netD.float().cuda()\n",
    "\n",
    "    optimizerD, schedulerD, optimizerG, schedulerG, criterion, L1Loss = get_controllers(netD, netG)\n",
    "\n",
    "    tb = 'stage' + str(stage) + '_b' + str(batch_size) + '_d' + (str(config.imageSize) if stage==1 else str(config.imageSize*4)) + '_' + str(recovered_epoch)\n",
    "    summary = SummaryWriter(os.path.join(config.tb_dir, tb))\n",
    "   \n",
    "    running_count = 0\n",
    "    # KL_coeff = torch.linspace(0., config.KL_COEFF, 30)\n",
    "    # alpha_l1, _ = torch.linspace(0., config.alpha_L1, 30).sort(descending=True)\n",
    "    KL_coeff = config.KL_COEFF\n",
    "    alpha_L1 = config.alpha_L1\n",
    "\n",
    "    print(f\"Traininig Stage: {stage}, outputs at: {config.tb_dir}, {config.out_img}, {config.model_out}\")\n",
    "\n",
    "    for epoch in range(recovered_epoch+1, config.max_epoch+1):\n",
    "        D_loss = 0\n",
    "        D_real_loss = 0\n",
    "        D_fake_loss = 0\n",
    "        D_wrong_loss = 0\n",
    "        G_loss = 0\n",
    "        KL_l = 0\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            with torch.no_grad():\n",
    "                text_embeddings = text_encoder(batch['input_ids'], batch['attention_mask'])\n",
    "            text_embeddings = text_embeddings.float().cuda()\n",
    "            real_images = batch['image']\n",
    "            caption = None\n",
    "            if 'caption' in batch:\n",
    "                caption = batch['caption']\n",
    "\n",
    "            noise.data.normal_(0,1)\n",
    "            # noise = torch.randn((batch_size, noise_dim)).float().cuda()\n",
    "            \n",
    "            low_res = None\n",
    "            if stage == 1:\n",
    "                # Generate fake image\n",
    "                inputs = (text_embeddings, noise)\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 64, f\"Image size {fake_images.shape[-1]} differs from 64\"\n",
    "            \n",
    "            else:\n",
    "                # Generate fake image\n",
    "                s1_inputs = (text_embeddings, noise)\n",
    "                with torch.no_grad():\n",
    "                    low_res, _, _ = nn.parallel.data_parallel(stage1_G, s1_inputs, gpus)\n",
    "                # pass stage 1 output to generator\n",
    "                s2_inputs = (text_embeddings, low_res.detach())\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, s2_inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 256, f\"Image size {fake_images.shape[-1]} differs from 256\"\n",
    "\n",
    "            rlabels = real_labels.clone()\n",
    "            flabels = fake_labels.clone()\n",
    "            # label smoothing and noisy labelling\n",
    "            if config.label_smoothening:\n",
    "                r = np.random.rand(1)[0]\n",
    "                if r <= 0.5:\n",
    "                    smoothening = np.random.choice(a=np.linspace(0., 0.20, num=5), replace=True, size=batch_size)\n",
    "                    smoothening = torch.tensor(smoothening).float().cuda()\n",
    "                    rlabels -= smoothening\n",
    "                    flabels += smoothening\n",
    "                # occasionally flip labels\n",
    "                else:\n",
    "                    rlabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.05,0.95])\n",
    "                    flabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.95,0.05])\n",
    "                    rlabels = torch.tensor(rlabels).float().cuda()\n",
    "                    flabels = torch.tensor(flabels).float().cuda()\n",
    "\n",
    "            # Update discriminator network\n",
    "            netD.zero_grad()\n",
    "            errD, errD_real, errD_wrong, errD_fake = compute_discriminator_loss(netD, criterion, real_images, fake_images,\n",
    "                                                                                rlabels, flabels, text_embeddings, gpus)\n",
    "            # errD.backward()\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            errD_fake_wrong = (errD_wrong + errD_fake) * 0.5\n",
    "            errD_fake_wrong.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netD.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerD.step()\n",
    "            D_loss += errD.item()\n",
    "            D_real_loss += errD_real\n",
    "            D_fake_loss += errD_fake\n",
    "            D_wrong_loss += errD_wrong\n",
    "\n",
    "            # Update generator network\n",
    "            netG.zero_grad()\n",
    "            errG_fake = compute_generator_loss(netD, criterion, fake_images, real_images, \n",
    "                                               real_labels, text_embeddings, gpus)\n",
    "            kl_loss = KL_loss(mu, logvar)\n",
    "            errG_total = errG_fake +  (KL_coeff * kl_loss)\n",
    "            if alpha_L1 > 0:\n",
    "                errG_L1 = L1_loss(L1Loss, fake_images, real_images)\n",
    "                errG_total += (alpha_L1 * errG_L1)\n",
    "            \n",
    "            # annealing KL_coeff and L1_loss\n",
    "            # if epoch -1 < 30:\n",
    "            #     kld_coeff = KL_coeff[epoch-1].item()\n",
    "            #     l1_coeff = alpha_l1[epoch-1].item()\n",
    "            # else:\n",
    "            #     kld_coeff = KL_coeff[-1].item()\n",
    "            #     l1_coeff = alpha_l1[-1].item()\n",
    "\n",
    "            errG_total.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netG.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerG.step()\n",
    "            G_loss += errG_total.item()\n",
    "            KL_l += kl_loss.item()\n",
    "\n",
    "            running_count += 1\n",
    "            if i%100 == 0:\n",
    "                print('[%d/%d] [%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, i, len(trainloader), errD.item(), errG_total.item(), kl_loss.item()))\n",
    "                save_img_results(real_images, fake_images, low_res,  caption, epoch, config.out_img)\n",
    "\n",
    "        summary.add_scalars('Discriminator', {'DLoss':D_loss/len(trainloader), \n",
    "                                              'RealLoss':D_real_loss/len(trainloader), \n",
    "                                              'FakeLoss':D_fake_loss/len(trainloader), \n",
    "                                              'WrongLoss':D_wrong_loss/len(trainloader)}, epoch)\n",
    "        summary.add_scalars('Generator', {'GLoss':G_loss/len(trainloader), \n",
    "                                          'KL_Loss':KL_l/len(trainloader)}, epoch)\n",
    "        # summary.add_scalars('Grad_Norm', {'D':np.mean(d_grad_norm),\n",
    "        #                                   'G':np.mean(g_grad_norm)}, epoch) \n",
    "        print('[%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, D_loss/len(trainloader), G_loss/len(trainloader), KL_l/len(trainloader)))\n",
    "        \n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        if epoch % config.save_snapshot == 0:\n",
    "            save_model(netG, netD, epoch, config.model_out, stage=stage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# call model.eval() before feeding the data, as this will change the behavior of the BatchNorm layer \n",
    "# to use the running estimates instead of calculating them for the current batch\n",
    "# \"\"\"\n",
    "# netG1.eval()\n",
    "# netD1.eval()\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 64\n",
      "using openai/clip-vit-base-patch32 as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 11199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=1\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, stage 1 discriminator\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Traininig Stage: 1, outputs at: clip/clip_out_VI/tensorboard_s2_clip_onels_1biasF, clip/clip_out_VI/results_s1_clip_onels_1biasF, clip/clip_out_VI/checkpoint_s2_clip_onels_1biasF\n",
      "[1/400] [0/349] Loss_D: 1.56134, Loss_G: 2.88431, Loss_KL: 0.01358\n",
      "[1/400] [100/349] Loss_D: 1.45109, Loss_G: 1.90724, Loss_KL: 0.00734\n",
      "[1/400] [200/349] Loss_D: 1.35805, Loss_G: 1.75431, Loss_KL: 0.01299\n",
      "[1/400] [300/349] Loss_D: 1.22572, Loss_G: 1.29255, Loss_KL: 0.03477\n",
      "[1/400] Loss_D: 1.43336, Loss_G: 2.20976, Loss_KL: 0.01748\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[2/400] [0/349] Loss_D: 1.41329, Loss_G: 0.73867, Loss_KL: 0.03777\n",
      "[2/400] [100/349] Loss_D: 1.29519, Loss_G: 1.31732, Loss_KL: 0.03257\n",
      "[2/400] [200/349] Loss_D: 1.11210, Loss_G: 2.03686, Loss_KL: 0.03934\n",
      "[2/400] [300/349] Loss_D: 1.10805, Loss_G: 1.69393, Loss_KL: 0.08831\n",
      "[2/400] Loss_D: 1.31323, Loss_G: 1.53875, Loss_KL: 0.05100\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[3/400] [0/349] Loss_D: 1.60003, Loss_G: 1.03207, Loss_KL: 0.12264\n",
      "[3/400] [100/349] Loss_D: 1.47621, Loss_G: 1.52389, Loss_KL: 0.12619\n",
      "[3/400] [200/349] Loss_D: 1.26911, Loss_G: 1.89486, Loss_KL: 0.13469\n",
      "[3/400] [300/349] Loss_D: 1.22492, Loss_G: 2.75250, Loss_KL: 0.12607\n",
      "[3/400] Loss_D: 1.28825, Loss_G: 1.72068, Loss_KL: 0.12673\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[4/400] [0/349] Loss_D: 1.24187, Loss_G: 1.91561, Loss_KL: 0.11002\n",
      "[4/400] [100/349] Loss_D: 1.36770, Loss_G: 1.79711, Loss_KL: 0.10485\n",
      "[4/400] [200/349] Loss_D: 1.26230, Loss_G: 1.60581, Loss_KL: 0.08548\n",
      "[4/400] [300/349] Loss_D: 1.21388, Loss_G: 1.78944, Loss_KL: 0.08674\n",
      "[4/400] Loss_D: 1.23148, Loss_G: 1.68189, Loss_KL: 0.09588\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[5/400] [0/349] Loss_D: 1.19103, Loss_G: 2.66755, Loss_KL: 0.07107\n",
      "[5/400] [100/349] Loss_D: 1.21713, Loss_G: 1.29931, Loss_KL: 0.08092\n",
      "[5/400] [200/349] Loss_D: 1.25367, Loss_G: 1.96152, Loss_KL: 0.06061\n",
      "[5/400] [300/349] Loss_D: 1.24617, Loss_G: 0.91440, Loss_KL: 0.04890\n",
      "[5/400] Loss_D: 1.23769, Loss_G: 1.42765, Loss_KL: 0.06587\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[6/400] [0/349] Loss_D: 1.25128, Loss_G: 1.09555, Loss_KL: 0.07460\n",
      "[6/400] [100/349] Loss_D: 1.25505, Loss_G: 1.60460, Loss_KL: 0.06262\n",
      "[6/400] [200/349] Loss_D: 1.37726, Loss_G: 1.85919, Loss_KL: 0.08170\n",
      "[6/400] [300/349] Loss_D: 1.14910, Loss_G: 1.17145, Loss_KL: 0.07065\n",
      "[6/400] Loss_D: 1.22347, Loss_G: 1.35954, Loss_KL: 0.07365\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[7/400] [0/349] Loss_D: 1.07862, Loss_G: 1.17491, Loss_KL: 0.07396\n",
      "[7/400] [100/349] Loss_D: 1.14396, Loss_G: 1.91816, Loss_KL: 0.09921\n",
      "[7/400] [200/349] Loss_D: 1.34655, Loss_G: 1.33102, Loss_KL: 0.07232\n",
      "[7/400] [300/349] Loss_D: 1.21687, Loss_G: 1.02770, Loss_KL: 0.08372\n",
      "[7/400] Loss_D: 1.21125, Loss_G: 1.36113, Loss_KL: 0.07090\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[8/400] [0/349] Loss_D: 1.23805, Loss_G: 2.03764, Loss_KL: 0.07867\n",
      "[8/400] [100/349] Loss_D: 1.17162, Loss_G: 1.02579, Loss_KL: 0.05892\n",
      "[8/400] [200/349] Loss_D: 1.24391, Loss_G: 0.84940, Loss_KL: 0.05981\n",
      "[8/400] [300/349] Loss_D: 1.06606, Loss_G: 0.94740, Loss_KL: 0.07713\n",
      "[8/400] Loss_D: 1.18815, Loss_G: 1.36384, Loss_KL: 0.08040\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[9/400] [0/349] Loss_D: 1.30049, Loss_G: 1.32016, Loss_KL: 0.07358\n",
      "[9/400] [100/349] Loss_D: 1.01531, Loss_G: 1.55403, Loss_KL: 0.05895\n",
      "[9/400] [200/349] Loss_D: 1.15118, Loss_G: 1.52569, Loss_KL: 0.08546\n",
      "[9/400] [300/349] Loss_D: 1.26130, Loss_G: 1.32847, Loss_KL: 0.07659\n",
      "[9/400] Loss_D: 1.16797, Loss_G: 1.38927, Loss_KL: 0.07900\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[10/400] [0/349] Loss_D: 1.16048, Loss_G: 1.02661, Loss_KL: 0.07004\n",
      "[10/400] [100/349] Loss_D: 1.09598, Loss_G: 2.34758, Loss_KL: 0.06612\n",
      "[10/400] [200/349] Loss_D: 1.14592, Loss_G: 1.19218, Loss_KL: 0.08425\n",
      "[10/400] [300/349] Loss_D: 1.07799, Loss_G: 1.26033, Loss_KL: 0.09596\n",
      "[10/400] Loss_D: 1.16143, Loss_G: 1.38786, Loss_KL: 0.08469\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[11/400] [0/349] Loss_D: 1.12998, Loss_G: 2.17449, Loss_KL: 0.09697\n",
      "[11/400] [100/349] Loss_D: 1.08120, Loss_G: 1.77255, Loss_KL: 0.09481\n",
      "[11/400] [200/349] Loss_D: 0.99817, Loss_G: 1.85790, Loss_KL: 0.08078\n",
      "[11/400] [300/349] Loss_D: 1.22416, Loss_G: 1.16927, Loss_KL: 0.08829\n",
      "[11/400] Loss_D: 1.14959, Loss_G: 1.41129, Loss_KL: 0.08687\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[12/400] [0/349] Loss_D: 0.85121, Loss_G: 1.24116, Loss_KL: 0.08684\n",
      "[12/400] [100/349] Loss_D: 1.24175, Loss_G: 1.72523, Loss_KL: 0.08197\n",
      "[12/400] [200/349] Loss_D: 1.28167, Loss_G: 1.03242, Loss_KL: 0.09935\n",
      "[12/400] [300/349] Loss_D: 1.03520, Loss_G: 1.29630, Loss_KL: 0.08625\n",
      "[12/400] Loss_D: 1.13237, Loss_G: 1.41219, Loss_KL: 0.08462\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[13/400] [0/349] Loss_D: 1.12225, Loss_G: 1.72817, Loss_KL: 0.08451\n",
      "[13/400] [100/349] Loss_D: 1.21586, Loss_G: 2.02721, Loss_KL: 0.10447\n",
      "[13/400] [200/349] Loss_D: 1.35092, Loss_G: 0.78870, Loss_KL: 0.08469\n",
      "[13/400] [300/349] Loss_D: 0.99548, Loss_G: 1.49281, Loss_KL: 0.08480\n",
      "[13/400] Loss_D: 1.09205, Loss_G: 1.49756, Loss_KL: 0.09380\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[14/400] [0/349] Loss_D: 1.23240, Loss_G: 1.65954, Loss_KL: 0.07798\n",
      "[14/400] [100/349] Loss_D: 1.13575, Loss_G: 2.16305, Loss_KL: 0.07712\n",
      "[14/400] [200/349] Loss_D: 1.31441, Loss_G: 2.39824, Loss_KL: 0.10232\n",
      "[14/400] [300/349] Loss_D: 1.13431, Loss_G: 1.59990, Loss_KL: 0.08344\n",
      "[14/400] Loss_D: 1.08379, Loss_G: 1.51029, Loss_KL: 0.09103\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[15/400] [0/349] Loss_D: 1.11271, Loss_G: 1.18427, Loss_KL: 0.11003\n",
      "[15/400] [100/349] Loss_D: 0.94591, Loss_G: 1.85247, Loss_KL: 0.07533\n",
      "[15/400] [200/349] Loss_D: 1.01578, Loss_G: 1.89726, Loss_KL: 0.10901\n",
      "[15/400] [300/349] Loss_D: 1.09408, Loss_G: 1.57934, Loss_KL: 0.10676\n",
      "[15/400] Loss_D: 1.07784, Loss_G: 1.53469, Loss_KL: 0.09449\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[16/400] [0/349] Loss_D: 1.06010, Loss_G: 1.89994, Loss_KL: 0.08531\n",
      "[16/400] [100/349] Loss_D: 0.91058, Loss_G: 1.89885, Loss_KL: 0.10433\n",
      "[16/400] [200/349] Loss_D: 0.93644, Loss_G: 1.21388, Loss_KL: 0.09318\n",
      "[16/400] [300/349] Loss_D: 0.95219, Loss_G: 1.91324, Loss_KL: 0.08012\n",
      "[16/400] Loss_D: 1.05503, Loss_G: 1.57700, Loss_KL: 0.09796\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[17/400] [0/349] Loss_D: 0.90233, Loss_G: 2.36007, Loss_KL: 0.09957\n",
      "[17/400] [100/349] Loss_D: 1.10459, Loss_G: 1.48505, Loss_KL: 0.09433\n",
      "[17/400] [200/349] Loss_D: 1.03941, Loss_G: 1.56155, Loss_KL: 0.08038\n",
      "[17/400] [300/349] Loss_D: 1.05789, Loss_G: 1.04869, Loss_KL: 0.09679\n",
      "[17/400] Loss_D: 1.04966, Loss_G: 1.63738, Loss_KL: 0.09808\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[18/400] [0/349] Loss_D: 1.06897, Loss_G: 1.65396, Loss_KL: 0.08271\n",
      "[18/400] [100/349] Loss_D: 1.03398, Loss_G: 1.72478, Loss_KL: 0.09132\n",
      "[18/400] [200/349] Loss_D: 1.03334, Loss_G: 2.09849, Loss_KL: 0.08634\n",
      "[18/400] [300/349] Loss_D: 0.87005, Loss_G: 1.95875, Loss_KL: 0.09574\n",
      "[18/400] Loss_D: 1.02279, Loss_G: 1.66337, Loss_KL: 0.10212\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[19/400] [0/349] Loss_D: 0.94457, Loss_G: 2.27622, Loss_KL: 0.09775\n",
      "[19/400] [100/349] Loss_D: 0.71424, Loss_G: 2.72495, Loss_KL: 0.11289\n",
      "[19/400] [200/349] Loss_D: 0.78773, Loss_G: 1.94504, Loss_KL: 0.09409\n",
      "[19/400] [300/349] Loss_D: 0.95995, Loss_G: 2.06001, Loss_KL: 0.12065\n",
      "[19/400] Loss_D: 0.99777, Loss_G: 1.72233, Loss_KL: 0.11111\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[20/400] [0/349] Loss_D: 0.96128, Loss_G: 1.31665, Loss_KL: 0.11453\n",
      "[20/400] [100/349] Loss_D: 0.79630, Loss_G: 1.96721, Loss_KL: 0.11264\n",
      "[20/400] [200/349] Loss_D: 0.77261, Loss_G: 2.02626, Loss_KL: 0.10239\n",
      "[20/400] [300/349] Loss_D: 1.15472, Loss_G: 0.99647, Loss_KL: 0.11274\n",
      "[20/400] Loss_D: 0.97399, Loss_G: 1.78315, Loss_KL: 0.11175\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[21/400] [0/349] Loss_D: 1.05835, Loss_G: 2.97453, Loss_KL: 0.07720\n",
      "[21/400] [100/349] Loss_D: 0.89216, Loss_G: 0.95151, Loss_KL: 0.12162\n",
      "[21/400] [200/349] Loss_D: 0.96422, Loss_G: 1.43806, Loss_KL: 0.12043\n",
      "[21/400] [300/349] Loss_D: 1.58602, Loss_G: 3.39038, Loss_KL: 0.12287\n",
      "[21/400] Loss_D: 0.96186, Loss_G: 1.80467, Loss_KL: 0.11560\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[22/400] [0/349] Loss_D: 1.16746, Loss_G: 1.48890, Loss_KL: 0.10761\n",
      "[22/400] [100/349] Loss_D: 0.85330, Loss_G: 1.61495, Loss_KL: 0.11086\n",
      "[22/400] [200/349] Loss_D: 1.23272, Loss_G: 2.42020, Loss_KL: 0.13529\n",
      "[22/400] [300/349] Loss_D: 0.68986, Loss_G: 2.25948, Loss_KL: 0.13057\n",
      "[22/400] Loss_D: 0.94594, Loss_G: 1.84672, Loss_KL: 0.12171\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[23/400] [0/349] Loss_D: 0.97318, Loss_G: 2.06510, Loss_KL: 0.11653\n",
      "[23/400] [100/349] Loss_D: 0.98160, Loss_G: 1.76624, Loss_KL: 0.11644\n",
      "[23/400] [200/349] Loss_D: 0.86941, Loss_G: 2.42441, Loss_KL: 0.10784\n",
      "[23/400] [300/349] Loss_D: 0.89930, Loss_G: 1.92832, Loss_KL: 0.12579\n",
      "[23/400] Loss_D: 0.94536, Loss_G: 1.86675, Loss_KL: 0.11763\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[24/400] [0/349] Loss_D: 0.94122, Loss_G: 2.42764, Loss_KL: 0.13980\n",
      "[24/400] [100/349] Loss_D: 0.87609, Loss_G: 3.43978, Loss_KL: 0.12756\n",
      "[24/400] [200/349] Loss_D: 0.81325, Loss_G: 1.85200, Loss_KL: 0.10680\n",
      "[24/400] [300/349] Loss_D: 0.86715, Loss_G: 1.64969, Loss_KL: 0.11241\n",
      "[24/400] Loss_D: 0.93619, Loss_G: 1.90594, Loss_KL: 0.12003\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[25/400] [0/349] Loss_D: 0.98996, Loss_G: 2.22856, Loss_KL: 0.10522\n",
      "[25/400] [100/349] Loss_D: 0.92065, Loss_G: 2.09725, Loss_KL: 0.12794\n",
      "[25/400] [200/349] Loss_D: 1.06811, Loss_G: 2.47777, Loss_KL: 0.12259\n",
      "[25/400] [300/349] Loss_D: 0.88263, Loss_G: 2.45903, Loss_KL: 0.14211\n",
      "[25/400] Loss_D: 0.92091, Loss_G: 1.92637, Loss_KL: 0.12210\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[26/400] [0/349] Loss_D: 0.81672, Loss_G: 1.99817, Loss_KL: 0.12808\n",
      "[26/400] [100/349] Loss_D: 0.98872, Loss_G: 1.22579, Loss_KL: 0.11986\n",
      "[26/400] [200/349] Loss_D: 0.89303, Loss_G: 2.07698, Loss_KL: 0.10596\n",
      "[26/400] [300/349] Loss_D: 0.95149, Loss_G: 2.83060, Loss_KL: 0.13760\n",
      "[26/400] Loss_D: 0.90515, Loss_G: 1.99358, Loss_KL: 0.13298\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[27/400] [0/349] Loss_D: 1.01899, Loss_G: 2.24836, Loss_KL: 0.15066\n",
      "[27/400] [100/349] Loss_D: 0.92114, Loss_G: 2.09581, Loss_KL: 0.12947\n",
      "[27/400] [200/349] Loss_D: 0.97709, Loss_G: 2.24078, Loss_KL: 0.13449\n",
      "[27/400] [300/349] Loss_D: 0.99922, Loss_G: 2.48594, Loss_KL: 0.10839\n",
      "[27/400] Loss_D: 0.88985, Loss_G: 2.06687, Loss_KL: 0.13324\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[28/400] [0/349] Loss_D: 0.90591, Loss_G: 1.80194, Loss_KL: 0.12673\n",
      "[28/400] [100/349] Loss_D: 1.04750, Loss_G: 2.94479, Loss_KL: 0.11749\n",
      "[28/400] [200/349] Loss_D: 0.89651, Loss_G: 2.28283, Loss_KL: 0.13460\n",
      "[28/400] [300/349] Loss_D: 0.74853, Loss_G: 1.80742, Loss_KL: 0.11698\n",
      "[28/400] Loss_D: 0.88954, Loss_G: 2.06889, Loss_KL: 0.13563\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[29/400] [0/349] Loss_D: 0.75902, Loss_G: 2.32778, Loss_KL: 0.15980\n",
      "[29/400] [100/349] Loss_D: 0.74851, Loss_G: 1.89290, Loss_KL: 0.12877\n",
      "[29/400] [200/349] Loss_D: 0.74933, Loss_G: 1.67267, Loss_KL: 0.15425\n",
      "[29/400] [300/349] Loss_D: 0.89399, Loss_G: 2.08849, Loss_KL: 0.12887\n",
      "[29/400] Loss_D: 0.87404, Loss_G: 2.06742, Loss_KL: 0.13267\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[30/400] [0/349] Loss_D: 1.31237, Loss_G: 3.82295, Loss_KL: 0.14279\n",
      "[30/400] [100/349] Loss_D: 0.68202, Loss_G: 2.92386, Loss_KL: 0.15632\n",
      "[30/400] [200/349] Loss_D: 0.66874, Loss_G: 2.50546, Loss_KL: 0.17821\n",
      "[30/400] [300/349] Loss_D: 0.94269, Loss_G: 2.51997, Loss_KL: 0.14302\n",
      "[30/400] Loss_D: 0.85713, Loss_G: 2.14077, Loss_KL: 0.14395\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[31/400] [0/349] Loss_D: 0.78535, Loss_G: 3.02788, Loss_KL: 0.14151\n",
      "[31/400] [100/349] Loss_D: 0.84197, Loss_G: 2.12821, Loss_KL: 0.12157\n",
      "[31/400] [200/349] Loss_D: 0.81414, Loss_G: 1.58109, Loss_KL: 0.11541\n",
      "[31/400] [300/349] Loss_D: 0.68352, Loss_G: 2.71221, Loss_KL: 0.12970\n",
      "[31/400] Loss_D: 0.84448, Loss_G: 2.17135, Loss_KL: 0.13777\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[32/400] [0/349] Loss_D: 0.50965, Loss_G: 3.35443, Loss_KL: 0.13252\n",
      "[32/400] [100/349] Loss_D: 0.49621, Loss_G: 3.09993, Loss_KL: 0.16921\n",
      "[32/400] [200/349] Loss_D: 0.74221, Loss_G: 1.84559, Loss_KL: 0.13388\n",
      "[32/400] [300/349] Loss_D: 0.78718, Loss_G: 1.95665, Loss_KL: 0.14842\n",
      "[32/400] Loss_D: 0.83157, Loss_G: 2.23712, Loss_KL: 0.14864\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[33/400] [0/349] Loss_D: 0.78205, Loss_G: 1.75551, Loss_KL: 0.12738\n",
      "[33/400] [100/349] Loss_D: 0.97137, Loss_G: 2.59109, Loss_KL: 0.16145\n",
      "[33/400] [200/349] Loss_D: 0.86883, Loss_G: 1.62501, Loss_KL: 0.12731\n",
      "[33/400] [300/349] Loss_D: 0.61688, Loss_G: 2.87190, Loss_KL: 0.17012\n",
      "[33/400] Loss_D: 0.83846, Loss_G: 2.23058, Loss_KL: 0.14337\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[34/400] [0/349] Loss_D: 1.18093, Loss_G: 2.43379, Loss_KL: 0.14430\n",
      "[34/400] [100/349] Loss_D: 0.86306, Loss_G: 1.80895, Loss_KL: 0.16792\n",
      "[34/400] [200/349] Loss_D: 1.00817, Loss_G: 2.40618, Loss_KL: 0.14043\n",
      "[34/400] [300/349] Loss_D: 0.91585, Loss_G: 2.67282, Loss_KL: 0.12480\n",
      "[34/400] Loss_D: 0.81803, Loss_G: 2.26779, Loss_KL: 0.14770\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[35/400] [0/349] Loss_D: 0.97059, Loss_G: 3.03925, Loss_KL: 0.15076\n",
      "[35/400] [100/349] Loss_D: 0.70541, Loss_G: 1.41270, Loss_KL: 0.15449\n",
      "[35/400] [200/349] Loss_D: 0.96572, Loss_G: 2.57230, Loss_KL: 0.15653\n",
      "[35/400] [300/349] Loss_D: 0.70420, Loss_G: 2.08918, Loss_KL: 0.14970\n",
      "[35/400] Loss_D: 0.80256, Loss_G: 2.29859, Loss_KL: 0.14570\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[36/400] [0/349] Loss_D: 0.79725, Loss_G: 2.28118, Loss_KL: 0.16265\n",
      "[36/400] [100/349] Loss_D: 0.73607, Loss_G: 1.80897, Loss_KL: 0.13007\n",
      "[36/400] [200/349] Loss_D: 0.82266, Loss_G: 1.68189, Loss_KL: 0.16280\n",
      "[36/400] [300/349] Loss_D: 0.60499, Loss_G: 1.65602, Loss_KL: 0.14796\n",
      "[36/400] Loss_D: 0.80493, Loss_G: 2.30631, Loss_KL: 0.14949\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[37/400] [0/349] Loss_D: 1.03496, Loss_G: 1.65813, Loss_KL: 0.16689\n",
      "[37/400] [100/349] Loss_D: 0.53784, Loss_G: 3.05652, Loss_KL: 0.15388\n",
      "[37/400] [200/349] Loss_D: 0.93640, Loss_G: 1.48554, Loss_KL: 0.18246\n",
      "[37/400] [300/349] Loss_D: 0.78443, Loss_G: 2.19177, Loss_KL: 0.14671\n",
      "[37/400] Loss_D: 0.81462, Loss_G: 2.33130, Loss_KL: 0.15317\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[38/400] [0/349] Loss_D: 0.79088, Loss_G: 2.30092, Loss_KL: 0.15618\n",
      "[38/400] [100/349] Loss_D: 0.56920, Loss_G: 2.26114, Loss_KL: 0.15382\n",
      "[38/400] [200/349] Loss_D: 0.83644, Loss_G: 1.76779, Loss_KL: 0.13553\n",
      "[38/400] [300/349] Loss_D: 0.84604, Loss_G: 1.89691, Loss_KL: 0.16890\n",
      "[38/400] Loss_D: 0.79301, Loss_G: 2.37917, Loss_KL: 0.14924\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[39/400] [0/349] Loss_D: 0.70106, Loss_G: 2.62060, Loss_KL: 0.12080\n",
      "[39/400] [100/349] Loss_D: 0.47616, Loss_G: 2.71157, Loss_KL: 0.15847\n",
      "[39/400] [200/349] Loss_D: 0.62933, Loss_G: 2.59728, Loss_KL: 0.16907\n",
      "[39/400] [300/349] Loss_D: 0.75330, Loss_G: 2.30685, Loss_KL: 0.14651\n",
      "[39/400] Loss_D: 0.78729, Loss_G: 2.38245, Loss_KL: 0.15456\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[40/400] [0/349] Loss_D: 0.75439, Loss_G: 2.35710, Loss_KL: 0.16722\n",
      "[40/400] [100/349] Loss_D: 0.84272, Loss_G: 2.66189, Loss_KL: 0.18523\n",
      "[40/400] [200/349] Loss_D: 0.95296, Loss_G: 3.49539, Loss_KL: 0.18169\n",
      "[40/400] [300/349] Loss_D: 0.86683, Loss_G: 2.76405, Loss_KL: 0.15478\n",
      "[40/400] Loss_D: 0.78776, Loss_G: 2.48772, Loss_KL: 0.17700\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[41/400] [0/349] Loss_D: 0.82321, Loss_G: 2.93065, Loss_KL: 0.15189\n",
      "[41/400] [100/349] Loss_D: 0.94937, Loss_G: 2.86175, Loss_KL: 0.18496\n",
      "[41/400] [200/349] Loss_D: 0.65174, Loss_G: 1.30866, Loss_KL: 0.15559\n",
      "[41/400] [300/349] Loss_D: 0.73127, Loss_G: 2.49771, Loss_KL: 0.17072\n",
      "[41/400] Loss_D: 0.79483, Loss_G: 2.46859, Loss_KL: 0.18373\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[42/400] [0/349] Loss_D: 0.87443, Loss_G: 1.84409, Loss_KL: 0.17514\n",
      "[42/400] [100/349] Loss_D: 0.68181, Loss_G: 2.72642, Loss_KL: 0.17940\n",
      "[42/400] [200/349] Loss_D: 0.89953, Loss_G: 2.82169, Loss_KL: 0.16077\n",
      "[42/400] [300/349] Loss_D: 0.60298, Loss_G: 2.90669, Loss_KL: 0.16302\n",
      "[42/400] Loss_D: 0.78969, Loss_G: 2.46571, Loss_KL: 0.17602\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[43/400] [0/349] Loss_D: 0.54617, Loss_G: 2.73680, Loss_KL: 0.19448\n",
      "[43/400] [100/349] Loss_D: 0.83261, Loss_G: 2.36782, Loss_KL: 0.18871\n",
      "[43/400] [200/349] Loss_D: 0.82080, Loss_G: 2.68936, Loss_KL: 0.18822\n",
      "[43/400] [300/349] Loss_D: 0.75268, Loss_G: 3.98668, Loss_KL: 0.15507\n",
      "[43/400] Loss_D: 0.78954, Loss_G: 2.47921, Loss_KL: 0.18431\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[44/400] [0/349] Loss_D: 0.77439, Loss_G: 2.81911, Loss_KL: 0.16519\n",
      "[44/400] [100/349] Loss_D: 0.76284, Loss_G: 3.11792, Loss_KL: 0.20145\n",
      "[44/400] [200/349] Loss_D: 0.84777, Loss_G: 3.71737, Loss_KL: 0.17187\n",
      "[44/400] [300/349] Loss_D: 0.52843, Loss_G: 3.03331, Loss_KL: 0.17555\n",
      "[44/400] Loss_D: 0.77323, Loss_G: 2.50888, Loss_KL: 0.18171\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[45/400] [0/349] Loss_D: 0.93923, Loss_G: 1.08550, Loss_KL: 0.14760\n",
      "[45/400] [100/349] Loss_D: 0.98277, Loss_G: 2.37601, Loss_KL: 0.20200\n",
      "[45/400] [200/349] Loss_D: 0.54687, Loss_G: 2.76384, Loss_KL: 0.15259\n",
      "[45/400] [300/349] Loss_D: 1.18627, Loss_G: 4.58078, Loss_KL: 0.21838\n",
      "[45/400] Loss_D: 0.76732, Loss_G: 2.51543, Loss_KL: 0.18345\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[46/400] [0/349] Loss_D: 0.87487, Loss_G: 2.81390, Loss_KL: 0.16428\n",
      "[46/400] [100/349] Loss_D: 0.62631, Loss_G: 3.12344, Loss_KL: 0.20221\n",
      "[46/400] [200/349] Loss_D: 0.75539, Loss_G: 2.64228, Loss_KL: 0.20308\n",
      "[46/400] [300/349] Loss_D: 0.61477, Loss_G: 3.33437, Loss_KL: 0.16709\n",
      "[46/400] Loss_D: 0.76955, Loss_G: 2.55797, Loss_KL: 0.18826\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[47/400] [0/349] Loss_D: 0.51287, Loss_G: 2.96194, Loss_KL: 0.19824\n",
      "[47/400] [100/349] Loss_D: 0.58986, Loss_G: 2.59475, Loss_KL: 0.16900\n",
      "[47/400] [200/349] Loss_D: 0.66157, Loss_G: 2.62743, Loss_KL: 0.13999\n",
      "[47/400] [300/349] Loss_D: 0.75090, Loss_G: 1.22097, Loss_KL: 0.23274\n",
      "[47/400] Loss_D: 0.76763, Loss_G: 2.59308, Loss_KL: 0.18127\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[48/400] [0/349] Loss_D: 0.81878, Loss_G: 2.23367, Loss_KL: 0.16273\n",
      "[48/400] [100/349] Loss_D: 0.45717, Loss_G: 2.70582, Loss_KL: 0.15228\n",
      "[48/400] [200/349] Loss_D: 0.49231, Loss_G: 3.32107, Loss_KL: 0.19649\n",
      "[48/400] [300/349] Loss_D: 0.73010, Loss_G: 2.14235, Loss_KL: 0.14382\n",
      "[48/400] Loss_D: 0.76965, Loss_G: 2.56109, Loss_KL: 0.19068\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[49/400] [0/349] Loss_D: 0.95839, Loss_G: 2.39539, Loss_KL: 0.18551\n",
      "[49/400] [100/349] Loss_D: 0.96640, Loss_G: 2.16307, Loss_KL: 0.18525\n",
      "[49/400] [200/349] Loss_D: 0.75250, Loss_G: 2.23821, Loss_KL: 0.16751\n",
      "[49/400] [300/349] Loss_D: 0.88162, Loss_G: 1.66949, Loss_KL: 0.21623\n",
      "[49/400] Loss_D: 0.77403, Loss_G: 2.57195, Loss_KL: 0.19387\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[50/400] [0/349] Loss_D: 1.03128, Loss_G: 4.53667, Loss_KL: 0.18379\n",
      "[50/400] [100/349] Loss_D: 1.16601, Loss_G: 3.03867, Loss_KL: 0.20497\n",
      "[50/400] [200/349] Loss_D: 0.90233, Loss_G: 3.16451, Loss_KL: 0.23994\n",
      "[50/400] [300/349] Loss_D: 0.48449, Loss_G: 2.40295, Loss_KL: 0.14918\n",
      "[50/400] Loss_D: 0.78108, Loss_G: 2.62658, Loss_KL: 0.19753\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[51/400] [0/349] Loss_D: 0.74514, Loss_G: 2.73894, Loss_KL: 0.19147\n",
      "[51/400] [100/349] Loss_D: 0.82437, Loss_G: 3.36707, Loss_KL: 0.17062\n",
      "[51/400] [200/349] Loss_D: 0.76545, Loss_G: 1.90407, Loss_KL: 0.18138\n",
      "[51/400] [300/349] Loss_D: 0.84033, Loss_G: 3.28591, Loss_KL: 0.18179\n",
      "[51/400] Loss_D: 0.76750, Loss_G: 2.60015, Loss_KL: 0.19265\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[52/400] [0/349] Loss_D: 0.59780, Loss_G: 3.13896, Loss_KL: 0.20944\n",
      "[52/400] [100/349] Loss_D: 0.85281, Loss_G: 2.59770, Loss_KL: 0.22404\n",
      "[52/400] [200/349] Loss_D: 0.53662, Loss_G: 2.21714, Loss_KL: 0.21570\n",
      "[52/400] [300/349] Loss_D: 0.80176, Loss_G: 2.24372, Loss_KL: 0.22706\n",
      "[52/400] Loss_D: 0.78457, Loss_G: 2.61688, Loss_KL: 0.19772\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[53/400] [0/349] Loss_D: 0.55793, Loss_G: 2.37109, Loss_KL: 0.15955\n",
      "[53/400] [100/349] Loss_D: 0.53401, Loss_G: 2.87795, Loss_KL: 0.17276\n",
      "[53/400] [200/349] Loss_D: 0.77168, Loss_G: 2.66014, Loss_KL: 0.20860\n",
      "[53/400] [300/349] Loss_D: 0.81361, Loss_G: 3.25053, Loss_KL: 0.25233\n",
      "[53/400] Loss_D: 0.78876, Loss_G: 2.56800, Loss_KL: 0.19448\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[54/400] [0/349] Loss_D: 0.80348, Loss_G: 3.25187, Loss_KL: 0.27346\n",
      "[54/400] [100/349] Loss_D: 0.84661, Loss_G: 1.90291, Loss_KL: 0.23125\n",
      "[54/400] [200/349] Loss_D: 0.61939, Loss_G: 2.20915, Loss_KL: 0.22493\n",
      "[54/400] [300/349] Loss_D: 0.84113, Loss_G: 1.36576, Loss_KL: 0.23816\n",
      "[54/400] Loss_D: 0.77865, Loss_G: 2.59413, Loss_KL: 0.19895\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[55/400] [0/349] Loss_D: 0.95749, Loss_G: 2.23529, Loss_KL: 0.21747\n",
      "[55/400] [100/349] Loss_D: 0.71510, Loss_G: 3.21875, Loss_KL: 0.16094\n",
      "[55/400] [200/349] Loss_D: 0.89701, Loss_G: 2.42504, Loss_KL: 0.17751\n",
      "[55/400] [300/349] Loss_D: 0.58021, Loss_G: 2.38782, Loss_KL: 0.17295\n",
      "[55/400] Loss_D: 0.77700, Loss_G: 2.57792, Loss_KL: 0.19147\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[56/400] [0/349] Loss_D: 0.56186, Loss_G: 3.50344, Loss_KL: 0.15507\n",
      "[56/400] [100/349] Loss_D: 0.74398, Loss_G: 3.06271, Loss_KL: 0.24923\n",
      "[56/400] [200/349] Loss_D: 0.73702, Loss_G: 2.00491, Loss_KL: 0.19238\n",
      "[56/400] [300/349] Loss_D: 0.73786, Loss_G: 1.90261, Loss_KL: 0.25762\n",
      "[56/400] Loss_D: 0.78379, Loss_G: 2.59032, Loss_KL: 0.20313\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[57/400] [0/349] Loss_D: 0.78158, Loss_G: 3.07436, Loss_KL: 0.16014\n",
      "[57/400] [100/349] Loss_D: 0.98312, Loss_G: 2.28523, Loss_KL: 0.26912\n",
      "[57/400] [200/349] Loss_D: 0.89527, Loss_G: 2.04904, Loss_KL: 0.19743\n",
      "[57/400] [300/349] Loss_D: 0.72672, Loss_G: 2.21129, Loss_KL: 0.31291\n",
      "[57/400] Loss_D: 0.76986, Loss_G: 2.64365, Loss_KL: 0.20781\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[58/400] [0/349] Loss_D: 0.46082, Loss_G: 2.19431, Loss_KL: 0.20621\n",
      "[58/400] [100/349] Loss_D: 1.39638, Loss_G: 1.14363, Loss_KL: 0.18171\n",
      "[58/400] [200/349] Loss_D: 0.77430, Loss_G: 2.99849, Loss_KL: 0.20679\n",
      "[58/400] [300/349] Loss_D: 1.01420, Loss_G: 2.37948, Loss_KL: 0.20266\n",
      "[58/400] Loss_D: 0.78036, Loss_G: 2.61673, Loss_KL: 0.20500\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[59/400] [0/349] Loss_D: 0.52458, Loss_G: 2.32737, Loss_KL: 0.17713\n",
      "[59/400] [100/349] Loss_D: 0.37838, Loss_G: 2.06103, Loss_KL: 0.27483\n",
      "[59/400] [200/349] Loss_D: 0.64786, Loss_G: 1.78856, Loss_KL: 0.22902\n",
      "[59/400] [300/349] Loss_D: 1.02052, Loss_G: 2.86349, Loss_KL: 0.18634\n",
      "[59/400] Loss_D: 0.76128, Loss_G: 2.61454, Loss_KL: 0.20590\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[60/400] [0/349] Loss_D: 0.69647, Loss_G: 4.14546, Loss_KL: 0.21848\n",
      "[60/400] [100/349] Loss_D: 1.65726, Loss_G: 0.81514, Loss_KL: 0.19756\n",
      "[60/400] [200/349] Loss_D: 0.29258, Loss_G: 2.70865, Loss_KL: 0.25317\n",
      "[60/400] [300/349] Loss_D: 0.56719, Loss_G: 3.16137, Loss_KL: 0.19696\n",
      "[60/400] Loss_D: 0.77302, Loss_G: 2.64908, Loss_KL: 0.20333\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[61/400] [0/349] Loss_D: 0.69000, Loss_G: 2.91386, Loss_KL: 0.16580\n",
      "[61/400] [100/349] Loss_D: 0.87114, Loss_G: 2.80728, Loss_KL: 0.27222\n",
      "[61/400] [200/349] Loss_D: 0.70863, Loss_G: 3.22391, Loss_KL: 0.15004\n",
      "[61/400] [300/349] Loss_D: 0.94593, Loss_G: 2.08284, Loss_KL: 0.19308\n",
      "[61/400] Loss_D: 0.74056, Loss_G: 2.64680, Loss_KL: 0.20624\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[62/400] [0/349] Loss_D: 0.91351, Loss_G: 3.73832, Loss_KL: 0.15125\n",
      "[62/400] [100/349] Loss_D: 0.87754, Loss_G: 2.51824, Loss_KL: 0.21277\n",
      "[62/400] [200/349] Loss_D: 0.77599, Loss_G: 1.68606, Loss_KL: 0.18051\n",
      "[62/400] [300/349] Loss_D: 0.77036, Loss_G: 2.21110, Loss_KL: 0.18546\n",
      "[62/400] Loss_D: 0.73512, Loss_G: 2.67750, Loss_KL: 0.22197\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[63/400] [0/349] Loss_D: 0.61977, Loss_G: 2.20320, Loss_KL: 0.19880\n",
      "[63/400] [100/349] Loss_D: 0.69758, Loss_G: 2.57976, Loss_KL: 0.20613\n",
      "[63/400] [200/349] Loss_D: 0.72737, Loss_G: 2.52676, Loss_KL: 0.18243\n",
      "[63/400] [300/349] Loss_D: 0.97895, Loss_G: 3.37482, Loss_KL: 0.18639\n",
      "[63/400] Loss_D: 0.76270, Loss_G: 2.69280, Loss_KL: 0.21826\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[64/400] [0/349] Loss_D: 0.56578, Loss_G: 3.39151, Loss_KL: 0.21105\n",
      "[64/400] [100/349] Loss_D: 0.77803, Loss_G: 2.23721, Loss_KL: 0.24570\n",
      "[64/400] [200/349] Loss_D: 0.57265, Loss_G: 2.06073, Loss_KL: 0.23210\n",
      "[64/400] [300/349] Loss_D: 0.72490, Loss_G: 3.06445, Loss_KL: 0.24642\n",
      "[64/400] Loss_D: 0.75346, Loss_G: 2.69662, Loss_KL: 0.21860\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[65/400] [0/349] Loss_D: 0.96124, Loss_G: 3.60499, Loss_KL: 0.20246\n",
      "[65/400] [100/349] Loss_D: 0.68086, Loss_G: 3.70968, Loss_KL: 0.19750\n",
      "[65/400] [200/349] Loss_D: 0.74797, Loss_G: 2.26803, Loss_KL: 0.18899\n",
      "[65/400] [300/349] Loss_D: 0.78978, Loss_G: 2.02577, Loss_KL: 0.21138\n",
      "[65/400] Loss_D: 0.74587, Loss_G: 2.70418, Loss_KL: 0.22229\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[66/400] [0/349] Loss_D: 0.99592, Loss_G: 2.48920, Loss_KL: 0.18786\n",
      "[66/400] [100/349] Loss_D: 0.61926, Loss_G: 3.83141, Loss_KL: 0.23765\n",
      "[66/400] [200/349] Loss_D: 0.46222, Loss_G: 2.58706, Loss_KL: 0.22046\n",
      "[66/400] [300/349] Loss_D: 0.64873, Loss_G: 3.21768, Loss_KL: 0.20284\n",
      "[66/400] Loss_D: 0.75080, Loss_G: 2.68075, Loss_KL: 0.21733\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[67/400] [0/349] Loss_D: 0.57822, Loss_G: 3.82472, Loss_KL: 0.23999\n",
      "[67/400] [100/349] Loss_D: 0.76942, Loss_G: 3.24623, Loss_KL: 0.23267\n",
      "[67/400] [200/349] Loss_D: 0.39972, Loss_G: 3.46221, Loss_KL: 0.28671\n",
      "[67/400] [300/349] Loss_D: 0.64793, Loss_G: 2.91435, Loss_KL: 0.23329\n",
      "[67/400] Loss_D: 0.74839, Loss_G: 2.69364, Loss_KL: 0.22457\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[68/400] [0/349] Loss_D: 1.35282, Loss_G: 1.41847, Loss_KL: 0.26930\n",
      "[68/400] [100/349] Loss_D: 1.02492, Loss_G: 4.21108, Loss_KL: 0.20595\n",
      "[68/400] [200/349] Loss_D: 0.74523, Loss_G: 2.59456, Loss_KL: 0.22542\n",
      "[68/400] [300/349] Loss_D: 0.86997, Loss_G: 2.55528, Loss_KL: 0.21184\n",
      "[68/400] Loss_D: 0.75890, Loss_G: 2.67387, Loss_KL: 0.22519\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[69/400] [0/349] Loss_D: 0.65494, Loss_G: 3.46682, Loss_KL: 0.21123\n",
      "[69/400] [100/349] Loss_D: 0.66578, Loss_G: 3.99122, Loss_KL: 0.21255\n",
      "[69/400] [200/349] Loss_D: 0.78781, Loss_G: 2.75658, Loss_KL: 0.14279\n",
      "[69/400] [300/349] Loss_D: 0.83102, Loss_G: 2.35440, Loss_KL: 0.23102\n",
      "[69/400] Loss_D: 0.73906, Loss_G: 2.68437, Loss_KL: 0.22118\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[70/400] [0/349] Loss_D: 0.79148, Loss_G: 2.38613, Loss_KL: 0.20545\n",
      "[70/400] [100/349] Loss_D: 0.71481, Loss_G: 2.19421, Loss_KL: 0.20185\n",
      "[70/400] [200/349] Loss_D: 0.57383, Loss_G: 2.04251, Loss_KL: 0.27387\n",
      "[70/400] [300/349] Loss_D: 0.75542, Loss_G: 2.36359, Loss_KL: 0.32640\n",
      "[70/400] Loss_D: 0.75449, Loss_G: 2.71295, Loss_KL: 0.23380\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[71/400] [0/349] Loss_D: 0.67337, Loss_G: 4.25078, Loss_KL: 0.28245\n",
      "[71/400] [100/349] Loss_D: 0.50915, Loss_G: 2.40612, Loss_KL: 0.19083\n",
      "[71/400] [200/349] Loss_D: 0.94670, Loss_G: 2.86354, Loss_KL: 0.21060\n",
      "[71/400] [300/349] Loss_D: 0.71749, Loss_G: 2.71658, Loss_KL: 0.21796\n",
      "[71/400] Loss_D: 0.75647, Loss_G: 2.68709, Loss_KL: 0.22434\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[72/400] [0/349] Loss_D: 0.51036, Loss_G: 2.90445, Loss_KL: 0.22592\n",
      "[72/400] [100/349] Loss_D: 0.64081, Loss_G: 2.77095, Loss_KL: 0.22732\n",
      "[72/400] [200/349] Loss_D: 0.91261, Loss_G: 3.56564, Loss_KL: 0.25138\n",
      "[72/400] [300/349] Loss_D: 0.62514, Loss_G: 2.17165, Loss_KL: 0.21121\n",
      "[72/400] Loss_D: 0.72165, Loss_G: 2.68608, Loss_KL: 0.23119\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[73/400] [0/349] Loss_D: 0.75462, Loss_G: 2.70070, Loss_KL: 0.25825\n",
      "[73/400] [100/349] Loss_D: 0.60531, Loss_G: 1.97155, Loss_KL: 0.20692\n",
      "[73/400] [200/349] Loss_D: 1.20285, Loss_G: 3.11304, Loss_KL: 0.20176\n",
      "[73/400] [300/349] Loss_D: 0.76114, Loss_G: 2.66497, Loss_KL: 0.20492\n",
      "[73/400] Loss_D: 0.74944, Loss_G: 2.70006, Loss_KL: 0.24075\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[74/400] [0/349] Loss_D: 0.70117, Loss_G: 2.52644, Loss_KL: 0.24738\n",
      "[74/400] [100/349] Loss_D: 1.05828, Loss_G: 1.38361, Loss_KL: 0.22173\n",
      "[74/400] [200/349] Loss_D: 0.71716, Loss_G: 1.70228, Loss_KL: 0.28365\n",
      "[74/400] [300/349] Loss_D: 0.64130, Loss_G: 1.96757, Loss_KL: 0.26550\n",
      "[74/400] Loss_D: 0.72735, Loss_G: 2.75142, Loss_KL: 0.26278\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[75/400] [0/349] Loss_D: 0.74525, Loss_G: 1.82377, Loss_KL: 0.40019\n",
      "[75/400] [100/349] Loss_D: 0.64573, Loss_G: 3.07328, Loss_KL: 0.24313\n",
      "[75/400] [200/349] Loss_D: 0.64616, Loss_G: 2.51647, Loss_KL: 0.19561\n",
      "[75/400] [300/349] Loss_D: 0.55551, Loss_G: 3.78117, Loss_KL: 0.32055\n",
      "[75/400] Loss_D: 0.74447, Loss_G: 2.76635, Loss_KL: 0.23993\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[76/400] [0/349] Loss_D: 0.66740, Loss_G: 3.04969, Loss_KL: 0.21819\n",
      "[76/400] [100/349] Loss_D: 0.79350, Loss_G: 1.87854, Loss_KL: 0.25495\n",
      "[76/400] [200/349] Loss_D: 0.60141, Loss_G: 3.36428, Loss_KL: 0.19785\n",
      "[76/400] [300/349] Loss_D: 0.87239, Loss_G: 2.27558, Loss_KL: 0.18817\n",
      "[76/400] Loss_D: 0.72642, Loss_G: 2.74324, Loss_KL: 0.22889\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[77/400] [0/349] Loss_D: 0.86565, Loss_G: 2.03384, Loss_KL: 0.18847\n",
      "[77/400] [100/349] Loss_D: 0.48220, Loss_G: 3.11019, Loss_KL: 0.25772\n",
      "[77/400] [200/349] Loss_D: 0.69306, Loss_G: 2.79573, Loss_KL: 0.25420\n",
      "[77/400] [300/349] Loss_D: 0.86591, Loss_G: 1.89083, Loss_KL: 0.24071\n",
      "[77/400] Loss_D: 0.72442, Loss_G: 2.77129, Loss_KL: 0.23491\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[78/400] [0/349] Loss_D: 0.73085, Loss_G: 2.53758, Loss_KL: 0.27556\n",
      "[78/400] [100/349] Loss_D: 1.16269, Loss_G: 3.15046, Loss_KL: 0.22542\n",
      "[78/400] [200/349] Loss_D: 0.71767, Loss_G: 2.80654, Loss_KL: 0.23301\n",
      "[78/400] [300/349] Loss_D: 0.92940, Loss_G: 3.26368, Loss_KL: 0.25082\n",
      "[78/400] Loss_D: 0.72952, Loss_G: 2.74238, Loss_KL: 0.24581\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[79/400] [0/349] Loss_D: 0.74710, Loss_G: 2.80081, Loss_KL: 0.16815\n",
      "[79/400] [100/349] Loss_D: 0.74283, Loss_G: 2.35406, Loss_KL: 0.25187\n",
      "[79/400] [200/349] Loss_D: 1.25580, Loss_G: 2.63597, Loss_KL: 0.19714\n",
      "[79/400] [300/349] Loss_D: 0.52316, Loss_G: 3.99572, Loss_KL: 0.27540\n",
      "[79/400] Loss_D: 0.74790, Loss_G: 2.73580, Loss_KL: 0.23603\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[80/400] [0/349] Loss_D: 0.55578, Loss_G: 2.85623, Loss_KL: 0.20440\n",
      "[80/400] [100/349] Loss_D: 0.89960, Loss_G: 3.43907, Loss_KL: 0.22995\n",
      "[80/400] [200/349] Loss_D: 0.58587, Loss_G: 3.55836, Loss_KL: 0.31291\n",
      "[80/400] [300/349] Loss_D: 0.66226, Loss_G: 2.48507, Loss_KL: 0.32327\n",
      "[80/400] Loss_D: 0.71959, Loss_G: 2.79383, Loss_KL: 0.25470\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[81/400] [0/349] Loss_D: 0.78177, Loss_G: 2.45399, Loss_KL: 0.28656\n",
      "[81/400] [100/349] Loss_D: 0.67029, Loss_G: 2.13649, Loss_KL: 0.21892\n",
      "[81/400] [200/349] Loss_D: 0.90440, Loss_G: 2.04940, Loss_KL: 0.28342\n",
      "[81/400] [300/349] Loss_D: 0.67800, Loss_G: 3.46752, Loss_KL: 0.21297\n",
      "[81/400] Loss_D: 0.71857, Loss_G: 2.82417, Loss_KL: 0.24886\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[82/400] [0/349] Loss_D: 1.03544, Loss_G: 3.01660, Loss_KL: 0.23676\n",
      "[82/400] [100/349] Loss_D: 0.76129, Loss_G: 2.94749, Loss_KL: 0.27770\n",
      "[82/400] [200/349] Loss_D: 1.01315, Loss_G: 2.92305, Loss_KL: 0.24892\n",
      "[82/400] [300/349] Loss_D: 0.64637, Loss_G: 2.80207, Loss_KL: 0.25884\n",
      "[82/400] Loss_D: 0.72925, Loss_G: 2.78569, Loss_KL: 0.25673\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[83/400] [0/349] Loss_D: 1.00067, Loss_G: 2.13756, Loss_KL: 0.32688\n",
      "[83/400] [100/349] Loss_D: 0.30261, Loss_G: 2.97866, Loss_KL: 0.26109\n",
      "[83/400] [200/349] Loss_D: 0.37753, Loss_G: 2.68269, Loss_KL: 0.22771\n",
      "[83/400] [300/349] Loss_D: 0.84646, Loss_G: 1.40820, Loss_KL: 0.27789\n",
      "[83/400] Loss_D: 0.74564, Loss_G: 2.76359, Loss_KL: 0.26436\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[84/400] [0/349] Loss_D: 0.86595, Loss_G: 3.67141, Loss_KL: 0.27763\n",
      "[84/400] [100/349] Loss_D: 0.84656, Loss_G: 1.70083, Loss_KL: 0.26071\n",
      "[84/400] [200/349] Loss_D: 1.18898, Loss_G: 3.83346, Loss_KL: 0.23342\n",
      "[84/400] [300/349] Loss_D: 0.70690, Loss_G: 2.49983, Loss_KL: 0.18107\n",
      "[84/400] Loss_D: 0.72916, Loss_G: 2.77329, Loss_KL: 0.24587\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[85/400] [0/349] Loss_D: 0.69701, Loss_G: 2.42344, Loss_KL: 0.29626\n",
      "[85/400] [100/349] Loss_D: 0.66454, Loss_G: 3.05380, Loss_KL: 0.20883\n",
      "[85/400] [200/349] Loss_D: 1.02650, Loss_G: 2.94899, Loss_KL: 0.23668\n",
      "[85/400] [300/349] Loss_D: 0.77366, Loss_G: 2.25351, Loss_KL: 0.15966\n",
      "[85/400] Loss_D: 0.73508, Loss_G: 2.75231, Loss_KL: 0.24650\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[86/400] [0/349] Loss_D: 0.51913, Loss_G: 2.08366, Loss_KL: 0.15676\n",
      "[86/400] [100/349] Loss_D: 0.83394, Loss_G: 1.72239, Loss_KL: 0.24156\n",
      "[86/400] [200/349] Loss_D: 0.42053, Loss_G: 2.95148, Loss_KL: 0.23473\n",
      "[86/400] [300/349] Loss_D: 0.72672, Loss_G: 2.30552, Loss_KL: 0.21954\n",
      "[86/400] Loss_D: 0.71868, Loss_G: 2.75460, Loss_KL: 0.25490\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[87/400] [0/349] Loss_D: 0.66183, Loss_G: 3.20119, Loss_KL: 0.26782\n",
      "[87/400] [100/349] Loss_D: 1.14117, Loss_G: 1.38359, Loss_KL: 0.27530\n",
      "[87/400] [200/349] Loss_D: 0.76988, Loss_G: 2.65649, Loss_KL: 0.24709\n",
      "[87/400] [300/349] Loss_D: 0.93020, Loss_G: 2.57584, Loss_KL: 0.33215\n",
      "[87/400] Loss_D: 0.72859, Loss_G: 2.81814, Loss_KL: 0.25928\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[88/400] [0/349] Loss_D: 0.29545, Loss_G: 3.17779, Loss_KL: 0.30069\n",
      "[88/400] [100/349] Loss_D: 0.62097, Loss_G: 2.24623, Loss_KL: 0.23923\n",
      "[88/400] [200/349] Loss_D: 0.77896, Loss_G: 2.98241, Loss_KL: 0.23875\n",
      "[88/400] [300/349] Loss_D: 0.47613, Loss_G: 2.46999, Loss_KL: 0.20523\n",
      "[88/400] Loss_D: 0.74455, Loss_G: 2.77049, Loss_KL: 0.25004\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[89/400] [0/349] Loss_D: 0.43546, Loss_G: 2.83270, Loss_KL: 0.23049\n",
      "[89/400] [100/349] Loss_D: 0.99022, Loss_G: 3.50257, Loss_KL: 0.18878\n",
      "[89/400] [200/349] Loss_D: 0.81915, Loss_G: 2.21334, Loss_KL: 0.24203\n",
      "[89/400] [300/349] Loss_D: 0.68092, Loss_G: 2.21779, Loss_KL: 0.36054\n",
      "[89/400] Loss_D: 0.73249, Loss_G: 2.77658, Loss_KL: 0.24808\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[90/400] [0/349] Loss_D: 0.61775, Loss_G: 2.96595, Loss_KL: 0.23938\n",
      "[90/400] [100/349] Loss_D: 0.43552, Loss_G: 3.68572, Loss_KL: 0.25826\n",
      "[90/400] [200/349] Loss_D: 0.44138, Loss_G: 2.79136, Loss_KL: 0.23766\n",
      "[90/400] [300/349] Loss_D: 0.85242, Loss_G: 2.26796, Loss_KL: 0.21946\n",
      "[90/400] Loss_D: 0.70186, Loss_G: 2.80372, Loss_KL: 0.26410\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[91/400] [0/349] Loss_D: 0.53405, Loss_G: 3.44633, Loss_KL: 0.24229\n",
      "[91/400] [100/349] Loss_D: 0.74909, Loss_G: 3.28783, Loss_KL: 0.30133\n",
      "[91/400] [200/349] Loss_D: 0.81212, Loss_G: 1.57556, Loss_KL: 0.25879\n",
      "[91/400] [300/349] Loss_D: 0.43887, Loss_G: 2.67564, Loss_KL: 0.30156\n",
      "[91/400] Loss_D: 0.71967, Loss_G: 2.78794, Loss_KL: 0.25787\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[92/400] [0/349] Loss_D: 1.31788, Loss_G: 3.83705, Loss_KL: 0.19375\n",
      "[92/400] [100/349] Loss_D: 0.74443, Loss_G: 3.50580, Loss_KL: 0.19758\n",
      "[92/400] [200/349] Loss_D: 0.40180, Loss_G: 2.89461, Loss_KL: 0.36848\n",
      "[92/400] [300/349] Loss_D: 0.83474, Loss_G: 3.05257, Loss_KL: 0.25386\n",
      "[92/400] Loss_D: 0.73277, Loss_G: 2.78501, Loss_KL: 0.26086\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[93/400] [0/349] Loss_D: 0.89112, Loss_G: 1.84452, Loss_KL: 0.29588\n",
      "[93/400] [100/349] Loss_D: 0.90725, Loss_G: 2.34718, Loss_KL: 0.31920\n",
      "[93/400] [200/349] Loss_D: 0.95432, Loss_G: 2.63399, Loss_KL: 0.26377\n",
      "[93/400] [300/349] Loss_D: 0.54344, Loss_G: 3.47298, Loss_KL: 0.23407\n",
      "[93/400] Loss_D: 0.71439, Loss_G: 2.80027, Loss_KL: 0.27096\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[94/400] [0/349] Loss_D: 0.65166, Loss_G: 3.02911, Loss_KL: 0.22106\n",
      "[94/400] [100/349] Loss_D: 0.93981, Loss_G: 2.72976, Loss_KL: 0.23866\n",
      "[94/400] [200/349] Loss_D: 0.66349, Loss_G: 2.27907, Loss_KL: 0.23332\n",
      "[94/400] [300/349] Loss_D: 0.70816, Loss_G: 2.48016, Loss_KL: 0.24157\n",
      "[94/400] Loss_D: 0.71280, Loss_G: 2.82924, Loss_KL: 0.25440\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[95/400] [0/349] Loss_D: 0.69464, Loss_G: 3.74175, Loss_KL: 0.29321\n",
      "[95/400] [100/349] Loss_D: 0.61066, Loss_G: 2.86195, Loss_KL: 0.34464\n",
      "[95/400] [200/349] Loss_D: 0.95659, Loss_G: 1.39336, Loss_KL: 0.22826\n",
      "[95/400] [300/349] Loss_D: 0.90848, Loss_G: 2.38318, Loss_KL: 0.25088\n",
      "[95/400] Loss_D: 0.72924, Loss_G: 2.79111, Loss_KL: 0.25304\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[96/400] [0/349] Loss_D: 0.60392, Loss_G: 3.51276, Loss_KL: 0.32973\n",
      "[96/400] [100/349] Loss_D: 0.46357, Loss_G: 2.74712, Loss_KL: 0.30002\n",
      "[96/400] [200/349] Loss_D: 0.79687, Loss_G: 2.81315, Loss_KL: 0.17575\n",
      "[96/400] [300/349] Loss_D: 0.87761, Loss_G: 2.64440, Loss_KL: 0.36154\n",
      "[96/400] Loss_D: 0.69663, Loss_G: 2.81602, Loss_KL: 0.25280\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[97/400] [0/349] Loss_D: 0.50162, Loss_G: 3.38234, Loss_KL: 0.23661\n",
      "[97/400] [100/349] Loss_D: 0.75576, Loss_G: 3.63890, Loss_KL: 0.26118\n",
      "[97/400] [200/349] Loss_D: 0.39572, Loss_G: 2.90694, Loss_KL: 0.18460\n",
      "[97/400] [300/349] Loss_D: 0.57240, Loss_G: 3.44526, Loss_KL: 0.25088\n",
      "[97/400] Loss_D: 0.69272, Loss_G: 2.83070, Loss_KL: 0.25795\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[98/400] [0/349] Loss_D: 0.50764, Loss_G: 3.57997, Loss_KL: 0.15225\n",
      "[98/400] [100/349] Loss_D: 0.77672, Loss_G: 2.96162, Loss_KL: 0.19403\n",
      "[98/400] [200/349] Loss_D: 0.73944, Loss_G: 3.38215, Loss_KL: 0.32250\n",
      "[98/400] [300/349] Loss_D: 0.47811, Loss_G: 2.73150, Loss_KL: 0.30982\n",
      "[98/400] Loss_D: 0.71335, Loss_G: 2.83892, Loss_KL: 0.26041\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[99/400] [0/349] Loss_D: 0.83084, Loss_G: 2.38459, Loss_KL: 0.34300\n",
      "[99/400] [100/349] Loss_D: 0.64357, Loss_G: 3.28405, Loss_KL: 0.23214\n",
      "[99/400] [200/349] Loss_D: 0.71629, Loss_G: 3.25963, Loss_KL: 0.21167\n",
      "[99/400] [300/349] Loss_D: 0.63245, Loss_G: 3.76674, Loss_KL: 0.27749\n",
      "[99/400] Loss_D: 0.70161, Loss_G: 2.80007, Loss_KL: 0.25875\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[100/400] [0/349] Loss_D: 0.56181, Loss_G: 3.45244, Loss_KL: 0.22239\n",
      "[100/400] [100/349] Loss_D: 0.87564, Loss_G: 2.93758, Loss_KL: 0.28215\n",
      "[100/400] [200/349] Loss_D: 0.39928, Loss_G: 2.87154, Loss_KL: 0.19039\n",
      "[100/400] [300/349] Loss_D: 0.73223, Loss_G: 2.53449, Loss_KL: 0.28803\n",
      "[100/400] Loss_D: 0.72641, Loss_G: 2.79481, Loss_KL: 0.26647\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[101/400] [0/349] Loss_D: 0.78617, Loss_G: 3.04903, Loss_KL: 0.31922\n",
      "[101/400] [100/349] Loss_D: 0.75417, Loss_G: 2.69033, Loss_KL: 0.36887\n",
      "[101/400] [200/349] Loss_D: 0.29308, Loss_G: 2.79170, Loss_KL: 0.22998\n",
      "[101/400] [300/349] Loss_D: 0.43703, Loss_G: 2.54760, Loss_KL: 0.31384\n",
      "[101/400] Loss_D: 0.65491, Loss_G: 2.62109, Loss_KL: 0.27520\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[102/400] [0/349] Loss_D: 0.42562, Loss_G: 4.06919, Loss_KL: 0.24174\n",
      "[102/400] [100/349] Loss_D: 0.81317, Loss_G: 1.99176, Loss_KL: 0.28413\n",
      "[102/400] [200/349] Loss_D: 0.71446, Loss_G: 2.41463, Loss_KL: 0.28739\n",
      "[102/400] [300/349] Loss_D: 0.66128, Loss_G: 3.15072, Loss_KL: 0.25507\n",
      "[102/400] Loss_D: 0.66642, Loss_G: 2.61576, Loss_KL: 0.27768\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[103/400] [0/349] Loss_D: 0.72836, Loss_G: 2.47049, Loss_KL: 0.27198\n",
      "[103/400] [100/349] Loss_D: 0.86871, Loss_G: 2.31603, Loss_KL: 0.28893\n",
      "[103/400] [200/349] Loss_D: 0.81665, Loss_G: 1.61197, Loss_KL: 0.36074\n",
      "[103/400] [300/349] Loss_D: 0.72495, Loss_G: 2.19376, Loss_KL: 0.34251\n",
      "[103/400] Loss_D: 0.66507, Loss_G: 2.69353, Loss_KL: 0.29990\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[104/400] [0/349] Loss_D: 0.79758, Loss_G: 3.71407, Loss_KL: 0.21303\n",
      "[104/400] [100/349] Loss_D: 0.21506, Loss_G: 3.42725, Loss_KL: 0.28108\n",
      "[104/400] [200/349] Loss_D: 0.49269, Loss_G: 2.92151, Loss_KL: 0.41639\n",
      "[104/400] [300/349] Loss_D: 0.57265, Loss_G: 3.44655, Loss_KL: 0.23981\n",
      "[104/400] Loss_D: 0.66284, Loss_G: 2.70497, Loss_KL: 0.30823\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[105/400] [0/349] Loss_D: 0.47488, Loss_G: 2.69286, Loss_KL: 0.39714\n",
      "[105/400] [100/349] Loss_D: 0.74512, Loss_G: 2.84619, Loss_KL: 0.29828\n",
      "[105/400] [200/349] Loss_D: 0.65629, Loss_G: 2.80134, Loss_KL: 0.28857\n",
      "[105/400] [300/349] Loss_D: 0.41176, Loss_G: 2.58279, Loss_KL: 0.40395\n",
      "[105/400] Loss_D: 0.67623, Loss_G: 2.69625, Loss_KL: 0.30821\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[106/400] [0/349] Loss_D: 0.87788, Loss_G: 2.50484, Loss_KL: 0.44590\n",
      "[106/400] [100/349] Loss_D: 0.89849, Loss_G: 1.92613, Loss_KL: 0.32272\n",
      "[106/400] [200/349] Loss_D: 1.07213, Loss_G: 1.70837, Loss_KL: 0.33533\n",
      "[106/400] [300/349] Loss_D: 0.38710, Loss_G: 2.97952, Loss_KL: 0.43068\n",
      "[106/400] Loss_D: 0.65279, Loss_G: 2.72456, Loss_KL: 0.30400\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[107/400] [0/349] Loss_D: 0.77291, Loss_G: 2.28439, Loss_KL: 0.28912\n",
      "[107/400] [100/349] Loss_D: 0.95723, Loss_G: 2.98821, Loss_KL: 0.25468\n",
      "[107/400] [200/349] Loss_D: 0.58137, Loss_G: 2.74435, Loss_KL: 0.28527\n",
      "[107/400] [300/349] Loss_D: 0.58247, Loss_G: 3.41446, Loss_KL: 0.28475\n",
      "[107/400] Loss_D: 0.65165, Loss_G: 2.78657, Loss_KL: 0.31258\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[108/400] [0/349] Loss_D: 0.87087, Loss_G: 2.70869, Loss_KL: 0.39581\n",
      "[108/400] [100/349] Loss_D: 0.47091, Loss_G: 2.47650, Loss_KL: 0.36338\n",
      "[108/400] [200/349] Loss_D: 0.49427, Loss_G: 2.95267, Loss_KL: 0.32134\n",
      "[108/400] [300/349] Loss_D: 0.56891, Loss_G: 2.42337, Loss_KL: 0.44018\n",
      "[108/400] Loss_D: 0.65628, Loss_G: 2.81144, Loss_KL: 0.32073\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[109/400] [0/349] Loss_D: 0.71114, Loss_G: 2.65242, Loss_KL: 0.31743\n",
      "[109/400] [100/349] Loss_D: 0.69275, Loss_G: 2.81274, Loss_KL: 0.32949\n",
      "[109/400] [200/349] Loss_D: 0.39345, Loss_G: 2.53670, Loss_KL: 0.31627\n",
      "[109/400] [300/349] Loss_D: 0.76539, Loss_G: 2.20044, Loss_KL: 0.37305\n",
      "[109/400] Loss_D: 0.66161, Loss_G: 2.75604, Loss_KL: 0.30846\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[110/400] [0/349] Loss_D: 0.72415, Loss_G: 2.61554, Loss_KL: 0.31571\n",
      "[110/400] [100/349] Loss_D: 0.28735, Loss_G: 3.22640, Loss_KL: 0.39489\n",
      "[110/400] [200/349] Loss_D: 0.92816, Loss_G: 2.93815, Loss_KL: 0.30903\n",
      "[110/400] [300/349] Loss_D: 0.72827, Loss_G: 2.08379, Loss_KL: 0.25800\n",
      "[110/400] Loss_D: 0.65341, Loss_G: 2.72063, Loss_KL: 0.30737\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[111/400] [0/349] Loss_D: 0.37315, Loss_G: 3.21039, Loss_KL: 0.29958\n",
      "[111/400] [100/349] Loss_D: 0.54771, Loss_G: 2.84758, Loss_KL: 0.28659\n",
      "[111/400] [200/349] Loss_D: 0.53687, Loss_G: 2.83203, Loss_KL: 0.28254\n",
      "[111/400] [300/349] Loss_D: 0.41833, Loss_G: 3.13706, Loss_KL: 0.29829\n",
      "[111/400] Loss_D: 0.65503, Loss_G: 2.80493, Loss_KL: 0.30985\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[112/400] [0/349] Loss_D: 0.57225, Loss_G: 3.02387, Loss_KL: 0.29016\n",
      "[112/400] [100/349] Loss_D: 0.84766, Loss_G: 2.47666, Loss_KL: 0.28635\n",
      "[112/400] [200/349] Loss_D: 0.37554, Loss_G: 2.53753, Loss_KL: 0.35849\n",
      "[112/400] [300/349] Loss_D: 0.46607, Loss_G: 2.96960, Loss_KL: 0.42367\n",
      "[112/400] Loss_D: 0.66484, Loss_G: 2.79472, Loss_KL: 0.32220\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[113/400] [0/349] Loss_D: 0.44857, Loss_G: 3.62809, Loss_KL: 0.33309\n",
      "[113/400] [100/349] Loss_D: 0.44005, Loss_G: 2.80187, Loss_KL: 0.28124\n",
      "[113/400] [200/349] Loss_D: 0.69083, Loss_G: 3.58197, Loss_KL: 0.33344\n",
      "[113/400] [300/349] Loss_D: 0.73312, Loss_G: 2.43442, Loss_KL: 0.34854\n",
      "[113/400] Loss_D: 0.66254, Loss_G: 2.79377, Loss_KL: 0.32656\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[114/400] [0/349] Loss_D: 0.60030, Loss_G: 3.43552, Loss_KL: 0.33857\n",
      "[114/400] [100/349] Loss_D: 0.75073, Loss_G: 2.93515, Loss_KL: 0.29221\n",
      "[114/400] [200/349] Loss_D: 0.46760, Loss_G: 2.77571, Loss_KL: 0.34262\n",
      "[114/400] [300/349] Loss_D: 0.50784, Loss_G: 3.38500, Loss_KL: 0.34505\n",
      "[114/400] Loss_D: 0.66318, Loss_G: 2.78154, Loss_KL: 0.31734\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[115/400] [0/349] Loss_D: 0.78101, Loss_G: 3.02039, Loss_KL: 0.37312\n",
      "[115/400] [100/349] Loss_D: 0.46643, Loss_G: 2.89552, Loss_KL: 0.29576\n",
      "[115/400] [200/349] Loss_D: 0.50239, Loss_G: 2.35018, Loss_KL: 0.30076\n",
      "[115/400] [300/349] Loss_D: 0.82302, Loss_G: 3.13744, Loss_KL: 0.31577\n",
      "[115/400] Loss_D: 0.65006, Loss_G: 2.81816, Loss_KL: 0.32700\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[116/400] [0/349] Loss_D: 0.44722, Loss_G: 3.40969, Loss_KL: 0.33691\n",
      "[116/400] [100/349] Loss_D: 0.59621, Loss_G: 2.51407, Loss_KL: 0.35220\n",
      "[116/400] [200/349] Loss_D: 0.19444, Loss_G: 3.80338, Loss_KL: 0.30962\n",
      "[116/400] [300/349] Loss_D: 0.68005, Loss_G: 2.98677, Loss_KL: 0.34393\n",
      "[116/400] Loss_D: 0.64121, Loss_G: 2.81885, Loss_KL: 0.34050\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[117/400] [0/349] Loss_D: 0.58331, Loss_G: 2.17960, Loss_KL: 0.28785\n",
      "[117/400] [100/349] Loss_D: 0.74466, Loss_G: 2.45530, Loss_KL: 0.30938\n",
      "[117/400] [200/349] Loss_D: 0.41136, Loss_G: 3.30650, Loss_KL: 0.34996\n",
      "[117/400] [300/349] Loss_D: 0.51359, Loss_G: 3.49119, Loss_KL: 0.41572\n",
      "[117/400] Loss_D: 0.66893, Loss_G: 2.85589, Loss_KL: 0.35517\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[118/400] [0/349] Loss_D: 0.57826, Loss_G: 3.55985, Loss_KL: 0.26241\n",
      "[118/400] [100/349] Loss_D: 0.53620, Loss_G: 2.76505, Loss_KL: 0.35131\n",
      "[118/400] [200/349] Loss_D: 0.43519, Loss_G: 3.88479, Loss_KL: 0.33882\n",
      "[118/400] [300/349] Loss_D: 0.75072, Loss_G: 2.82540, Loss_KL: 0.34450\n",
      "[118/400] Loss_D: 0.65125, Loss_G: 2.84150, Loss_KL: 0.35493\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[119/400] [0/349] Loss_D: 0.68353, Loss_G: 2.91964, Loss_KL: 0.58033\n",
      "[119/400] [100/349] Loss_D: 0.40995, Loss_G: 2.49394, Loss_KL: 0.27412\n",
      "[119/400] [200/349] Loss_D: 0.43117, Loss_G: 4.05466, Loss_KL: 0.35142\n",
      "[119/400] [300/349] Loss_D: 0.70979, Loss_G: 2.31540, Loss_KL: 0.29672\n",
      "[119/400] Loss_D: 0.64329, Loss_G: 2.85616, Loss_KL: 0.35043\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[120/400] [0/349] Loss_D: 0.74532, Loss_G: 2.83983, Loss_KL: 0.30163\n",
      "[120/400] [100/349] Loss_D: 0.78482, Loss_G: 3.10156, Loss_KL: 0.44798\n",
      "[120/400] [200/349] Loss_D: 0.66574, Loss_G: 3.30745, Loss_KL: 0.38291\n",
      "[120/400] [300/349] Loss_D: 0.40817, Loss_G: 2.46070, Loss_KL: 0.38277\n",
      "[120/400] Loss_D: 0.64292, Loss_G: 2.90165, Loss_KL: 0.36925\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[121/400] [0/349] Loss_D: 0.80650, Loss_G: 2.79429, Loss_KL: 0.38483\n",
      "[121/400] [100/349] Loss_D: 0.44225, Loss_G: 3.12905, Loss_KL: 0.40437\n",
      "[121/400] [200/349] Loss_D: 0.70210, Loss_G: 2.84589, Loss_KL: 0.35328\n",
      "[121/400] [300/349] Loss_D: 0.39314, Loss_G: 3.18150, Loss_KL: 0.32877\n",
      "[121/400] Loss_D: 0.66052, Loss_G: 2.82115, Loss_KL: 0.34803\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[122/400] [0/349] Loss_D: 0.78352, Loss_G: 2.27136, Loss_KL: 0.34022\n",
      "[122/400] [100/349] Loss_D: 0.73769, Loss_G: 2.69531, Loss_KL: 0.28285\n",
      "[122/400] [200/349] Loss_D: 0.41546, Loss_G: 3.15162, Loss_KL: 0.33896\n",
      "[122/400] [300/349] Loss_D: 0.32421, Loss_G: 2.60376, Loss_KL: 0.37947\n",
      "[122/400] Loss_D: 0.64792, Loss_G: 2.84781, Loss_KL: 0.34749\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[123/400] [0/349] Loss_D: 0.75144, Loss_G: 2.32840, Loss_KL: 0.37468\n",
      "[123/400] [100/349] Loss_D: 0.71772, Loss_G: 2.27360, Loss_KL: 0.32653\n",
      "[123/400] [200/349] Loss_D: 0.80240, Loss_G: 1.93248, Loss_KL: 0.23835\n",
      "[123/400] [300/349] Loss_D: 0.44174, Loss_G: 2.58598, Loss_KL: 0.39950\n",
      "[123/400] Loss_D: 0.65104, Loss_G: 2.84217, Loss_KL: 0.34088\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[124/400] [0/349] Loss_D: 0.88079, Loss_G: 2.51184, Loss_KL: 0.37528\n",
      "[124/400] [100/349] Loss_D: 0.67049, Loss_G: 2.78193, Loss_KL: 0.38924\n",
      "[124/400] [200/349] Loss_D: 0.52088, Loss_G: 3.20759, Loss_KL: 0.35619\n",
      "[124/400] [300/349] Loss_D: 0.24725, Loss_G: 3.06780, Loss_KL: 0.40240\n",
      "[124/400] Loss_D: 0.63995, Loss_G: 2.87372, Loss_KL: 0.35771\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[125/400] [0/349] Loss_D: 0.81144, Loss_G: 2.97956, Loss_KL: 0.45724\n",
      "[125/400] [100/349] Loss_D: 0.70171, Loss_G: 2.76975, Loss_KL: 0.34707\n",
      "[125/400] [200/349] Loss_D: 0.60703, Loss_G: 2.23254, Loss_KL: 0.39953\n",
      "[125/400] [300/349] Loss_D: 0.82725, Loss_G: 2.13878, Loss_KL: 0.33669\n",
      "[125/400] Loss_D: 0.68462, Loss_G: 2.84246, Loss_KL: 0.35960\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[126/400] [0/349] Loss_D: 0.38554, Loss_G: 3.73287, Loss_KL: 0.26996\n",
      "[126/400] [100/349] Loss_D: 0.90801, Loss_G: 2.82222, Loss_KL: 0.33778\n",
      "[126/400] [200/349] Loss_D: 0.70907, Loss_G: 3.11729, Loss_KL: 0.35624\n",
      "[126/400] [300/349] Loss_D: 0.76587, Loss_G: 2.86016, Loss_KL: 0.44863\n",
      "[126/400] Loss_D: 0.65102, Loss_G: 2.86354, Loss_KL: 0.35249\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[127/400] [0/349] Loss_D: 0.86292, Loss_G: 2.21578, Loss_KL: 0.33779\n",
      "[127/400] [100/349] Loss_D: 0.43948, Loss_G: 3.15446, Loss_KL: 0.27563\n",
      "[127/400] [200/349] Loss_D: 0.75227, Loss_G: 3.26044, Loss_KL: 0.34629\n",
      "[127/400] [300/349] Loss_D: 0.61775, Loss_G: 2.60476, Loss_KL: 0.29002\n",
      "[127/400] Loss_D: 0.64496, Loss_G: 2.87191, Loss_KL: 0.35361\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[128/400] [0/349] Loss_D: 0.79825, Loss_G: 2.84791, Loss_KL: 0.34212\n",
      "[128/400] [100/349] Loss_D: 0.76030, Loss_G: 3.16905, Loss_KL: 0.39318\n",
      "[128/400] [200/349] Loss_D: 0.73885, Loss_G: 2.74917, Loss_KL: 0.38349\n",
      "[128/400] [300/349] Loss_D: 1.28588, Loss_G: 3.59145, Loss_KL: 0.36778\n",
      "[128/400] Loss_D: 0.65385, Loss_G: 2.86330, Loss_KL: 0.34892\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[129/400] [0/349] Loss_D: 0.67699, Loss_G: 2.50715, Loss_KL: 0.47013\n",
      "[129/400] [100/349] Loss_D: 0.71654, Loss_G: 2.64564, Loss_KL: 0.34518\n",
      "[129/400] [200/349] Loss_D: 0.44026, Loss_G: 3.03075, Loss_KL: 0.43335\n",
      "[129/400] [300/349] Loss_D: 0.74937, Loss_G: 2.19517, Loss_KL: 0.35059\n",
      "[129/400] Loss_D: 0.67188, Loss_G: 2.84704, Loss_KL: 0.37600\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[130/400] [0/349] Loss_D: 0.75308, Loss_G: 3.26832, Loss_KL: 0.34727\n",
      "[130/400] [100/349] Loss_D: 0.70238, Loss_G: 3.28642, Loss_KL: 0.39553\n",
      "[130/400] [200/349] Loss_D: 0.73153, Loss_G: 2.78104, Loss_KL: 0.46169\n",
      "[130/400] [300/349] Loss_D: 0.52960, Loss_G: 2.79931, Loss_KL: 0.44297\n",
      "[130/400] Loss_D: 0.64425, Loss_G: 2.87240, Loss_KL: 0.37891\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[131/400] [0/349] Loss_D: 1.25620, Loss_G: 2.70914, Loss_KL: 0.36849\n",
      "[131/400] [100/349] Loss_D: 0.73838, Loss_G: 3.27487, Loss_KL: 0.38134\n",
      "[131/400] [200/349] Loss_D: 0.63044, Loss_G: 2.82301, Loss_KL: 0.35533\n",
      "[131/400] [300/349] Loss_D: 0.82157, Loss_G: 3.48688, Loss_KL: 0.47124\n",
      "[131/400] Loss_D: 0.64795, Loss_G: 2.89175, Loss_KL: 0.37694\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[132/400] [0/349] Loss_D: 0.77704, Loss_G: 2.86143, Loss_KL: 0.34358\n",
      "[132/400] [100/349] Loss_D: 0.87478, Loss_G: 2.92666, Loss_KL: 0.36520\n",
      "[132/400] [200/349] Loss_D: 0.48755, Loss_G: 4.11491, Loss_KL: 0.42528\n",
      "[132/400] [300/349] Loss_D: 0.67519, Loss_G: 2.49840, Loss_KL: 0.36563\n",
      "[132/400] Loss_D: 0.64481, Loss_G: 2.88825, Loss_KL: 0.37530\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[133/400] [0/349] Loss_D: 0.16258, Loss_G: 3.39397, Loss_KL: 0.40035\n",
      "[133/400] [100/349] Loss_D: 0.64425, Loss_G: 2.74875, Loss_KL: 0.37355\n",
      "[133/400] [200/349] Loss_D: 0.83622, Loss_G: 3.39682, Loss_KL: 0.39800\n",
      "[133/400] [300/349] Loss_D: 0.87816, Loss_G: 2.29674, Loss_KL: 0.34239\n",
      "[133/400] Loss_D: 0.64361, Loss_G: 2.89659, Loss_KL: 0.37157\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[134/400] [0/349] Loss_D: 0.59549, Loss_G: 2.67618, Loss_KL: 0.36291\n",
      "[134/400] [100/349] Loss_D: 0.72760, Loss_G: 2.42944, Loss_KL: 0.35806\n",
      "[134/400] [200/349] Loss_D: 0.48304, Loss_G: 2.92619, Loss_KL: 0.43589\n",
      "[134/400] [300/349] Loss_D: 0.72204, Loss_G: 2.83453, Loss_KL: 0.31696\n",
      "[134/400] Loss_D: 0.64605, Loss_G: 2.80763, Loss_KL: 0.35233\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[135/400] [0/349] Loss_D: 0.74292, Loss_G: 2.16544, Loss_KL: 0.31521\n",
      "[135/400] [100/349] Loss_D: 0.64464, Loss_G: 3.07805, Loss_KL: 0.37277\n",
      "[135/400] [200/349] Loss_D: 0.82113, Loss_G: 2.35056, Loss_KL: 0.34975\n",
      "[135/400] [300/349] Loss_D: 0.77972, Loss_G: 3.26564, Loss_KL: 0.33470\n",
      "[135/400] Loss_D: 0.65711, Loss_G: 2.85208, Loss_KL: 0.35296\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[136/400] [0/349] Loss_D: 0.74281, Loss_G: 2.65141, Loss_KL: 0.31962\n",
      "[136/400] [100/349] Loss_D: 0.59469, Loss_G: 2.56013, Loss_KL: 0.38358\n",
      "[136/400] [200/349] Loss_D: 0.83188, Loss_G: 3.12241, Loss_KL: 0.36451\n",
      "[136/400] [300/349] Loss_D: 0.41344, Loss_G: 2.36699, Loss_KL: 0.31184\n",
      "[136/400] Loss_D: 0.63850, Loss_G: 2.90431, Loss_KL: 0.36140\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[137/400] [0/349] Loss_D: 0.71067, Loss_G: 3.51803, Loss_KL: 0.37124\n",
      "[137/400] [100/349] Loss_D: 0.79413, Loss_G: 3.12032, Loss_KL: 0.38404\n",
      "[137/400] [200/349] Loss_D: 0.71343, Loss_G: 2.43057, Loss_KL: 0.43682\n",
      "[137/400] [300/349] Loss_D: 0.35094, Loss_G: 2.65307, Loss_KL: 0.42850\n",
      "[137/400] Loss_D: 0.63686, Loss_G: 2.93117, Loss_KL: 0.36929\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[138/400] [0/349] Loss_D: 0.87597, Loss_G: 3.14678, Loss_KL: 0.37538\n",
      "[138/400] [100/349] Loss_D: 0.41632, Loss_G: 2.20409, Loss_KL: 0.36125\n",
      "[138/400] [200/349] Loss_D: 0.29279, Loss_G: 2.67896, Loss_KL: 0.30042\n",
      "[138/400] [300/349] Loss_D: 0.40891, Loss_G: 3.33603, Loss_KL: 0.31203\n",
      "[138/400] Loss_D: 0.65705, Loss_G: 2.85260, Loss_KL: 0.34905\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[139/400] [0/349] Loss_D: 0.78290, Loss_G: 2.65981, Loss_KL: 0.39252\n",
      "[139/400] [100/349] Loss_D: 0.71701, Loss_G: 2.59357, Loss_KL: 0.34797\n",
      "[139/400] [200/349] Loss_D: 0.33463, Loss_G: 2.54870, Loss_KL: 0.34524\n",
      "[139/400] [300/349] Loss_D: 0.46248, Loss_G: 2.57696, Loss_KL: 0.27483\n",
      "[139/400] Loss_D: 0.66442, Loss_G: 2.81301, Loss_KL: 0.34531\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[140/400] [0/349] Loss_D: 0.72549, Loss_G: 3.00238, Loss_KL: 0.36185\n",
      "[140/400] [100/349] Loss_D: 0.64092, Loss_G: 3.45607, Loss_KL: 0.29752\n",
      "[140/400] [200/349] Loss_D: 0.67947, Loss_G: 3.31364, Loss_KL: 0.38849\n",
      "[140/400] [300/349] Loss_D: 0.42299, Loss_G: 2.55651, Loss_KL: 0.32932\n",
      "[140/400] Loss_D: 0.65868, Loss_G: 2.82544, Loss_KL: 0.33841\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[141/400] [0/349] Loss_D: 0.77133, Loss_G: 2.35253, Loss_KL: 0.31077\n",
      "[141/400] [100/349] Loss_D: 0.50890, Loss_G: 3.51200, Loss_KL: 0.36116\n",
      "[141/400] [200/349] Loss_D: 0.78260, Loss_G: 2.56996, Loss_KL: 0.38247\n",
      "[141/400] [300/349] Loss_D: 0.81209, Loss_G: 3.29470, Loss_KL: 0.34255\n",
      "[141/400] Loss_D: 0.65762, Loss_G: 2.84277, Loss_KL: 0.35148\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[142/400] [0/349] Loss_D: 0.93174, Loss_G: 3.40800, Loss_KL: 0.31127\n",
      "[142/400] [100/349] Loss_D: 0.77908, Loss_G: 2.50240, Loss_KL: 0.36154\n",
      "[142/400] [200/349] Loss_D: 0.57615, Loss_G: 3.08744, Loss_KL: 0.34952\n",
      "[142/400] [300/349] Loss_D: 0.37743, Loss_G: 3.09320, Loss_KL: 0.46019\n",
      "[142/400] Loss_D: 0.66156, Loss_G: 2.82659, Loss_KL: 0.36224\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[143/400] [0/349] Loss_D: 0.76196, Loss_G: 2.48251, Loss_KL: 0.30115\n",
      "[143/400] [100/349] Loss_D: 0.76750, Loss_G: 2.36914, Loss_KL: 0.34124\n",
      "[143/400] [200/349] Loss_D: 0.92214, Loss_G: 2.63448, Loss_KL: 0.41121\n",
      "[143/400] [300/349] Loss_D: 0.67599, Loss_G: 2.76285, Loss_KL: 0.30423\n",
      "[143/400] Loss_D: 0.65794, Loss_G: 2.86972, Loss_KL: 0.36140\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[144/400] [0/349] Loss_D: 0.81319, Loss_G: 3.44392, Loss_KL: 0.35475\n",
      "[144/400] [100/349] Loss_D: 0.89627, Loss_G: 3.33538, Loss_KL: 0.31519\n",
      "[144/400] [200/349] Loss_D: 0.77976, Loss_G: 2.84460, Loss_KL: 0.31841\n",
      "[144/400] [300/349] Loss_D: 0.57146, Loss_G: 2.60749, Loss_KL: 0.36747\n",
      "[144/400] Loss_D: 0.66133, Loss_G: 2.89015, Loss_KL: 0.36841\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[145/400] [0/349] Loss_D: 0.84329, Loss_G: 2.28927, Loss_KL: 0.30685\n",
      "[145/400] [100/349] Loss_D: 0.42758, Loss_G: 3.05355, Loss_KL: 0.37877\n",
      "[145/400] [200/349] Loss_D: 0.48925, Loss_G: 3.23807, Loss_KL: 0.37486\n",
      "[145/400] [300/349] Loss_D: 0.28377, Loss_G: 3.33566, Loss_KL: 0.42481\n",
      "[145/400] Loss_D: 0.65305, Loss_G: 2.87233, Loss_KL: 0.36525\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[146/400] [0/349] Loss_D: 0.53440, Loss_G: 2.95747, Loss_KL: 0.33142\n",
      "[146/400] [100/349] Loss_D: 0.65442, Loss_G: 2.59101, Loss_KL: 0.40010\n",
      "[146/400] [200/349] Loss_D: 1.01799, Loss_G: 1.75482, Loss_KL: 0.26612\n",
      "[146/400] [300/349] Loss_D: 0.57625, Loss_G: 2.62084, Loss_KL: 0.34652\n",
      "[146/400] Loss_D: 0.66217, Loss_G: 2.85253, Loss_KL: 0.35433\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[147/400] [0/349] Loss_D: 0.82676, Loss_G: 2.62282, Loss_KL: 0.26063\n",
      "[147/400] [100/349] Loss_D: 0.73507, Loss_G: 3.76214, Loss_KL: 0.35326\n",
      "[147/400] [200/349] Loss_D: 0.77173, Loss_G: 2.99867, Loss_KL: 0.34264\n",
      "[147/400] [300/349] Loss_D: 0.84264, Loss_G: 2.79979, Loss_KL: 0.39883\n",
      "[147/400] Loss_D: 0.65524, Loss_G: 2.85954, Loss_KL: 0.34804\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[148/400] [0/349] Loss_D: 0.54720, Loss_G: 2.52695, Loss_KL: 0.31014\n",
      "[148/400] [100/349] Loss_D: 0.37798, Loss_G: 2.19443, Loss_KL: 0.29190\n",
      "[148/400] [200/349] Loss_D: 0.45573, Loss_G: 3.28777, Loss_KL: 0.32952\n",
      "[148/400] [300/349] Loss_D: 0.91253, Loss_G: 3.66455, Loss_KL: 0.33670\n",
      "[148/400] Loss_D: 0.65219, Loss_G: 2.87050, Loss_KL: 0.35436\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[149/400] [0/349] Loss_D: 0.85492, Loss_G: 1.93496, Loss_KL: 0.44868\n",
      "[149/400] [100/349] Loss_D: 0.80427, Loss_G: 3.23633, Loss_KL: 0.30253\n",
      "[149/400] [200/349] Loss_D: 0.42756, Loss_G: 2.30824, Loss_KL: 0.31842\n",
      "[149/400] [300/349] Loss_D: 0.63434, Loss_G: 2.90801, Loss_KL: 0.32927\n",
      "[149/400] Loss_D: 0.64863, Loss_G: 2.87662, Loss_KL: 0.34815\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[150/400] [0/349] Loss_D: 0.67757, Loss_G: 2.88873, Loss_KL: 0.33621\n",
      "[150/400] [100/349] Loss_D: 0.66291, Loss_G: 3.61827, Loss_KL: 0.31473\n",
      "[150/400] [200/349] Loss_D: 0.65400, Loss_G: 2.23355, Loss_KL: 0.38926\n",
      "[150/400] [300/349] Loss_D: 0.47323, Loss_G: 2.71851, Loss_KL: 0.34229\n",
      "[150/400] Loss_D: 0.67075, Loss_G: 2.87094, Loss_KL: 0.35872\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[151/400] [0/349] Loss_D: 0.61148, Loss_G: 3.22103, Loss_KL: 0.32366\n",
      "[151/400] [100/349] Loss_D: 0.73757, Loss_G: 3.52151, Loss_KL: 0.33298\n",
      "[151/400] [200/349] Loss_D: 0.45757, Loss_G: 3.37329, Loss_KL: 0.33080\n",
      "[151/400] [300/349] Loss_D: 0.66112, Loss_G: 2.95723, Loss_KL: 0.30318\n",
      "[151/400] Loss_D: 0.65856, Loss_G: 2.83977, Loss_KL: 0.35557\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[152/400] [0/349] Loss_D: 0.49631, Loss_G: 4.02890, Loss_KL: 0.32784\n",
      "[152/400] [100/349] Loss_D: 0.83591, Loss_G: 3.12428, Loss_KL: 0.40322\n",
      "[152/400] [200/349] Loss_D: 0.62345, Loss_G: 2.53168, Loss_KL: 0.43233\n",
      "[152/400] [300/349] Loss_D: 0.85862, Loss_G: 2.54669, Loss_KL: 0.38380\n",
      "[152/400] Loss_D: 0.65488, Loss_G: 2.88567, Loss_KL: 0.34392\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[153/400] [0/349] Loss_D: 0.73455, Loss_G: 2.92213, Loss_KL: 0.27674\n",
      "[153/400] [100/349] Loss_D: 0.75056, Loss_G: 3.71713, Loss_KL: 0.43353\n",
      "[153/400] [200/349] Loss_D: 0.66662, Loss_G: 2.19230, Loss_KL: 0.30912\n",
      "[153/400] [300/349] Loss_D: 0.84060, Loss_G: 2.01990, Loss_KL: 0.39818\n",
      "[153/400] Loss_D: 0.65629, Loss_G: 2.89067, Loss_KL: 0.35484\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[154/400] [0/349] Loss_D: 0.35634, Loss_G: 2.63463, Loss_KL: 0.46283\n",
      "[154/400] [100/349] Loss_D: 0.78471, Loss_G: 2.67404, Loss_KL: 0.32150\n",
      "[154/400] [200/349] Loss_D: 0.75796, Loss_G: 3.71983, Loss_KL: 0.34232\n",
      "[154/400] [300/349] Loss_D: 0.55566, Loss_G: 2.15061, Loss_KL: 0.34305\n",
      "[154/400] Loss_D: 0.66470, Loss_G: 2.85897, Loss_KL: 0.36169\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[155/400] [0/349] Loss_D: 0.70092, Loss_G: 2.80139, Loss_KL: 0.38434\n",
      "[155/400] [100/349] Loss_D: 0.60865, Loss_G: 2.76823, Loss_KL: 0.37576\n",
      "[155/400] [200/349] Loss_D: 0.76377, Loss_G: 2.87984, Loss_KL: 0.36175\n",
      "[155/400] [300/349] Loss_D: 0.75160, Loss_G: 2.67596, Loss_KL: 0.41697\n",
      "[155/400] Loss_D: 0.67545, Loss_G: 2.81683, Loss_KL: 0.36197\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[156/400] [0/349] Loss_D: 0.67531, Loss_G: 2.90430, Loss_KL: 0.34625\n",
      "[156/400] [100/349] Loss_D: 0.30536, Loss_G: 2.85450, Loss_KL: 0.35457\n",
      "[156/400] [200/349] Loss_D: 0.76092, Loss_G: 2.91079, Loss_KL: 0.29054\n",
      "[156/400] [300/349] Loss_D: 0.72190, Loss_G: 3.01007, Loss_KL: 0.39314\n",
      "[156/400] Loss_D: 0.65537, Loss_G: 2.82903, Loss_KL: 0.35250\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[157/400] [0/349] Loss_D: 0.64143, Loss_G: 2.44782, Loss_KL: 0.35601\n",
      "[157/400] [100/349] Loss_D: 0.87558, Loss_G: 2.61011, Loss_KL: 0.31246\n",
      "[157/400] [200/349] Loss_D: 0.64471, Loss_G: 2.59048, Loss_KL: 0.27757\n",
      "[157/400] [300/349] Loss_D: 0.78642, Loss_G: 2.79288, Loss_KL: 0.32298\n",
      "[157/400] Loss_D: 0.65598, Loss_G: 2.87971, Loss_KL: 0.35290\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[158/400] [0/349] Loss_D: 0.58593, Loss_G: 2.97576, Loss_KL: 0.34866\n",
      "[158/400] [100/349] Loss_D: 0.70351, Loss_G: 2.83040, Loss_KL: 0.33747\n",
      "[158/400] [200/349] Loss_D: 0.69985, Loss_G: 2.58605, Loss_KL: 0.42929\n",
      "[158/400] [300/349] Loss_D: 0.84082, Loss_G: 2.78219, Loss_KL: 0.32023\n",
      "[158/400] Loss_D: 0.65115, Loss_G: 2.83666, Loss_KL: 0.35066\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[159/400] [0/349] Loss_D: 0.71705, Loss_G: 2.83433, Loss_KL: 0.45328\n",
      "[159/400] [100/349] Loss_D: 0.71112, Loss_G: 2.05499, Loss_KL: 0.30019\n",
      "[159/400] [200/349] Loss_D: 0.77198, Loss_G: 2.65556, Loss_KL: 0.36399\n",
      "[159/400] [300/349] Loss_D: 0.80687, Loss_G: 2.59744, Loss_KL: 0.37569\n",
      "[159/400] Loss_D: 0.64795, Loss_G: 2.88858, Loss_KL: 0.34868\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[160/400] [0/349] Loss_D: 0.59057, Loss_G: 3.13972, Loss_KL: 0.37004\n",
      "[160/400] [100/349] Loss_D: 0.73932, Loss_G: 2.21964, Loss_KL: 0.32038\n",
      "[160/400] [200/349] Loss_D: 0.42292, Loss_G: 2.95168, Loss_KL: 0.31593\n",
      "[160/400] [300/349] Loss_D: 0.42319, Loss_G: 2.22358, Loss_KL: 0.32365\n",
      "[160/400] Loss_D: 0.64480, Loss_G: 2.88573, Loss_KL: 0.33402\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[161/400] [0/349] Loss_D: 0.65279, Loss_G: 2.56153, Loss_KL: 0.40949\n",
      "[161/400] [100/349] Loss_D: 0.66474, Loss_G: 3.33355, Loss_KL: 0.40974\n",
      "[161/400] [200/349] Loss_D: 0.71103, Loss_G: 2.79831, Loss_KL: 0.35184\n",
      "[161/400] [300/349] Loss_D: 0.85587, Loss_G: 3.67338, Loss_KL: 0.28558\n",
      "[161/400] Loss_D: 0.65754, Loss_G: 2.83723, Loss_KL: 0.33793\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[162/400] [0/349] Loss_D: 0.72937, Loss_G: 2.68677, Loss_KL: 0.35235\n",
      "[162/400] [100/349] Loss_D: 0.78181, Loss_G: 2.55544, Loss_KL: 0.29529\n",
      "[162/400] [200/349] Loss_D: 0.42696, Loss_G: 3.65764, Loss_KL: 0.35046\n",
      "[162/400] [300/349] Loss_D: 0.76923, Loss_G: 2.52226, Loss_KL: 0.28287\n",
      "[162/400] Loss_D: 0.63670, Loss_G: 2.89433, Loss_KL: 0.33960\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[163/400] [0/349] Loss_D: 0.26733, Loss_G: 2.96075, Loss_KL: 0.26487\n",
      "[163/400] [100/349] Loss_D: 0.95618, Loss_G: 2.32114, Loss_KL: 0.36182\n",
      "[163/400] [200/349] Loss_D: 0.72122, Loss_G: 2.46965, Loss_KL: 0.36077\n",
      "[163/400] [300/349] Loss_D: 0.93824, Loss_G: 2.88232, Loss_KL: 0.32597\n",
      "[163/400] Loss_D: 0.64234, Loss_G: 2.89074, Loss_KL: 0.34051\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[164/400] [0/349] Loss_D: 0.67509, Loss_G: 2.56362, Loss_KL: 0.31260\n",
      "[164/400] [100/349] Loss_D: 0.84724, Loss_G: 2.11676, Loss_KL: 0.27910\n",
      "[164/400] [200/349] Loss_D: 0.74826, Loss_G: 2.66424, Loss_KL: 0.37257\n",
      "[164/400] [300/349] Loss_D: 0.65869, Loss_G: 2.96748, Loss_KL: 0.53549\n",
      "[164/400] Loss_D: 0.66480, Loss_G: 2.85162, Loss_KL: 0.34082\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[165/400] [0/349] Loss_D: 0.79417, Loss_G: 2.45381, Loss_KL: 0.35552\n",
      "[165/400] [100/349] Loss_D: 0.73063, Loss_G: 2.55407, Loss_KL: 0.33521\n",
      "[165/400] [200/349] Loss_D: 0.73891, Loss_G: 2.66230, Loss_KL: 0.32150\n",
      "[165/400] [300/349] Loss_D: 0.79623, Loss_G: 3.17604, Loss_KL: 0.41356\n",
      "[165/400] Loss_D: 0.66159, Loss_G: 2.88357, Loss_KL: 0.34777\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[166/400] [0/349] Loss_D: 0.69097, Loss_G: 2.88494, Loss_KL: 0.34110\n",
      "[166/400] [100/349] Loss_D: 0.43965, Loss_G: 2.91365, Loss_KL: 0.36479\n",
      "[166/400] [200/349] Loss_D: 0.80912, Loss_G: 2.17628, Loss_KL: 0.34388\n",
      "[166/400] [300/349] Loss_D: 0.79338, Loss_G: 2.60892, Loss_KL: 0.38096\n",
      "[166/400] Loss_D: 0.64873, Loss_G: 2.84376, Loss_KL: 0.33741\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[167/400] [0/349] Loss_D: 0.43772, Loss_G: 2.64203, Loss_KL: 0.34076\n",
      "[167/400] [100/349] Loss_D: 0.77392, Loss_G: 2.90832, Loss_KL: 0.29609\n",
      "[167/400] [200/349] Loss_D: 1.07057, Loss_G: 2.45466, Loss_KL: 0.38146\n",
      "[167/400] [300/349] Loss_D: 0.57943, Loss_G: 2.06997, Loss_KL: 0.35759\n",
      "[167/400] Loss_D: 0.67172, Loss_G: 2.84438, Loss_KL: 0.33552\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[168/400] [0/349] Loss_D: 0.84648, Loss_G: 2.47736, Loss_KL: 0.30618\n",
      "[168/400] [100/349] Loss_D: 0.59595, Loss_G: 3.61103, Loss_KL: 0.27850\n",
      "[168/400] [200/349] Loss_D: 0.47086, Loss_G: 3.50730, Loss_KL: 0.29380\n",
      "[168/400] [300/349] Loss_D: 0.85653, Loss_G: 2.61098, Loss_KL: 0.28775\n",
      "[168/400] Loss_D: 0.65150, Loss_G: 2.88384, Loss_KL: 0.32278\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[169/400] [0/349] Loss_D: 0.33596, Loss_G: 3.10743, Loss_KL: 0.37968\n",
      "[169/400] [100/349] Loss_D: 0.47716, Loss_G: 3.89742, Loss_KL: 0.33450\n",
      "[169/400] [200/349] Loss_D: 0.54377, Loss_G: 2.48516, Loss_KL: 0.36951\n",
      "[169/400] [300/349] Loss_D: 0.30904, Loss_G: 4.17127, Loss_KL: 0.29631\n",
      "[169/400] Loss_D: 0.65965, Loss_G: 2.84515, Loss_KL: 0.33367\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[170/400] [0/349] Loss_D: 0.76144, Loss_G: 2.83049, Loss_KL: 0.38312\n",
      "[170/400] [100/349] Loss_D: 0.72546, Loss_G: 2.86750, Loss_KL: 0.36343\n",
      "[170/400] [200/349] Loss_D: 0.64848, Loss_G: 2.89934, Loss_KL: 0.37766\n",
      "[170/400] [300/349] Loss_D: 0.48692, Loss_G: 2.77834, Loss_KL: 0.30733\n",
      "[170/400] Loss_D: 0.63918, Loss_G: 2.86540, Loss_KL: 0.34132\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[171/400] [0/349] Loss_D: 0.37699, Loss_G: 3.50190, Loss_KL: 0.52958\n",
      "[171/400] [100/349] Loss_D: 0.84858, Loss_G: 3.13836, Loss_KL: 0.33725\n",
      "[171/400] [200/349] Loss_D: 0.45842, Loss_G: 3.20018, Loss_KL: 0.28445\n",
      "[171/400] [300/349] Loss_D: 0.67165, Loss_G: 2.83339, Loss_KL: 0.30580\n",
      "[171/400] Loss_D: 0.65327, Loss_G: 2.88976, Loss_KL: 0.33227\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[172/400] [0/349] Loss_D: 0.65166, Loss_G: 3.20889, Loss_KL: 0.41020\n",
      "[172/400] [100/349] Loss_D: 0.79429, Loss_G: 2.51854, Loss_KL: 0.27466\n",
      "[172/400] [200/349] Loss_D: 0.42100, Loss_G: 3.07848, Loss_KL: 0.32706\n",
      "[172/400] [300/349] Loss_D: 0.76416, Loss_G: 3.32841, Loss_KL: 0.34480\n",
      "[172/400] Loss_D: 0.64588, Loss_G: 2.87572, Loss_KL: 0.33843\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[173/400] [0/349] Loss_D: 0.85379, Loss_G: 3.24775, Loss_KL: 0.40042\n",
      "[173/400] [100/349] Loss_D: 0.75287, Loss_G: 2.11754, Loss_KL: 0.29689\n",
      "[173/400] [200/349] Loss_D: 0.71181, Loss_G: 3.16493, Loss_KL: 0.35630\n",
      "[173/400] [300/349] Loss_D: 0.44097, Loss_G: 3.37167, Loss_KL: 0.34858\n",
      "[173/400] Loss_D: 0.66440, Loss_G: 2.85640, Loss_KL: 0.33889\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[174/400] [0/349] Loss_D: 0.39946, Loss_G: 3.15220, Loss_KL: 0.36545\n",
      "[174/400] [100/349] Loss_D: 0.78538, Loss_G: 2.80866, Loss_KL: 0.31278\n",
      "[174/400] [200/349] Loss_D: 0.78822, Loss_G: 2.56749, Loss_KL: 0.37624\n",
      "[174/400] [300/349] Loss_D: 0.50018, Loss_G: 2.37471, Loss_KL: 0.37258\n",
      "[174/400] Loss_D: 0.66847, Loss_G: 2.85308, Loss_KL: 0.35791\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[175/400] [0/349] Loss_D: 0.63627, Loss_G: 2.71484, Loss_KL: 0.30454\n",
      "[175/400] [100/349] Loss_D: 0.52430, Loss_G: 2.32921, Loss_KL: 0.29368\n",
      "[175/400] [200/349] Loss_D: 0.69006, Loss_G: 2.83245, Loss_KL: 0.37872\n",
      "[175/400] [300/349] Loss_D: 0.62420, Loss_G: 2.78670, Loss_KL: 0.33919\n",
      "[175/400] Loss_D: 0.65064, Loss_G: 2.90676, Loss_KL: 0.34973\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[176/400] [0/349] Loss_D: 0.55073, Loss_G: 3.46913, Loss_KL: 0.38025\n",
      "[176/400] [100/349] Loss_D: 0.64086, Loss_G: 3.57710, Loss_KL: 0.33341\n",
      "[176/400] [200/349] Loss_D: 0.60291, Loss_G: 2.93806, Loss_KL: 0.34007\n",
      "[176/400] [300/349] Loss_D: 0.87735, Loss_G: 3.01469, Loss_KL: 0.31091\n",
      "[176/400] Loss_D: 0.64827, Loss_G: 2.89487, Loss_KL: 0.34385\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[177/400] [0/349] Loss_D: 0.73747, Loss_G: 2.45096, Loss_KL: 0.28748\n",
      "[177/400] [100/349] Loss_D: 0.46352, Loss_G: 3.36286, Loss_KL: 0.28489\n",
      "[177/400] [200/349] Loss_D: 0.87672, Loss_G: 3.35012, Loss_KL: 0.37070\n",
      "[177/400] [300/349] Loss_D: 0.78732, Loss_G: 2.78014, Loss_KL: 0.34819\n",
      "[177/400] Loss_D: 0.64983, Loss_G: 2.87636, Loss_KL: 0.34900\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[178/400] [0/349] Loss_D: 0.67808, Loss_G: 3.63442, Loss_KL: 0.36080\n",
      "[178/400] [100/349] Loss_D: 0.85778, Loss_G: 2.54729, Loss_KL: 0.32332\n",
      "[178/400] [200/349] Loss_D: 0.82720, Loss_G: 2.59776, Loss_KL: 0.29220\n",
      "[178/400] [300/349] Loss_D: 0.81925, Loss_G: 3.03889, Loss_KL: 0.30918\n",
      "[178/400] Loss_D: 0.64553, Loss_G: 2.87634, Loss_KL: 0.33409\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[179/400] [0/349] Loss_D: 0.44265, Loss_G: 2.67713, Loss_KL: 0.36354\n",
      "[179/400] [100/349] Loss_D: 0.74354, Loss_G: 2.19979, Loss_KL: 0.35733\n",
      "[179/400] [200/349] Loss_D: 0.99515, Loss_G: 2.28102, Loss_KL: 0.34677\n",
      "[179/400] [300/349] Loss_D: 0.62237, Loss_G: 3.29511, Loss_KL: 0.30357\n",
      "[179/400] Loss_D: 0.65345, Loss_G: 2.92165, Loss_KL: 0.33900\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[180/400] [0/349] Loss_D: 0.92417, Loss_G: 2.95252, Loss_KL: 0.34471\n",
      "[180/400] [100/349] Loss_D: 1.22053, Loss_G: 3.48393, Loss_KL: 0.36849\n",
      "[180/400] [200/349] Loss_D: 0.51115, Loss_G: 3.43555, Loss_KL: 0.34532\n",
      "[180/400] [300/349] Loss_D: 0.42686, Loss_G: 2.46227, Loss_KL: 0.37892\n",
      "[180/400] Loss_D: 0.64800, Loss_G: 2.87145, Loss_KL: 0.34057\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[181/400] [0/349] Loss_D: 0.55752, Loss_G: 2.34332, Loss_KL: 0.33880\n",
      "[181/400] [100/349] Loss_D: 0.30377, Loss_G: 3.07163, Loss_KL: 0.29485\n",
      "[181/400] [200/349] Loss_D: 0.77432, Loss_G: 2.90251, Loss_KL: 0.27443\n",
      "[181/400] [300/349] Loss_D: 0.85634, Loss_G: 3.74498, Loss_KL: 0.33656\n",
      "[181/400] Loss_D: 0.64455, Loss_G: 2.86829, Loss_KL: 0.33037\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[182/400] [0/349] Loss_D: 0.74816, Loss_G: 2.69896, Loss_KL: 0.32266\n",
      "[182/400] [100/349] Loss_D: 0.70595, Loss_G: 2.88484, Loss_KL: 0.32449\n",
      "[182/400] [200/349] Loss_D: 0.46894, Loss_G: 3.18590, Loss_KL: 0.28704\n",
      "[182/400] [300/349] Loss_D: 0.64522, Loss_G: 2.36880, Loss_KL: 0.32516\n",
      "[182/400] Loss_D: 0.64072, Loss_G: 2.86181, Loss_KL: 0.32666\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[183/400] [0/349] Loss_D: 0.69621, Loss_G: 2.55801, Loss_KL: 0.35254\n",
      "[183/400] [100/349] Loss_D: 0.65880, Loss_G: 2.75674, Loss_KL: 0.31723\n",
      "[183/400] [200/349] Loss_D: 0.89473, Loss_G: 1.95258, Loss_KL: 0.36211\n",
      "[183/400] [300/349] Loss_D: 0.93724, Loss_G: 3.28523, Loss_KL: 0.31576\n",
      "[183/400] Loss_D: 0.64586, Loss_G: 2.87280, Loss_KL: 0.33640\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[184/400] [0/349] Loss_D: 0.47027, Loss_G: 2.42011, Loss_KL: 0.33471\n",
      "[184/400] [100/349] Loss_D: 0.77928, Loss_G: 2.84613, Loss_KL: 0.36033\n",
      "[184/400] [200/349] Loss_D: 0.75430, Loss_G: 3.48956, Loss_KL: 0.36851\n",
      "[184/400] [300/349] Loss_D: 0.61607, Loss_G: 2.81842, Loss_KL: 0.40177\n",
      "[184/400] Loss_D: 0.65078, Loss_G: 2.87297, Loss_KL: 0.33143\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[185/400] [0/349] Loss_D: 0.54890, Loss_G: 2.63955, Loss_KL: 0.32150\n",
      "[185/400] [100/349] Loss_D: 0.82802, Loss_G: 2.69233, Loss_KL: 0.31140\n",
      "[185/400] [200/349] Loss_D: 0.78100, Loss_G: 3.19793, Loss_KL: 0.37677\n",
      "[185/400] [300/349] Loss_D: 0.79700, Loss_G: 2.31313, Loss_KL: 0.30454\n",
      "[185/400] Loss_D: 0.62965, Loss_G: 2.90522, Loss_KL: 0.31717\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[186/400] [0/349] Loss_D: 0.52093, Loss_G: 2.90958, Loss_KL: 0.42816\n",
      "[186/400] [100/349] Loss_D: 0.18211, Loss_G: 4.01370, Loss_KL: 0.27415\n",
      "[186/400] [200/349] Loss_D: 0.86955, Loss_G: 2.86438, Loss_KL: 0.29612\n",
      "[186/400] [300/349] Loss_D: 0.32112, Loss_G: 2.83049, Loss_KL: 0.40803\n",
      "[186/400] Loss_D: 0.64760, Loss_G: 2.88393, Loss_KL: 0.33088\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[187/400] [0/349] Loss_D: 0.46949, Loss_G: 3.27073, Loss_KL: 0.27371\n",
      "[187/400] [100/349] Loss_D: 0.78248, Loss_G: 2.29445, Loss_KL: 0.35043\n",
      "[187/400] [200/349] Loss_D: 0.63470, Loss_G: 2.89090, Loss_KL: 0.28094\n",
      "[187/400] [300/349] Loss_D: 0.85389, Loss_G: 1.93743, Loss_KL: 0.27818\n",
      "[187/400] Loss_D: 0.64242, Loss_G: 2.87481, Loss_KL: 0.31474\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[188/400] [0/349] Loss_D: 0.57636, Loss_G: 3.49607, Loss_KL: 0.41157\n",
      "[188/400] [100/349] Loss_D: 0.64596, Loss_G: 2.36630, Loss_KL: 0.38111\n",
      "[188/400] [200/349] Loss_D: 0.81948, Loss_G: 3.09785, Loss_KL: 0.35263\n",
      "[188/400] [300/349] Loss_D: 0.80265, Loss_G: 2.88805, Loss_KL: 0.30482\n",
      "[188/400] Loss_D: 0.63994, Loss_G: 2.88654, Loss_KL: 0.32010\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[189/400] [0/349] Loss_D: 0.48750, Loss_G: 2.33312, Loss_KL: 0.35614\n",
      "[189/400] [100/349] Loss_D: 0.78671, Loss_G: 2.16890, Loss_KL: 0.30052\n",
      "[189/400] [200/349] Loss_D: 0.32607, Loss_G: 3.13738, Loss_KL: 0.37242\n",
      "[189/400] [300/349] Loss_D: 0.90014, Loss_G: 2.56450, Loss_KL: 0.33379\n",
      "[189/400] Loss_D: 0.65733, Loss_G: 2.89523, Loss_KL: 0.33336\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[190/400] [0/349] Loss_D: 0.60403, Loss_G: 2.43832, Loss_KL: 0.35038\n",
      "[190/400] [100/349] Loss_D: 0.99664, Loss_G: 2.94884, Loss_KL: 0.30973\n",
      "[190/400] [200/349] Loss_D: 0.42632, Loss_G: 2.63880, Loss_KL: 0.28297\n",
      "[190/400] [300/349] Loss_D: 0.79520, Loss_G: 2.45342, Loss_KL: 0.42121\n",
      "[190/400] Loss_D: 0.64778, Loss_G: 2.85602, Loss_KL: 0.32557\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[191/400] [0/349] Loss_D: 0.49305, Loss_G: 2.46251, Loss_KL: 0.37343\n",
      "[191/400] [100/349] Loss_D: 0.67164, Loss_G: 3.69396, Loss_KL: 0.32243\n",
      "[191/400] [200/349] Loss_D: 0.47004, Loss_G: 3.36958, Loss_KL: 0.31486\n",
      "[191/400] [300/349] Loss_D: 0.59221, Loss_G: 2.65071, Loss_KL: 0.26849\n",
      "[191/400] Loss_D: 0.64983, Loss_G: 2.86313, Loss_KL: 0.32056\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[192/400] [0/349] Loss_D: 0.73882, Loss_G: 3.51947, Loss_KL: 0.33609\n",
      "[192/400] [100/349] Loss_D: 0.68934, Loss_G: 3.63206, Loss_KL: 0.27093\n",
      "[192/400] [200/349] Loss_D: 0.61601, Loss_G: 3.73499, Loss_KL: 0.37294\n",
      "[192/400] [300/349] Loss_D: 0.60709, Loss_G: 1.97383, Loss_KL: 0.26108\n",
      "[192/400] Loss_D: 0.65164, Loss_G: 2.90189, Loss_KL: 0.32475\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[193/400] [0/349] Loss_D: 0.76555, Loss_G: 2.57034, Loss_KL: 0.39598\n",
      "[193/400] [100/349] Loss_D: 0.83997, Loss_G: 2.11993, Loss_KL: 0.30196\n",
      "[193/400] [200/349] Loss_D: 0.70819, Loss_G: 2.65636, Loss_KL: 0.36288\n",
      "[193/400] [300/349] Loss_D: 0.64988, Loss_G: 2.67818, Loss_KL: 0.35217\n",
      "[193/400] Loss_D: 0.65348, Loss_G: 2.84519, Loss_KL: 0.32894\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[194/400] [0/349] Loss_D: 0.94095, Loss_G: 2.39776, Loss_KL: 0.28177\n",
      "[194/400] [100/349] Loss_D: 0.42970, Loss_G: 3.25947, Loss_KL: 0.28636\n",
      "[194/400] [200/349] Loss_D: 0.80307, Loss_G: 3.37799, Loss_KL: 0.34089\n",
      "[194/400] [300/349] Loss_D: 0.37940, Loss_G: 2.98276, Loss_KL: 0.30537\n",
      "[194/400] Loss_D: 0.65517, Loss_G: 2.82213, Loss_KL: 0.31136\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[195/400] [0/349] Loss_D: 0.73175, Loss_G: 2.84207, Loss_KL: 0.29870\n",
      "[195/400] [100/349] Loss_D: 0.70768, Loss_G: 2.56208, Loss_KL: 0.30511\n",
      "[195/400] [200/349] Loss_D: 0.74509, Loss_G: 2.66275, Loss_KL: 0.27297\n",
      "[195/400] [300/349] Loss_D: 0.76922, Loss_G: 2.72686, Loss_KL: 0.28620\n",
      "[195/400] Loss_D: 0.64855, Loss_G: 2.85798, Loss_KL: 0.29680\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[196/400] [0/349] Loss_D: 0.64116, Loss_G: 2.89517, Loss_KL: 0.30549\n",
      "[196/400] [100/349] Loss_D: 0.74371, Loss_G: 3.45987, Loss_KL: 0.31359\n",
      "[196/400] [200/349] Loss_D: 0.42142, Loss_G: 2.72687, Loss_KL: 0.33972\n",
      "[196/400] [300/349] Loss_D: 0.58616, Loss_G: 2.85940, Loss_KL: 0.30010\n",
      "[196/400] Loss_D: 0.63753, Loss_G: 2.88764, Loss_KL: 0.31063\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[197/400] [0/349] Loss_D: 0.37172, Loss_G: 2.60123, Loss_KL: 0.27617\n",
      "[197/400] [100/349] Loss_D: 0.69313, Loss_G: 2.73957, Loss_KL: 0.34499\n",
      "[197/400] [200/349] Loss_D: 0.89078, Loss_G: 2.47836, Loss_KL: 0.28847\n",
      "[197/400] [300/349] Loss_D: 0.47085, Loss_G: 2.47266, Loss_KL: 0.33174\n",
      "[197/400] Loss_D: 0.63364, Loss_G: 2.85031, Loss_KL: 0.32193\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[198/400] [0/349] Loss_D: 0.82053, Loss_G: 2.85094, Loss_KL: 0.44591\n",
      "[198/400] [100/349] Loss_D: 0.43087, Loss_G: 3.19476, Loss_KL: 0.25033\n",
      "[198/400] [200/349] Loss_D: 0.59627, Loss_G: 3.47090, Loss_KL: 0.30871\n",
      "[198/400] [300/349] Loss_D: 0.88443, Loss_G: 3.03110, Loss_KL: 0.25684\n",
      "[198/400] Loss_D: 0.64356, Loss_G: 2.87869, Loss_KL: 0.32536\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[199/400] [0/349] Loss_D: 0.49029, Loss_G: 2.81079, Loss_KL: 0.39478\n",
      "[199/400] [100/349] Loss_D: 0.53216, Loss_G: 2.99293, Loss_KL: 0.33656\n",
      "[199/400] [200/349] Loss_D: 0.31015, Loss_G: 3.58570, Loss_KL: 0.34150\n",
      "[199/400] [300/349] Loss_D: 0.81094, Loss_G: 2.72267, Loss_KL: 0.30628\n",
      "[199/400] Loss_D: 0.64675, Loss_G: 2.89545, Loss_KL: 0.31623\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[200/400] [0/349] Loss_D: 0.33054, Loss_G: 3.71022, Loss_KL: 0.25866\n",
      "[200/400] [100/349] Loss_D: 0.14784, Loss_G: 3.72565, Loss_KL: 0.31558\n",
      "[200/400] [200/349] Loss_D: 0.79580, Loss_G: 2.67188, Loss_KL: 0.34173\n",
      "[200/400] [300/349] Loss_D: 0.77688, Loss_G: 2.52426, Loss_KL: 0.32565\n",
      "[200/400] Loss_D: 0.63488, Loss_G: 2.90300, Loss_KL: 0.31177\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[201/400] [0/349] Loss_D: 0.47591, Loss_G: 3.13070, Loss_KL: 0.30223\n",
      "[201/400] [100/349] Loss_D: 0.73081, Loss_G: 2.55538, Loss_KL: 0.25056\n",
      "[201/400] [200/349] Loss_D: 0.45329, Loss_G: 3.29278, Loss_KL: 0.28696\n",
      "[201/400] [300/349] Loss_D: 0.56468, Loss_G: 2.30308, Loss_KL: 0.35498\n",
      "[201/400] Loss_D: 0.62472, Loss_G: 2.71900, Loss_KL: 0.31178\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[202/400] [0/349] Loss_D: 0.72100, Loss_G: 2.44873, Loss_KL: 0.29204\n",
      "[202/400] [100/349] Loss_D: 0.34721, Loss_G: 3.10352, Loss_KL: 0.45247\n",
      "[202/400] [200/349] Loss_D: 0.49809, Loss_G: 2.50131, Loss_KL: 0.27621\n",
      "[202/400] [300/349] Loss_D: 0.74537, Loss_G: 2.43561, Loss_KL: 0.28447\n",
      "[202/400] Loss_D: 0.62664, Loss_G: 2.72113, Loss_KL: 0.31014\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[203/400] [0/349] Loss_D: 0.56041, Loss_G: 2.83240, Loss_KL: 0.28299\n",
      "[203/400] [100/349] Loss_D: 0.69034, Loss_G: 3.02342, Loss_KL: 0.37308\n",
      "[203/400] [200/349] Loss_D: 0.29859, Loss_G: 3.05304, Loss_KL: 0.25091\n",
      "[203/400] [300/349] Loss_D: 0.78320, Loss_G: 2.78164, Loss_KL: 0.37755\n",
      "[203/400] Loss_D: 0.61659, Loss_G: 2.74652, Loss_KL: 0.31824\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[204/400] [0/349] Loss_D: 0.91042, Loss_G: 3.03168, Loss_KL: 0.29581\n",
      "[204/400] [100/349] Loss_D: 0.74749, Loss_G: 2.97401, Loss_KL: 0.33019\n",
      "[204/400] [200/349] Loss_D: 0.24286, Loss_G: 2.91665, Loss_KL: 0.24893\n",
      "[204/400] [300/349] Loss_D: 0.39602, Loss_G: 3.21201, Loss_KL: 0.30395\n",
      "[204/400] Loss_D: 0.63632, Loss_G: 2.77072, Loss_KL: 0.31581\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[205/400] [0/349] Loss_D: 0.29333, Loss_G: 2.66935, Loss_KL: 0.39812\n",
      "[205/400] [100/349] Loss_D: 0.82022, Loss_G: 2.73881, Loss_KL: 0.27031\n",
      "[205/400] [200/349] Loss_D: 0.54558, Loss_G: 2.70561, Loss_KL: 0.33745\n",
      "[205/400] [300/349] Loss_D: 0.37016, Loss_G: 3.63367, Loss_KL: 0.34071\n",
      "[205/400] Loss_D: 0.61844, Loss_G: 2.77634, Loss_KL: 0.32317\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[206/400] [0/349] Loss_D: 0.56910, Loss_G: 3.16815, Loss_KL: 0.25029\n",
      "[206/400] [100/349] Loss_D: 0.51868, Loss_G: 2.59505, Loss_KL: 0.35737\n",
      "[206/400] [200/349] Loss_D: 0.76726, Loss_G: 2.84354, Loss_KL: 0.30119\n",
      "[206/400] [300/349] Loss_D: 0.80221, Loss_G: 2.68384, Loss_KL: 0.36108\n",
      "[206/400] Loss_D: 0.61145, Loss_G: 2.78508, Loss_KL: 0.33619\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[207/400] [0/349] Loss_D: 0.84758, Loss_G: 2.09835, Loss_KL: 0.33217\n",
      "[207/400] [100/349] Loss_D: 0.65638, Loss_G: 2.98390, Loss_KL: 0.31650\n",
      "[207/400] [200/349] Loss_D: 0.79964, Loss_G: 2.69895, Loss_KL: 0.37959\n",
      "[207/400] [300/349] Loss_D: 0.63546, Loss_G: 3.42417, Loss_KL: 0.46134\n",
      "[207/400] Loss_D: 0.63439, Loss_G: 2.75040, Loss_KL: 0.33963\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[208/400] [0/349] Loss_D: 0.87488, Loss_G: 1.95627, Loss_KL: 0.39922\n",
      "[208/400] [100/349] Loss_D: 0.25363, Loss_G: 4.03554, Loss_KL: 0.42120\n",
      "[208/400] [200/349] Loss_D: 0.71966, Loss_G: 2.67836, Loss_KL: 0.42325\n",
      "[208/400] [300/349] Loss_D: 0.43215, Loss_G: 2.57352, Loss_KL: 0.30827\n",
      "[208/400] Loss_D: 0.60131, Loss_G: 2.84894, Loss_KL: 0.34175\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[209/400] [0/349] Loss_D: 0.43354, Loss_G: 2.85310, Loss_KL: 0.34858\n",
      "[209/400] [100/349] Loss_D: 0.51884, Loss_G: 2.75370, Loss_KL: 0.38226\n",
      "[209/400] [200/349] Loss_D: 0.72858, Loss_G: 2.18502, Loss_KL: 0.29555\n",
      "[209/400] [300/349] Loss_D: 0.41798, Loss_G: 3.21008, Loss_KL: 0.31331\n",
      "[209/400] Loss_D: 0.62706, Loss_G: 2.75842, Loss_KL: 0.33842\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[210/400] [0/349] Loss_D: 0.35916, Loss_G: 3.13139, Loss_KL: 0.40653\n",
      "[210/400] [100/349] Loss_D: 0.76075, Loss_G: 2.46239, Loss_KL: 0.28534\n",
      "[210/400] [200/349] Loss_D: 0.25838, Loss_G: 2.78366, Loss_KL: 0.41625\n",
      "[210/400] [300/349] Loss_D: 0.61008, Loss_G: 2.33566, Loss_KL: 0.31288\n",
      "[210/400] Loss_D: 0.63198, Loss_G: 2.76225, Loss_KL: 0.33247\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[211/400] [0/349] Loss_D: 0.47872, Loss_G: 2.64720, Loss_KL: 0.31136\n",
      "[211/400] [100/349] Loss_D: 0.59947, Loss_G: 3.83346, Loss_KL: 0.28723\n",
      "[211/400] [200/349] Loss_D: 0.41919, Loss_G: 4.25187, Loss_KL: 0.38344\n",
      "[211/400] [300/349] Loss_D: 0.26574, Loss_G: 3.89983, Loss_KL: 0.32521\n",
      "[211/400] Loss_D: 0.61618, Loss_G: 2.80151, Loss_KL: 0.33315\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[212/400] [0/349] Loss_D: 0.78424, Loss_G: 3.27665, Loss_KL: 0.32635\n",
      "[212/400] [100/349] Loss_D: 0.78844, Loss_G: 2.09318, Loss_KL: 0.32931\n",
      "[212/400] [200/349] Loss_D: 0.63454, Loss_G: 2.14673, Loss_KL: 0.33789\n",
      "[212/400] [300/349] Loss_D: 0.40864, Loss_G: 2.63629, Loss_KL: 0.27466\n",
      "[212/400] Loss_D: 0.61218, Loss_G: 2.80711, Loss_KL: 0.33817\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[213/400] [0/349] Loss_D: 0.35045, Loss_G: 2.96685, Loss_KL: 0.37466\n",
      "[213/400] [100/349] Loss_D: 0.71387, Loss_G: 3.42738, Loss_KL: 0.31194\n",
      "[213/400] [200/349] Loss_D: 0.71076, Loss_G: 2.36452, Loss_KL: 0.30721\n",
      "[213/400] [300/349] Loss_D: 0.48025, Loss_G: 2.51484, Loss_KL: 0.34011\n",
      "[213/400] Loss_D: 0.61745, Loss_G: 2.83408, Loss_KL: 0.34716\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[214/400] [0/349] Loss_D: 0.79439, Loss_G: 2.23069, Loss_KL: 0.37263\n",
      "[214/400] [100/349] Loss_D: 0.45147, Loss_G: 3.04849, Loss_KL: 0.33780\n",
      "[214/400] [200/349] Loss_D: 0.66676, Loss_G: 3.50557, Loss_KL: 0.43081\n",
      "[214/400] [300/349] Loss_D: 0.23002, Loss_G: 3.40149, Loss_KL: 0.39767\n",
      "[214/400] Loss_D: 0.62552, Loss_G: 2.77658, Loss_KL: 0.34964\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[215/400] [0/349] Loss_D: 0.83135, Loss_G: 3.28316, Loss_KL: 0.37575\n",
      "[215/400] [100/349] Loss_D: 0.81102, Loss_G: 3.09234, Loss_KL: 0.40525\n",
      "[215/400] [200/349] Loss_D: 0.68958, Loss_G: 2.81160, Loss_KL: 0.33216\n",
      "[215/400] [300/349] Loss_D: 0.48871, Loss_G: 2.43456, Loss_KL: 0.31178\n",
      "[215/400] Loss_D: 0.61616, Loss_G: 2.79595, Loss_KL: 0.33692\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[216/400] [0/349] Loss_D: 0.74201, Loss_G: 3.06317, Loss_KL: 0.38565\n",
      "[216/400] [100/349] Loss_D: 0.52291, Loss_G: 2.54981, Loss_KL: 0.29361\n",
      "[216/400] [200/349] Loss_D: 0.76851, Loss_G: 2.18897, Loss_KL: 0.34140\n",
      "[216/400] [300/349] Loss_D: 0.20625, Loss_G: 2.61165, Loss_KL: 0.37317\n",
      "[216/400] Loss_D: 0.62522, Loss_G: 2.78997, Loss_KL: 0.33016\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[217/400] [0/349] Loss_D: 0.80250, Loss_G: 3.41370, Loss_KL: 0.28000\n",
      "[217/400] [100/349] Loss_D: 0.62110, Loss_G: 2.26633, Loss_KL: 0.33508\n",
      "[217/400] [200/349] Loss_D: 0.19637, Loss_G: 3.31660, Loss_KL: 0.31913\n",
      "[217/400] [300/349] Loss_D: 0.74178, Loss_G: 2.89996, Loss_KL: 0.32749\n",
      "[217/400] Loss_D: 0.63801, Loss_G: 2.78104, Loss_KL: 0.32599\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[218/400] [0/349] Loss_D: 0.63780, Loss_G: 2.34603, Loss_KL: 0.31520\n",
      "[218/400] [100/349] Loss_D: 0.69479, Loss_G: 2.58612, Loss_KL: 0.32282\n",
      "[218/400] [200/349] Loss_D: 0.42716, Loss_G: 2.67888, Loss_KL: 0.38234\n",
      "[218/400] [300/349] Loss_D: 0.45674, Loss_G: 2.29898, Loss_KL: 0.38583\n",
      "[218/400] Loss_D: 0.61891, Loss_G: 2.80288, Loss_KL: 0.32543\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[219/400] [0/349] Loss_D: 0.37134, Loss_G: 3.37462, Loss_KL: 0.29424\n",
      "[219/400] [100/349] Loss_D: 0.68511, Loss_G: 3.73663, Loss_KL: 0.35296\n",
      "[219/400] [200/349] Loss_D: 0.38177, Loss_G: 2.80591, Loss_KL: 0.37752\n",
      "[219/400] [300/349] Loss_D: 0.63983, Loss_G: 2.72194, Loss_KL: 0.27432\n",
      "[219/400] Loss_D: 0.62540, Loss_G: 2.79862, Loss_KL: 0.32637\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[220/400] [0/349] Loss_D: 0.47214, Loss_G: 2.89193, Loss_KL: 0.35200\n",
      "[220/400] [100/349] Loss_D: 0.64814, Loss_G: 3.04905, Loss_KL: 0.33358\n",
      "[220/400] [200/349] Loss_D: 0.60025, Loss_G: 2.49835, Loss_KL: 0.32767\n",
      "[220/400] [300/349] Loss_D: 0.86340, Loss_G: 2.54855, Loss_KL: 0.32879\n",
      "[220/400] Loss_D: 0.63314, Loss_G: 2.77953, Loss_KL: 0.33082\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[221/400] [0/349] Loss_D: 0.90813, Loss_G: 2.14704, Loss_KL: 0.32636\n",
      "[221/400] [100/349] Loss_D: 0.70813, Loss_G: 3.31546, Loss_KL: 0.30258\n",
      "[221/400] [200/349] Loss_D: 0.64001, Loss_G: 2.44281, Loss_KL: 0.31593\n",
      "[221/400] [300/349] Loss_D: 0.55601, Loss_G: 2.39200, Loss_KL: 0.33028\n",
      "[221/400] Loss_D: 0.60984, Loss_G: 2.79814, Loss_KL: 0.32762\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[222/400] [0/349] Loss_D: 0.68199, Loss_G: 2.71934, Loss_KL: 0.39183\n",
      "[222/400] [100/349] Loss_D: 0.72581, Loss_G: 2.28113, Loss_KL: 0.30884\n",
      "[222/400] [200/349] Loss_D: 0.42235, Loss_G: 2.37035, Loss_KL: 0.31752\n",
      "[222/400] [300/349] Loss_D: 0.61628, Loss_G: 2.57186, Loss_KL: 0.35199\n",
      "[222/400] Loss_D: 0.62404, Loss_G: 2.80425, Loss_KL: 0.33604\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[223/400] [0/349] Loss_D: 0.51898, Loss_G: 2.63509, Loss_KL: 0.36864\n",
      "[223/400] [100/349] Loss_D: 0.38078, Loss_G: 3.39971, Loss_KL: 0.31906\n",
      "[223/400] [200/349] Loss_D: 0.69761, Loss_G: 3.03829, Loss_KL: 0.28145\n",
      "[223/400] [300/349] Loss_D: 0.55055, Loss_G: 3.01504, Loss_KL: 0.34735\n",
      "[223/400] Loss_D: 0.60677, Loss_G: 2.82405, Loss_KL: 0.34517\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[224/400] [0/349] Loss_D: 0.73022, Loss_G: 2.57947, Loss_KL: 0.36143\n",
      "[224/400] [100/349] Loss_D: 0.79379, Loss_G: 2.37688, Loss_KL: 0.27630\n",
      "[224/400] [200/349] Loss_D: 0.69167, Loss_G: 3.38670, Loss_KL: 0.43658\n",
      "[224/400] [300/349] Loss_D: 0.76834, Loss_G: 2.03770, Loss_KL: 0.38373\n",
      "[224/400] Loss_D: 0.62555, Loss_G: 2.82152, Loss_KL: 0.34603\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[225/400] [0/349] Loss_D: 0.50675, Loss_G: 3.27856, Loss_KL: 0.35323\n",
      "[225/400] [100/349] Loss_D: 0.62687, Loss_G: 2.48165, Loss_KL: 0.45994\n",
      "[225/400] [200/349] Loss_D: 0.61973, Loss_G: 3.17681, Loss_KL: 0.42169\n",
      "[225/400] [300/349] Loss_D: 0.77022, Loss_G: 2.44960, Loss_KL: 0.29262\n",
      "[225/400] Loss_D: 0.62199, Loss_G: 2.82639, Loss_KL: 0.34021\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[226/400] [0/349] Loss_D: 0.49146, Loss_G: 2.81470, Loss_KL: 0.43285\n",
      "[226/400] [100/349] Loss_D: 0.65829, Loss_G: 2.85123, Loss_KL: 0.41379\n",
      "[226/400] [200/349] Loss_D: 0.58865, Loss_G: 2.77723, Loss_KL: 0.34169\n",
      "[226/400] [300/349] Loss_D: 0.56634, Loss_G: 2.73837, Loss_KL: 0.39034\n",
      "[226/400] Loss_D: 0.61848, Loss_G: 2.82191, Loss_KL: 0.33834\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[227/400] [0/349] Loss_D: 0.81737, Loss_G: 2.40279, Loss_KL: 0.37631\n",
      "[227/400] [100/349] Loss_D: 0.29939, Loss_G: 2.63899, Loss_KL: 0.29499\n",
      "[227/400] [200/349] Loss_D: 0.73299, Loss_G: 2.36341, Loss_KL: 0.32501\n",
      "[227/400] [300/349] Loss_D: 0.60046, Loss_G: 2.98373, Loss_KL: 0.29224\n",
      "[227/400] Loss_D: 0.62031, Loss_G: 2.80888, Loss_KL: 0.33404\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[228/400] [0/349] Loss_D: 0.70175, Loss_G: 2.98499, Loss_KL: 0.33807\n",
      "[228/400] [100/349] Loss_D: 0.58514, Loss_G: 2.42222, Loss_KL: 0.34395\n",
      "[228/400] [200/349] Loss_D: 0.59515, Loss_G: 2.88694, Loss_KL: 0.37432\n",
      "[228/400] [300/349] Loss_D: 0.78036, Loss_G: 2.60103, Loss_KL: 0.40136\n",
      "[228/400] Loss_D: 0.61999, Loss_G: 2.83537, Loss_KL: 0.33332\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[229/400] [0/349] Loss_D: 0.74149, Loss_G: 2.48823, Loss_KL: 0.29542\n",
      "[229/400] [100/349] Loss_D: 0.64611, Loss_G: 2.43818, Loss_KL: 0.36573\n",
      "[229/400] [200/349] Loss_D: 0.45745, Loss_G: 2.71300, Loss_KL: 0.36198\n",
      "[229/400] [300/349] Loss_D: 0.70131, Loss_G: 3.45136, Loss_KL: 0.31457\n",
      "[229/400] Loss_D: 0.62116, Loss_G: 2.81680, Loss_KL: 0.33987\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[230/400] [0/349] Loss_D: 0.50808, Loss_G: 2.48808, Loss_KL: 0.39683\n",
      "[230/400] [100/349] Loss_D: 0.41199, Loss_G: 2.47339, Loss_KL: 0.29243\n",
      "[230/400] [200/349] Loss_D: 0.70007, Loss_G: 2.76416, Loss_KL: 0.32723\n",
      "[230/400] [300/349] Loss_D: 0.59186, Loss_G: 2.63454, Loss_KL: 0.30620\n",
      "[230/400] Loss_D: 0.60887, Loss_G: 2.81587, Loss_KL: 0.33942\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[231/400] [0/349] Loss_D: 0.84486, Loss_G: 2.71120, Loss_KL: 0.37208\n",
      "[231/400] [100/349] Loss_D: 0.47762, Loss_G: 3.80446, Loss_KL: 0.29510\n",
      "[231/400] [200/349] Loss_D: 0.75750, Loss_G: 3.70106, Loss_KL: 0.37039\n",
      "[231/400] [300/349] Loss_D: 0.48130, Loss_G: 2.40283, Loss_KL: 0.33070\n",
      "[231/400] Loss_D: 0.62313, Loss_G: 2.83315, Loss_KL: 0.34027\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[232/400] [0/349] Loss_D: 0.74957, Loss_G: 2.68579, Loss_KL: 0.33894\n",
      "[232/400] [100/349] Loss_D: 0.57129, Loss_G: 2.31011, Loss_KL: 0.34946\n",
      "[232/400] [200/349] Loss_D: 0.43731, Loss_G: 3.03946, Loss_KL: 0.34159\n",
      "[232/400] [300/349] Loss_D: 0.73909, Loss_G: 2.70260, Loss_KL: 0.37437\n",
      "[232/400] Loss_D: 0.62528, Loss_G: 2.81151, Loss_KL: 0.34216\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[233/400] [0/349] Loss_D: 0.77234, Loss_G: 2.23254, Loss_KL: 0.29781\n",
      "[233/400] [100/349] Loss_D: 0.36034, Loss_G: 2.74426, Loss_KL: 0.34660\n",
      "[233/400] [200/349] Loss_D: 0.67100, Loss_G: 2.67140, Loss_KL: 0.34650\n",
      "[233/400] [300/349] Loss_D: 0.74393, Loss_G: 2.09842, Loss_KL: 0.33228\n",
      "[233/400] Loss_D: 0.61345, Loss_G: 2.83971, Loss_KL: 0.34336\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[234/400] [0/349] Loss_D: 0.62656, Loss_G: 2.88183, Loss_KL: 0.28323\n",
      "[234/400] [100/349] Loss_D: 0.81814, Loss_G: 2.61675, Loss_KL: 0.46772\n",
      "[234/400] [200/349] Loss_D: 0.40632, Loss_G: 2.80708, Loss_KL: 0.41111\n",
      "[234/400] [300/349] Loss_D: 0.57027, Loss_G: 2.72361, Loss_KL: 0.41382\n",
      "[234/400] Loss_D: 0.62146, Loss_G: 2.80234, Loss_KL: 0.34460\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[235/400] [0/349] Loss_D: 0.57021, Loss_G: 2.25990, Loss_KL: 0.32027\n",
      "[235/400] [100/349] Loss_D: 0.42918, Loss_G: 2.99754, Loss_KL: 0.40006\n",
      "[235/400] [200/349] Loss_D: 0.72067, Loss_G: 3.01389, Loss_KL: 0.34450\n",
      "[235/400] [300/349] Loss_D: 0.69610, Loss_G: 2.59547, Loss_KL: 0.37344\n",
      "[235/400] Loss_D: 0.61067, Loss_G: 2.83879, Loss_KL: 0.34969\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[236/400] [0/349] Loss_D: 0.33991, Loss_G: 3.54842, Loss_KL: 0.34907\n",
      "[236/400] [100/349] Loss_D: 0.58184, Loss_G: 2.36254, Loss_KL: 0.38069\n",
      "[236/400] [200/349] Loss_D: 0.67273, Loss_G: 3.08467, Loss_KL: 0.34874\n",
      "[236/400] [300/349] Loss_D: 0.41447, Loss_G: 2.81185, Loss_KL: 0.37974\n",
      "[236/400] Loss_D: 0.60964, Loss_G: 2.85999, Loss_KL: 0.35328\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[237/400] [0/349] Loss_D: 0.37297, Loss_G: 3.15816, Loss_KL: 0.32936\n",
      "[237/400] [100/349] Loss_D: 0.58269, Loss_G: 3.00018, Loss_KL: 0.40028\n",
      "[237/400] [200/349] Loss_D: 0.35163, Loss_G: 2.68715, Loss_KL: 0.31971\n",
      "[237/400] [300/349] Loss_D: 0.69784, Loss_G: 2.45834, Loss_KL: 0.29842\n",
      "[237/400] Loss_D: 0.60903, Loss_G: 2.81920, Loss_KL: 0.34308\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[238/400] [0/349] Loss_D: 0.81275, Loss_G: 2.04997, Loss_KL: 0.33764\n",
      "[238/400] [100/349] Loss_D: 0.72756, Loss_G: 2.43331, Loss_KL: 0.36397\n",
      "[238/400] [200/349] Loss_D: 0.68027, Loss_G: 3.47274, Loss_KL: 0.31060\n",
      "[238/400] [300/349] Loss_D: 0.56672, Loss_G: 2.19262, Loss_KL: 0.40786\n",
      "[238/400] Loss_D: 0.62583, Loss_G: 2.79523, Loss_KL: 0.33971\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[239/400] [0/349] Loss_D: 0.81703, Loss_G: 2.19549, Loss_KL: 0.31404\n",
      "[239/400] [100/349] Loss_D: 0.61422, Loss_G: 3.00282, Loss_KL: 0.34792\n",
      "[239/400] [200/349] Loss_D: 0.54265, Loss_G: 2.47241, Loss_KL: 0.33666\n",
      "[239/400] [300/349] Loss_D: 0.74753, Loss_G: 2.69685, Loss_KL: 0.27594\n",
      "[239/400] Loss_D: 0.62810, Loss_G: 2.84960, Loss_KL: 0.33987\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[240/400] [0/349] Loss_D: 0.66406, Loss_G: 2.73929, Loss_KL: 0.34842\n",
      "[240/400] [100/349] Loss_D: 0.80687, Loss_G: 2.55351, Loss_KL: 0.35462\n",
      "[240/400] [200/349] Loss_D: 0.36605, Loss_G: 3.57166, Loss_KL: 0.38688\n",
      "[240/400] [300/349] Loss_D: 0.67234, Loss_G: 2.67233, Loss_KL: 0.39460\n",
      "[240/400] Loss_D: 0.61959, Loss_G: 2.81966, Loss_KL: 0.33871\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[241/400] [0/349] Loss_D: 0.53074, Loss_G: 2.58138, Loss_KL: 0.28004\n",
      "[241/400] [100/349] Loss_D: 0.58353, Loss_G: 2.58312, Loss_KL: 0.32635\n",
      "[241/400] [200/349] Loss_D: 0.75429, Loss_G: 2.87330, Loss_KL: 0.29965\n",
      "[241/400] [300/349] Loss_D: 0.47003, Loss_G: 2.48742, Loss_KL: 0.34275\n",
      "[241/400] Loss_D: 0.61430, Loss_G: 2.88217, Loss_KL: 0.33260\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[242/400] [0/349] Loss_D: 0.57175, Loss_G: 2.39260, Loss_KL: 0.34475\n",
      "[242/400] [100/349] Loss_D: 0.54970, Loss_G: 2.98926, Loss_KL: 0.24353\n",
      "[242/400] [200/349] Loss_D: 0.46385, Loss_G: 2.87974, Loss_KL: 0.29321\n",
      "[242/400] [300/349] Loss_D: 0.54774, Loss_G: 3.34715, Loss_KL: 0.24640\n",
      "[242/400] Loss_D: 0.62165, Loss_G: 2.85060, Loss_KL: 0.32592\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[243/400] [0/349] Loss_D: 0.75902, Loss_G: 3.33845, Loss_KL: 0.32144\n",
      "[243/400] [100/349] Loss_D: 0.72575, Loss_G: 3.13598, Loss_KL: 0.32177\n",
      "[243/400] [200/349] Loss_D: 0.70258, Loss_G: 2.63719, Loss_KL: 0.30964\n",
      "[243/400] [300/349] Loss_D: 0.46725, Loss_G: 2.79807, Loss_KL: 0.27558\n",
      "[243/400] Loss_D: 0.62423, Loss_G: 2.79828, Loss_KL: 0.32561\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[244/400] [0/349] Loss_D: 0.30980, Loss_G: 2.65899, Loss_KL: 0.37365\n",
      "[244/400] [100/349] Loss_D: 0.64375, Loss_G: 3.32510, Loss_KL: 0.28660\n",
      "[244/400] [200/349] Loss_D: 0.38443, Loss_G: 3.35401, Loss_KL: 0.28777\n",
      "[244/400] [300/349] Loss_D: 0.60916, Loss_G: 2.51334, Loss_KL: 0.32352\n",
      "[244/400] Loss_D: 0.59954, Loss_G: 2.89418, Loss_KL: 0.32919\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[245/400] [0/349] Loss_D: 0.47992, Loss_G: 2.82960, Loss_KL: 0.30224\n",
      "[245/400] [100/349] Loss_D: 0.57020, Loss_G: 3.12133, Loss_KL: 0.24825\n",
      "[245/400] [200/349] Loss_D: 0.59868, Loss_G: 3.35801, Loss_KL: 0.33181\n",
      "[245/400] [300/349] Loss_D: 0.69254, Loss_G: 3.03473, Loss_KL: 0.27551\n",
      "[245/400] Loss_D: 0.62471, Loss_G: 2.80880, Loss_KL: 0.33062\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[246/400] [0/349] Loss_D: 0.61198, Loss_G: 3.16033, Loss_KL: 0.36187\n",
      "[246/400] [100/349] Loss_D: 0.75509, Loss_G: 2.11229, Loss_KL: 0.33338\n",
      "[246/400] [200/349] Loss_D: 0.69751, Loss_G: 2.69080, Loss_KL: 0.43085\n",
      "[246/400] [300/349] Loss_D: 0.73621, Loss_G: 2.65867, Loss_KL: 0.25121\n",
      "[246/400] Loss_D: 0.60467, Loss_G: 2.86928, Loss_KL: 0.33121\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[247/400] [0/349] Loss_D: 0.87540, Loss_G: 3.41843, Loss_KL: 0.32622\n",
      "[247/400] [100/349] Loss_D: 0.47399, Loss_G: 2.97656, Loss_KL: 0.35263\n",
      "[247/400] [200/349] Loss_D: 0.58842, Loss_G: 2.25678, Loss_KL: 0.29732\n",
      "[247/400] [300/349] Loss_D: 0.59205, Loss_G: 3.22404, Loss_KL: 0.25578\n",
      "[247/400] Loss_D: 0.62768, Loss_G: 2.82463, Loss_KL: 0.32902\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[248/400] [0/349] Loss_D: 0.70299, Loss_G: 2.28564, Loss_KL: 0.34479\n",
      "[248/400] [100/349] Loss_D: 0.67385, Loss_G: 2.99436, Loss_KL: 0.38707\n",
      "[248/400] [200/349] Loss_D: 0.68014, Loss_G: 2.53580, Loss_KL: 0.30487\n",
      "[248/400] [300/349] Loss_D: 0.50720, Loss_G: 3.09620, Loss_KL: 0.30044\n",
      "[248/400] Loss_D: 0.61220, Loss_G: 2.84495, Loss_KL: 0.33258\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[249/400] [0/349] Loss_D: 0.74221, Loss_G: 2.74919, Loss_KL: 0.31293\n",
      "[249/400] [100/349] Loss_D: 0.56830, Loss_G: 2.97470, Loss_KL: 0.31882\n",
      "[249/400] [200/349] Loss_D: 0.66401, Loss_G: 2.83714, Loss_KL: 0.32294\n",
      "[249/400] [300/349] Loss_D: 0.57914, Loss_G: 2.70124, Loss_KL: 0.28711\n",
      "[249/400] Loss_D: 0.62173, Loss_G: 2.80620, Loss_KL: 0.32669\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[250/400] [0/349] Loss_D: 0.70697, Loss_G: 2.47576, Loss_KL: 0.25865\n",
      "[250/400] [100/349] Loss_D: 0.72837, Loss_G: 2.49425, Loss_KL: 0.31971\n",
      "[250/400] [200/349] Loss_D: 0.45136, Loss_G: 3.26215, Loss_KL: 0.35347\n",
      "[250/400] [300/349] Loss_D: 0.76673, Loss_G: 2.91283, Loss_KL: 0.27493\n",
      "[250/400] Loss_D: 0.62392, Loss_G: 2.85106, Loss_KL: 0.32448\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[251/400] [0/349] Loss_D: 0.40098, Loss_G: 3.60290, Loss_KL: 0.32237\n",
      "[251/400] [100/349] Loss_D: 0.73801, Loss_G: 2.28085, Loss_KL: 0.34541\n",
      "[251/400] [200/349] Loss_D: 0.74724, Loss_G: 3.38550, Loss_KL: 0.32990\n",
      "[251/400] [300/349] Loss_D: 0.64566, Loss_G: 2.65254, Loss_KL: 0.33511\n",
      "[251/400] Loss_D: 0.61608, Loss_G: 2.81144, Loss_KL: 0.32361\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[252/400] [0/349] Loss_D: 0.72397, Loss_G: 2.81246, Loss_KL: 0.32886\n",
      "[252/400] [100/349] Loss_D: 0.77224, Loss_G: 2.66943, Loss_KL: 0.45078\n",
      "[252/400] [200/349] Loss_D: 0.70577, Loss_G: 2.75879, Loss_KL: 0.35114\n",
      "[252/400] [300/349] Loss_D: 0.74653, Loss_G: 2.48153, Loss_KL: 0.35629\n",
      "[252/400] Loss_D: 0.62608, Loss_G: 2.76038, Loss_KL: 0.32387\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[253/400] [0/349] Loss_D: 0.83466, Loss_G: 2.84872, Loss_KL: 0.36535\n",
      "[253/400] [100/349] Loss_D: 0.86402, Loss_G: 2.74693, Loss_KL: 0.28927\n",
      "[253/400] [200/349] Loss_D: 0.34122, Loss_G: 3.28753, Loss_KL: 0.37341\n",
      "[253/400] [300/349] Loss_D: 0.71270, Loss_G: 2.93195, Loss_KL: 0.40099\n",
      "[253/400] Loss_D: 0.61376, Loss_G: 2.86310, Loss_KL: 0.31971\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[254/400] [0/349] Loss_D: 0.63510, Loss_G: 2.83612, Loss_KL: 0.28380\n",
      "[254/400] [100/349] Loss_D: 0.65312, Loss_G: 3.40706, Loss_KL: 0.26896\n",
      "[254/400] [200/349] Loss_D: 0.72404, Loss_G: 2.05693, Loss_KL: 0.28395\n",
      "[254/400] [300/349] Loss_D: 0.55058, Loss_G: 2.75522, Loss_KL: 0.33334\n",
      "[254/400] Loss_D: 0.62450, Loss_G: 2.79762, Loss_KL: 0.32555\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[255/400] [0/349] Loss_D: 0.78506, Loss_G: 2.58756, Loss_KL: 0.26035\n",
      "[255/400] [100/349] Loss_D: 0.55653, Loss_G: 2.53234, Loss_KL: 0.31042\n",
      "[255/400] [200/349] Loss_D: 0.72946, Loss_G: 2.97788, Loss_KL: 0.29974\n",
      "[255/400] [300/349] Loss_D: 0.53016, Loss_G: 2.58840, Loss_KL: 0.29600\n",
      "[255/400] Loss_D: 0.61215, Loss_G: 2.81734, Loss_KL: 0.32236\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[256/400] [0/349] Loss_D: 0.80270, Loss_G: 3.50825, Loss_KL: 0.40433\n",
      "[256/400] [100/349] Loss_D: 0.52437, Loss_G: 2.68874, Loss_KL: 0.39890\n",
      "[256/400] [200/349] Loss_D: 0.72150, Loss_G: 3.23883, Loss_KL: 0.31290\n",
      "[256/400] [300/349] Loss_D: 0.38939, Loss_G: 2.56980, Loss_KL: 0.27683\n",
      "[256/400] Loss_D: 0.60713, Loss_G: 2.85959, Loss_KL: 0.32634\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[257/400] [0/349] Loss_D: 0.28787, Loss_G: 3.24773, Loss_KL: 0.32051\n",
      "[257/400] [100/349] Loss_D: 0.60192, Loss_G: 2.49232, Loss_KL: 0.33668\n",
      "[257/400] [200/349] Loss_D: 0.77405, Loss_G: 3.07973, Loss_KL: 0.32746\n",
      "[257/400] [300/349] Loss_D: 0.69012, Loss_G: 3.13194, Loss_KL: 0.36833\n",
      "[257/400] Loss_D: 0.61282, Loss_G: 2.83009, Loss_KL: 0.32970\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[258/400] [0/349] Loss_D: 0.85882, Loss_G: 2.29608, Loss_KL: 0.24247\n",
      "[258/400] [100/349] Loss_D: 0.71784, Loss_G: 2.69336, Loss_KL: 0.43822\n",
      "[258/400] [200/349] Loss_D: 0.74263, Loss_G: 2.43952, Loss_KL: 0.30744\n",
      "[258/400] [300/349] Loss_D: 0.50389, Loss_G: 2.35567, Loss_KL: 0.33470\n",
      "[258/400] Loss_D: 0.63176, Loss_G: 2.83471, Loss_KL: 0.33348\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[259/400] [0/349] Loss_D: 0.61230, Loss_G: 2.44105, Loss_KL: 0.31431\n",
      "[259/400] [100/349] Loss_D: 0.77465, Loss_G: 3.71281, Loss_KL: 0.28344\n",
      "[259/400] [200/349] Loss_D: 0.53469, Loss_G: 3.15861, Loss_KL: 0.32234\n",
      "[259/400] [300/349] Loss_D: 0.79703, Loss_G: 2.18382, Loss_KL: 0.34889\n",
      "[259/400] Loss_D: 0.63661, Loss_G: 2.82113, Loss_KL: 0.33465\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[260/400] [0/349] Loss_D: 0.47485, Loss_G: 2.26308, Loss_KL: 0.37485\n",
      "[260/400] [100/349] Loss_D: 0.72596, Loss_G: 3.08624, Loss_KL: 0.33504\n",
      "[260/400] [200/349] Loss_D: 0.73399, Loss_G: 2.21810, Loss_KL: 0.30189\n",
      "[260/400] [300/349] Loss_D: 0.52533, Loss_G: 2.45766, Loss_KL: 0.28878\n",
      "[260/400] Loss_D: 0.61260, Loss_G: 2.81422, Loss_KL: 0.33408\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[261/400] [0/349] Loss_D: 0.24590, Loss_G: 2.68975, Loss_KL: 0.39829\n",
      "[261/400] [100/349] Loss_D: 0.52790, Loss_G: 2.61587, Loss_KL: 0.30197\n",
      "[261/400] [200/349] Loss_D: 0.33475, Loss_G: 2.55867, Loss_KL: 0.34964\n",
      "[261/400] [300/349] Loss_D: 0.16776, Loss_G: 2.84076, Loss_KL: 0.35249\n",
      "[261/400] Loss_D: 0.61925, Loss_G: 2.83310, Loss_KL: 0.32419\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[262/400] [0/349] Loss_D: 0.47310, Loss_G: 3.27070, Loss_KL: 0.40488\n",
      "[262/400] [100/349] Loss_D: 0.50633, Loss_G: 2.89470, Loss_KL: 0.31533\n",
      "[262/400] [200/349] Loss_D: 0.74295, Loss_G: 2.86355, Loss_KL: 0.36387\n",
      "[262/400] [300/349] Loss_D: 0.31304, Loss_G: 3.00642, Loss_KL: 0.34420\n",
      "[262/400] Loss_D: 0.60157, Loss_G: 2.86162, Loss_KL: 0.32973\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[263/400] [0/349] Loss_D: 0.36805, Loss_G: 2.86845, Loss_KL: 0.44673\n",
      "[263/400] [100/349] Loss_D: 0.68488, Loss_G: 2.44231, Loss_KL: 0.27704\n",
      "[263/400] [200/349] Loss_D: 0.61700, Loss_G: 2.60351, Loss_KL: 0.29403\n",
      "[263/400] [300/349] Loss_D: 0.68350, Loss_G: 2.64977, Loss_KL: 0.29995\n",
      "[263/400] Loss_D: 0.61451, Loss_G: 2.85055, Loss_KL: 0.33600\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[264/400] [0/349] Loss_D: 0.65940, Loss_G: 3.45508, Loss_KL: 0.31847\n",
      "[264/400] [100/349] Loss_D: 0.75053, Loss_G: 2.48231, Loss_KL: 0.37939\n",
      "[264/400] [200/349] Loss_D: 0.78777, Loss_G: 2.96885, Loss_KL: 0.40215\n",
      "[264/400] [300/349] Loss_D: 0.56504, Loss_G: 2.52685, Loss_KL: 0.37916\n",
      "[264/400] Loss_D: 0.61454, Loss_G: 2.86436, Loss_KL: 0.33855\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[265/400] [0/349] Loss_D: 0.84027, Loss_G: 2.38660, Loss_KL: 0.42386\n",
      "[265/400] [100/349] Loss_D: 0.56724, Loss_G: 2.96320, Loss_KL: 0.29854\n",
      "[265/400] [200/349] Loss_D: 0.78550, Loss_G: 2.75246, Loss_KL: 0.34237\n",
      "[265/400] [300/349] Loss_D: 0.78912, Loss_G: 2.47934, Loss_KL: 0.30822\n",
      "[265/400] Loss_D: 0.62246, Loss_G: 2.84593, Loss_KL: 0.34611\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[266/400] [0/349] Loss_D: 0.75706, Loss_G: 3.17815, Loss_KL: 0.26760\n",
      "[266/400] [100/349] Loss_D: 0.74286, Loss_G: 2.80325, Loss_KL: 0.35507\n",
      "[266/400] [200/349] Loss_D: 0.67897, Loss_G: 2.33605, Loss_KL: 0.40936\n",
      "[266/400] [300/349] Loss_D: 0.83296, Loss_G: 2.17072, Loss_KL: 0.34861\n",
      "[266/400] Loss_D: 0.61336, Loss_G: 2.84977, Loss_KL: 0.34170\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[267/400] [0/349] Loss_D: 0.68933, Loss_G: 2.99932, Loss_KL: 0.31500\n",
      "[267/400] [100/349] Loss_D: 0.79980, Loss_G: 2.46690, Loss_KL: 0.39172\n",
      "[267/400] [200/349] Loss_D: 0.33776, Loss_G: 2.43156, Loss_KL: 0.30547\n",
      "[267/400] [300/349] Loss_D: 0.49715, Loss_G: 3.29747, Loss_KL: 0.29253\n",
      "[267/400] Loss_D: 0.62906, Loss_G: 2.83479, Loss_KL: 0.33495\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[268/400] [0/349] Loss_D: 0.59565, Loss_G: 2.98661, Loss_KL: 0.34712\n",
      "[268/400] [100/349] Loss_D: 0.70391, Loss_G: 2.25871, Loss_KL: 0.33346\n",
      "[268/400] [200/349] Loss_D: 0.74758, Loss_G: 2.43241, Loss_KL: 0.26380\n",
      "[268/400] [300/349] Loss_D: 0.68296, Loss_G: 2.94539, Loss_KL: 0.37724\n",
      "[268/400] Loss_D: 0.62941, Loss_G: 2.80661, Loss_KL: 0.33418\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[269/400] [0/349] Loss_D: 0.74744, Loss_G: 2.41763, Loss_KL: 0.32790\n",
      "[269/400] [100/349] Loss_D: 0.66227, Loss_G: 2.63143, Loss_KL: 0.38305\n",
      "[269/400] [200/349] Loss_D: 0.50378, Loss_G: 2.59466, Loss_KL: 0.41187\n",
      "[269/400] [300/349] Loss_D: 0.41914, Loss_G: 2.18798, Loss_KL: 0.34418\n",
      "[269/400] Loss_D: 0.61991, Loss_G: 2.82762, Loss_KL: 0.33569\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[270/400] [0/349] Loss_D: 0.40091, Loss_G: 3.12716, Loss_KL: 0.27786\n",
      "[270/400] [100/349] Loss_D: 0.65489, Loss_G: 2.27716, Loss_KL: 0.33252\n",
      "[270/400] [200/349] Loss_D: 0.25815, Loss_G: 3.09488, Loss_KL: 0.33209\n",
      "[270/400] [300/349] Loss_D: 0.75681, Loss_G: 2.17331, Loss_KL: 0.38558\n",
      "[270/400] Loss_D: 0.60687, Loss_G: 2.86289, Loss_KL: 0.33558\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[271/400] [0/349] Loss_D: 0.48898, Loss_G: 3.11060, Loss_KL: 0.32108\n",
      "[271/400] [100/349] Loss_D: 0.66640, Loss_G: 2.29375, Loss_KL: 0.40030\n",
      "[271/400] [200/349] Loss_D: 0.28679, Loss_G: 3.30227, Loss_KL: 0.31121\n",
      "[271/400] [300/349] Loss_D: 0.69514, Loss_G: 2.97614, Loss_KL: 0.35237\n",
      "[271/400] Loss_D: 0.60353, Loss_G: 2.88050, Loss_KL: 0.34004\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[272/400] [0/349] Loss_D: 0.71212, Loss_G: 2.70877, Loss_KL: 0.35085\n",
      "[272/400] [100/349] Loss_D: 0.76441, Loss_G: 2.34483, Loss_KL: 0.34095\n",
      "[272/400] [200/349] Loss_D: 0.70482, Loss_G: 2.43372, Loss_KL: 0.36638\n",
      "[272/400] [300/349] Loss_D: 0.60265, Loss_G: 2.92401, Loss_KL: 0.33001\n",
      "[272/400] Loss_D: 0.61020, Loss_G: 2.84834, Loss_KL: 0.34264\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[273/400] [0/349] Loss_D: 0.42999, Loss_G: 3.77423, Loss_KL: 0.29528\n",
      "[273/400] [100/349] Loss_D: 0.63072, Loss_G: 3.18514, Loss_KL: 0.25788\n",
      "[273/400] [200/349] Loss_D: 0.57826, Loss_G: 2.42197, Loss_KL: 0.26904\n",
      "[273/400] [300/349] Loss_D: 0.46331, Loss_G: 2.50948, Loss_KL: 0.27681\n",
      "[273/400] Loss_D: 0.60769, Loss_G: 2.87107, Loss_KL: 0.33581\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[274/400] [0/349] Loss_D: 0.62197, Loss_G: 2.12021, Loss_KL: 0.26619\n",
      "[274/400] [100/349] Loss_D: 0.63927, Loss_G: 2.46020, Loss_KL: 0.29602\n",
      "[274/400] [200/349] Loss_D: 0.70362, Loss_G: 2.58099, Loss_KL: 0.39562\n",
      "[274/400] [300/349] Loss_D: 0.53146, Loss_G: 2.69644, Loss_KL: 0.37575\n",
      "[274/400] Loss_D: 0.62108, Loss_G: 2.84438, Loss_KL: 0.33091\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[275/400] [0/349] Loss_D: 0.31945, Loss_G: 3.34205, Loss_KL: 0.25168\n",
      "[275/400] [100/349] Loss_D: 0.32946, Loss_G: 2.54316, Loss_KL: 0.31953\n",
      "[275/400] [200/349] Loss_D: 0.39388, Loss_G: 3.47859, Loss_KL: 0.44041\n",
      "[275/400] [300/349] Loss_D: 0.68927, Loss_G: 2.62898, Loss_KL: 0.26596\n",
      "[275/400] Loss_D: 0.59818, Loss_G: 2.87260, Loss_KL: 0.33185\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[276/400] [0/349] Loss_D: 0.57505, Loss_G: 3.38713, Loss_KL: 0.35778\n",
      "[276/400] [100/349] Loss_D: 0.55623, Loss_G: 2.14966, Loss_KL: 0.33502\n",
      "[276/400] [200/349] Loss_D: 0.78904, Loss_G: 2.64613, Loss_KL: 0.31640\n",
      "[276/400] [300/349] Loss_D: 0.74570, Loss_G: 2.82871, Loss_KL: 0.33622\n",
      "[276/400] Loss_D: 0.61893, Loss_G: 2.84251, Loss_KL: 0.32537\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[277/400] [0/349] Loss_D: 0.71579, Loss_G: 2.69378, Loss_KL: 0.30718\n",
      "[277/400] [100/349] Loss_D: 0.49581, Loss_G: 2.82944, Loss_KL: 0.30844\n",
      "[277/400] [200/349] Loss_D: 0.51223, Loss_G: 2.85969, Loss_KL: 0.37460\n",
      "[277/400] [300/349] Loss_D: 0.18620, Loss_G: 2.67532, Loss_KL: 0.33999\n",
      "[277/400] Loss_D: 0.62007, Loss_G: 2.81567, Loss_KL: 0.32307\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[278/400] [0/349] Loss_D: 0.80223, Loss_G: 2.86068, Loss_KL: 0.36919\n",
      "[278/400] [100/349] Loss_D: 0.72952, Loss_G: 2.73227, Loss_KL: 0.44278\n",
      "[278/400] [200/349] Loss_D: 0.90970, Loss_G: 3.52775, Loss_KL: 0.31690\n",
      "[278/400] [300/349] Loss_D: 0.62206, Loss_G: 3.09343, Loss_KL: 0.34982\n",
      "[278/400] Loss_D: 0.63135, Loss_G: 2.83287, Loss_KL: 0.32938\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[279/400] [0/349] Loss_D: 0.74662, Loss_G: 3.29487, Loss_KL: 0.23587\n",
      "[279/400] [100/349] Loss_D: 0.58277, Loss_G: 2.86860, Loss_KL: 0.35927\n",
      "[279/400] [200/349] Loss_D: 0.75074, Loss_G: 2.10444, Loss_KL: 0.31110\n",
      "[279/400] [300/349] Loss_D: 0.72695, Loss_G: 2.81069, Loss_KL: 0.33827\n",
      "[279/400] Loss_D: 0.61953, Loss_G: 2.82926, Loss_KL: 0.32634\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[280/400] [0/349] Loss_D: 0.56868, Loss_G: 2.97700, Loss_KL: 0.33321\n",
      "[280/400] [100/349] Loss_D: 0.48963, Loss_G: 2.65483, Loss_KL: 0.30540\n",
      "[280/400] [200/349] Loss_D: 0.96013, Loss_G: 2.55855, Loss_KL: 0.24097\n",
      "[280/400] [300/349] Loss_D: 0.78939, Loss_G: 3.30251, Loss_KL: 0.36334\n",
      "[280/400] Loss_D: 0.60958, Loss_G: 2.83928, Loss_KL: 0.32913\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[281/400] [0/349] Loss_D: 0.81357, Loss_G: 2.71324, Loss_KL: 0.30604\n",
      "[281/400] [100/349] Loss_D: 0.83498, Loss_G: 2.22644, Loss_KL: 0.33526\n",
      "[281/400] [200/349] Loss_D: 0.59494, Loss_G: 2.42191, Loss_KL: 0.29900\n",
      "[281/400] [300/349] Loss_D: 0.65517, Loss_G: 3.24904, Loss_KL: 0.36108\n",
      "[281/400] Loss_D: 0.63087, Loss_G: 2.84076, Loss_KL: 0.32842\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[282/400] [0/349] Loss_D: 0.35643, Loss_G: 2.38669, Loss_KL: 0.27841\n",
      "[282/400] [100/349] Loss_D: 0.54276, Loss_G: 3.08370, Loss_KL: 0.29416\n",
      "[282/400] [200/349] Loss_D: 0.79487, Loss_G: 3.61334, Loss_KL: 0.26429\n",
      "[282/400] [300/349] Loss_D: 0.75315, Loss_G: 3.02215, Loss_KL: 0.36262\n",
      "[282/400] Loss_D: 0.61732, Loss_G: 2.84086, Loss_KL: 0.32427\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[283/400] [0/349] Loss_D: 0.69339, Loss_G: 2.90477, Loss_KL: 0.29310\n",
      "[283/400] [100/349] Loss_D: 0.36108, Loss_G: 3.65991, Loss_KL: 0.32588\n",
      "[283/400] [200/349] Loss_D: 0.62235, Loss_G: 2.42451, Loss_KL: 0.29174\n",
      "[283/400] [300/349] Loss_D: 0.80922, Loss_G: 2.59572, Loss_KL: 0.35169\n",
      "[283/400] Loss_D: 0.61444, Loss_G: 2.81276, Loss_KL: 0.31688\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[284/400] [0/349] Loss_D: 0.43859, Loss_G: 2.83974, Loss_KL: 0.32240\n",
      "[284/400] [100/349] Loss_D: 0.66528, Loss_G: 3.35822, Loss_KL: 0.30104\n",
      "[284/400] [200/349] Loss_D: 0.69684, Loss_G: 2.50100, Loss_KL: 0.33328\n",
      "[284/400] [300/349] Loss_D: 0.73033, Loss_G: 2.50797, Loss_KL: 0.28390\n",
      "[284/400] Loss_D: 0.60972, Loss_G: 2.85294, Loss_KL: 0.31378\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[285/400] [0/349] Loss_D: 0.32044, Loss_G: 2.89354, Loss_KL: 0.32046\n",
      "[285/400] [100/349] Loss_D: 0.41745, Loss_G: 2.45429, Loss_KL: 0.29639\n",
      "[285/400] [200/349] Loss_D: 0.62651, Loss_G: 3.29590, Loss_KL: 0.29437\n",
      "[285/400] [300/349] Loss_D: 0.50213, Loss_G: 2.98224, Loss_KL: 0.29526\n",
      "[285/400] Loss_D: 0.59659, Loss_G: 2.85058, Loss_KL: 0.31947\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[286/400] [0/349] Loss_D: 0.79386, Loss_G: 2.67595, Loss_KL: 0.31718\n",
      "[286/400] [100/349] Loss_D: 0.35896, Loss_G: 2.40500, Loss_KL: 0.33042\n",
      "[286/400] [200/349] Loss_D: 0.26004, Loss_G: 2.71156, Loss_KL: 0.29275\n",
      "[286/400] [300/349] Loss_D: 0.84658, Loss_G: 2.59318, Loss_KL: 0.27045\n",
      "[286/400] Loss_D: 0.59942, Loss_G: 2.86177, Loss_KL: 0.32557\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[287/400] [0/349] Loss_D: 0.88275, Loss_G: 3.26494, Loss_KL: 0.31902\n",
      "[287/400] [100/349] Loss_D: 0.70165, Loss_G: 2.96191, Loss_KL: 0.31048\n",
      "[287/400] [200/349] Loss_D: 0.81427, Loss_G: 3.21777, Loss_KL: 0.38094\n",
      "[287/400] [300/349] Loss_D: 0.69195, Loss_G: 2.61689, Loss_KL: 0.32439\n",
      "[287/400] Loss_D: 0.62369, Loss_G: 2.81414, Loss_KL: 0.32000\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[288/400] [0/349] Loss_D: 0.54700, Loss_G: 3.09333, Loss_KL: 0.48826\n",
      "[288/400] [100/349] Loss_D: 0.53513, Loss_G: 2.40355, Loss_KL: 0.31735\n",
      "[288/400] [200/349] Loss_D: 0.76765, Loss_G: 3.17062, Loss_KL: 0.33457\n",
      "[288/400] [300/349] Loss_D: 0.79063, Loss_G: 2.00493, Loss_KL: 0.38516\n",
      "[288/400] Loss_D: 0.60874, Loss_G: 2.82618, Loss_KL: 0.32290\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[289/400] [0/349] Loss_D: 0.70259, Loss_G: 3.06795, Loss_KL: 0.24819\n",
      "[289/400] [100/349] Loss_D: 0.74925, Loss_G: 3.69281, Loss_KL: 0.32215\n",
      "[289/400] [200/349] Loss_D: 0.55061, Loss_G: 2.61054, Loss_KL: 0.32208\n",
      "[289/400] [300/349] Loss_D: 0.67937, Loss_G: 2.86425, Loss_KL: 0.34540\n",
      "[289/400] Loss_D: 0.61885, Loss_G: 2.82078, Loss_KL: 0.32428\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[290/400] [0/349] Loss_D: 0.58408, Loss_G: 2.53621, Loss_KL: 0.40597\n",
      "[290/400] [100/349] Loss_D: 0.37509, Loss_G: 2.79913, Loss_KL: 0.23941\n",
      "[290/400] [200/349] Loss_D: 0.45346, Loss_G: 2.92751, Loss_KL: 0.32813\n",
      "[290/400] [300/349] Loss_D: 0.59702, Loss_G: 2.88290, Loss_KL: 0.34874\n",
      "[290/400] Loss_D: 0.61251, Loss_G: 2.84936, Loss_KL: 0.32564\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[291/400] [0/349] Loss_D: 0.52785, Loss_G: 3.07040, Loss_KL: 0.48162\n",
      "[291/400] [100/349] Loss_D: 0.63558, Loss_G: 2.74142, Loss_KL: 0.32671\n",
      "[291/400] [200/349] Loss_D: 0.51710, Loss_G: 2.34232, Loss_KL: 0.33255\n",
      "[291/400] [300/349] Loss_D: 0.77724, Loss_G: 3.06593, Loss_KL: 0.32602\n",
      "[291/400] Loss_D: 0.63103, Loss_G: 2.83026, Loss_KL: 0.32711\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[292/400] [0/349] Loss_D: 0.62836, Loss_G: 3.40429, Loss_KL: 0.32257\n",
      "[292/400] [100/349] Loss_D: 0.23545, Loss_G: 3.09711, Loss_KL: 0.29645\n",
      "[292/400] [200/349] Loss_D: 0.85102, Loss_G: 3.52321, Loss_KL: 0.30709\n",
      "[292/400] [300/349] Loss_D: 0.39991, Loss_G: 2.43497, Loss_KL: 0.34361\n",
      "[292/400] Loss_D: 0.62464, Loss_G: 2.83392, Loss_KL: 0.33301\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[293/400] [0/349] Loss_D: 0.59417, Loss_G: 2.91278, Loss_KL: 0.34240\n",
      "[293/400] [100/349] Loss_D: 0.43922, Loss_G: 3.13236, Loss_KL: 0.32816\n",
      "[293/400] [200/349] Loss_D: 0.55339, Loss_G: 2.40035, Loss_KL: 0.42079\n",
      "[293/400] [300/349] Loss_D: 0.70974, Loss_G: 3.14117, Loss_KL: 0.37690\n",
      "[293/400] Loss_D: 0.60073, Loss_G: 2.85812, Loss_KL: 0.33365\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[294/400] [0/349] Loss_D: 0.75843, Loss_G: 2.34369, Loss_KL: 0.39444\n",
      "[294/400] [100/349] Loss_D: 0.61397, Loss_G: 3.24585, Loss_KL: 0.29061\n",
      "[294/400] [200/349] Loss_D: 0.46447, Loss_G: 2.95754, Loss_KL: 0.31920\n",
      "[294/400] [300/349] Loss_D: 0.61717, Loss_G: 3.24072, Loss_KL: 0.33867\n",
      "[294/400] Loss_D: 0.61924, Loss_G: 2.87816, Loss_KL: 0.33448\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[295/400] [0/349] Loss_D: 0.66659, Loss_G: 2.64629, Loss_KL: 0.26723\n",
      "[295/400] [100/349] Loss_D: 0.69212, Loss_G: 2.43287, Loss_KL: 0.35318\n",
      "[295/400] [200/349] Loss_D: 0.37965, Loss_G: 3.15094, Loss_KL: 0.28989\n",
      "[295/400] [300/349] Loss_D: 0.74072, Loss_G: 3.14250, Loss_KL: 0.31575\n",
      "[295/400] Loss_D: 0.61736, Loss_G: 2.84159, Loss_KL: 0.33590\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[296/400] [0/349] Loss_D: 0.66211, Loss_G: 3.23273, Loss_KL: 0.41235\n",
      "[296/400] [100/349] Loss_D: 0.65754, Loss_G: 2.74526, Loss_KL: 0.31433\n",
      "[296/400] [200/349] Loss_D: 0.66352, Loss_G: 2.29181, Loss_KL: 0.27926\n",
      "[296/400] [300/349] Loss_D: 0.36686, Loss_G: 4.27534, Loss_KL: 0.36734\n",
      "[296/400] Loss_D: 0.60908, Loss_G: 2.88173, Loss_KL: 0.33677\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[297/400] [0/349] Loss_D: 0.71802, Loss_G: 3.23909, Loss_KL: 0.39762\n",
      "[297/400] [100/349] Loss_D: 0.74451, Loss_G: 2.74446, Loss_KL: 0.32721\n",
      "[297/400] [200/349] Loss_D: 0.64382, Loss_G: 2.42432, Loss_KL: 0.35056\n",
      "[297/400] [300/349] Loss_D: 0.72083, Loss_G: 2.91318, Loss_KL: 0.40304\n",
      "[297/400] Loss_D: 0.60588, Loss_G: 2.87836, Loss_KL: 0.33580\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[298/400] [0/349] Loss_D: 0.83219, Loss_G: 2.26014, Loss_KL: 0.41876\n",
      "[298/400] [100/349] Loss_D: 0.53566, Loss_G: 2.75557, Loss_KL: 0.32868\n",
      "[298/400] [200/349] Loss_D: 0.62455, Loss_G: 2.45036, Loss_KL: 0.35247\n",
      "[298/400] [300/349] Loss_D: 0.30949, Loss_G: 3.18985, Loss_KL: 0.26752\n",
      "[298/400] Loss_D: 0.60313, Loss_G: 2.87677, Loss_KL: 0.32998\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[299/400] [0/349] Loss_D: 0.36840, Loss_G: 2.25920, Loss_KL: 0.35789\n",
      "[299/400] [100/349] Loss_D: 0.80629, Loss_G: 3.35116, Loss_KL: 0.46722\n",
      "[299/400] [200/349] Loss_D: 0.62892, Loss_G: 2.85676, Loss_KL: 0.33903\n",
      "[299/400] [300/349] Loss_D: 0.43680, Loss_G: 2.48532, Loss_KL: 0.34721\n",
      "[299/400] Loss_D: 0.61767, Loss_G: 2.83503, Loss_KL: 0.32739\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[300/400] [0/349] Loss_D: 0.70537, Loss_G: 3.00383, Loss_KL: 0.38922\n",
      "[300/400] [100/349] Loss_D: 0.47763, Loss_G: 2.91311, Loss_KL: 0.25791\n",
      "[300/400] [200/349] Loss_D: 0.76384, Loss_G: 2.58166, Loss_KL: 0.38830\n",
      "[300/400] [300/349] Loss_D: 0.64570, Loss_G: 2.40944, Loss_KL: 0.31178\n",
      "[300/400] Loss_D: 0.61592, Loss_G: 2.84814, Loss_KL: 0.32606\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[301/400] [0/349] Loss_D: 0.76236, Loss_G: 2.94106, Loss_KL: 0.36867\n",
      "[301/400] [100/349] Loss_D: 0.80897, Loss_G: 2.74868, Loss_KL: 0.33404\n",
      "[301/400] [200/349] Loss_D: 0.64069, Loss_G: 3.25482, Loss_KL: 0.32545\n",
      "[301/400] [300/349] Loss_D: 0.61989, Loss_G: 3.45077, Loss_KL: 0.35909\n",
      "[301/400] Loss_D: 0.60990, Loss_G: 2.81250, Loss_KL: 0.33375\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[302/400] [0/349] Loss_D: 0.39492, Loss_G: 2.93722, Loss_KL: 0.37147\n",
      "[302/400] [100/349] Loss_D: 0.51687, Loss_G: 2.43665, Loss_KL: 0.30171\n",
      "[302/400] [200/349] Loss_D: 0.52420, Loss_G: 2.42997, Loss_KL: 0.34249\n",
      "[302/400] [300/349] Loss_D: 1.03102, Loss_G: 2.48340, Loss_KL: 0.39048\n",
      "[302/400] Loss_D: 0.60690, Loss_G: 2.77159, Loss_KL: 0.33073\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[303/400] [0/349] Loss_D: 0.37654, Loss_G: 2.66075, Loss_KL: 0.38817\n",
      "[303/400] [100/349] Loss_D: 0.38364, Loss_G: 2.44689, Loss_KL: 0.30224\n",
      "[303/400] [200/349] Loss_D: 0.32817, Loss_G: 3.35849, Loss_KL: 0.34277\n",
      "[303/400] [300/349] Loss_D: 0.25509, Loss_G: 4.23851, Loss_KL: 0.35384\n",
      "[303/400] Loss_D: 0.59968, Loss_G: 2.78748, Loss_KL: 0.32824\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[304/400] [0/349] Loss_D: 0.83578, Loss_G: 2.00836, Loss_KL: 0.35578\n",
      "[304/400] [100/349] Loss_D: 0.83361, Loss_G: 2.59052, Loss_KL: 0.34867\n",
      "[304/400] [200/349] Loss_D: 0.77238, Loss_G: 2.51322, Loss_KL: 0.42896\n",
      "[304/400] [300/349] Loss_D: 0.80277, Loss_G: 3.03533, Loss_KL: 0.26706\n",
      "[304/400] Loss_D: 0.58903, Loss_G: 2.77954, Loss_KL: 0.32598\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[305/400] [0/349] Loss_D: 0.78586, Loss_G: 2.91826, Loss_KL: 0.34393\n",
      "[305/400] [100/349] Loss_D: 0.88232, Loss_G: 3.10354, Loss_KL: 0.25529\n",
      "[305/400] [200/349] Loss_D: 0.60603, Loss_G: 2.83794, Loss_KL: 0.34736\n",
      "[305/400] [300/349] Loss_D: 0.53573, Loss_G: 2.63017, Loss_KL: 0.32144\n",
      "[305/400] Loss_D: 0.59827, Loss_G: 2.77594, Loss_KL: 0.32451\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[306/400] [0/349] Loss_D: 0.72036, Loss_G: 2.36173, Loss_KL: 0.34836\n",
      "[306/400] [100/349] Loss_D: 0.73486, Loss_G: 2.72178, Loss_KL: 0.35744\n",
      "[306/400] [200/349] Loss_D: 0.67032, Loss_G: 2.97024, Loss_KL: 0.32574\n",
      "[306/400] [300/349] Loss_D: 0.71050, Loss_G: 3.55367, Loss_KL: 0.34715\n",
      "[306/400] Loss_D: 0.60793, Loss_G: 2.75177, Loss_KL: 0.32406\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[307/400] [0/349] Loss_D: 0.40984, Loss_G: 3.38991, Loss_KL: 0.32890\n",
      "[307/400] [100/349] Loss_D: 0.73480, Loss_G: 2.66804, Loss_KL: 0.33123\n",
      "[307/400] [200/349] Loss_D: 0.37522, Loss_G: 4.18286, Loss_KL: 0.30506\n",
      "[307/400] [300/349] Loss_D: 0.68693, Loss_G: 3.35305, Loss_KL: 0.32403\n",
      "[307/400] Loss_D: 0.60561, Loss_G: 2.77437, Loss_KL: 0.32382\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[308/400] [0/349] Loss_D: 0.77641, Loss_G: 3.16868, Loss_KL: 0.40075\n",
      "[308/400] [100/349] Loss_D: 0.39263, Loss_G: 2.17286, Loss_KL: 0.33530\n",
      "[308/400] [200/349] Loss_D: 0.45654, Loss_G: 2.44272, Loss_KL: 0.28552\n",
      "[308/400] [300/349] Loss_D: 0.45191, Loss_G: 3.52952, Loss_KL: 0.29327\n",
      "[308/400] Loss_D: 0.60338, Loss_G: 2.79983, Loss_KL: 0.32980\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[309/400] [0/349] Loss_D: 0.75452, Loss_G: 2.24609, Loss_KL: 0.29005\n",
      "[309/400] [100/349] Loss_D: 0.52230, Loss_G: 3.33780, Loss_KL: 0.34622\n",
      "[309/400] [200/349] Loss_D: 0.66047, Loss_G: 2.94152, Loss_KL: 0.30068\n",
      "[309/400] [300/349] Loss_D: 0.63111, Loss_G: 2.52873, Loss_KL: 0.38111\n",
      "[309/400] Loss_D: 0.60598, Loss_G: 2.77102, Loss_KL: 0.33050\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[310/400] [0/349] Loss_D: 0.64744, Loss_G: 2.10870, Loss_KL: 0.38095\n",
      "[310/400] [100/349] Loss_D: 0.81033, Loss_G: 3.37563, Loss_KL: 0.41374\n",
      "[310/400] [200/349] Loss_D: 0.54089, Loss_G: 2.25317, Loss_KL: 0.32092\n",
      "[310/400] [300/349] Loss_D: 0.82406, Loss_G: 2.32160, Loss_KL: 0.36530\n",
      "[310/400] Loss_D: 0.60255, Loss_G: 2.75537, Loss_KL: 0.32952\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[311/400] [0/349] Loss_D: 0.81209, Loss_G: 3.02147, Loss_KL: 0.34034\n",
      "[311/400] [100/349] Loss_D: 0.72524, Loss_G: 2.76008, Loss_KL: 0.28121\n",
      "[311/400] [200/349] Loss_D: 0.62550, Loss_G: 2.49498, Loss_KL: 0.32165\n",
      "[311/400] [300/349] Loss_D: 0.73883, Loss_G: 2.28896, Loss_KL: 0.28419\n",
      "[311/400] Loss_D: 0.60026, Loss_G: 2.81771, Loss_KL: 0.33045\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[312/400] [0/349] Loss_D: 0.45366, Loss_G: 2.80392, Loss_KL: 0.35514\n",
      "[312/400] [100/349] Loss_D: 0.82497, Loss_G: 2.97987, Loss_KL: 0.39234\n",
      "[312/400] [200/349] Loss_D: 0.70950, Loss_G: 2.27554, Loss_KL: 0.30264\n",
      "[312/400] [300/349] Loss_D: 0.78837, Loss_G: 2.87370, Loss_KL: 0.30805\n",
      "[312/400] Loss_D: 0.59492, Loss_G: 2.78077, Loss_KL: 0.33022\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[313/400] [0/349] Loss_D: 0.31741, Loss_G: 2.91242, Loss_KL: 0.29863\n",
      "[313/400] [100/349] Loss_D: 0.75276, Loss_G: 2.54594, Loss_KL: 0.33176\n",
      "[313/400] [200/349] Loss_D: 0.76672, Loss_G: 2.44174, Loss_KL: 0.36753\n",
      "[313/400] [300/349] Loss_D: 0.65648, Loss_G: 2.87551, Loss_KL: 0.39011\n",
      "[313/400] Loss_D: 0.60343, Loss_G: 2.83056, Loss_KL: 0.33282\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[314/400] [0/349] Loss_D: 0.24903, Loss_G: 3.22413, Loss_KL: 0.30596\n",
      "[314/400] [100/349] Loss_D: 0.85186, Loss_G: 2.91689, Loss_KL: 0.32160\n",
      "[314/400] [200/349] Loss_D: 0.79102, Loss_G: 2.60551, Loss_KL: 0.35433\n",
      "[314/400] [300/349] Loss_D: 0.60762, Loss_G: 2.95817, Loss_KL: 0.35535\n",
      "[314/400] Loss_D: 0.60067, Loss_G: 2.80811, Loss_KL: 0.33513\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[315/400] [0/349] Loss_D: 0.92004, Loss_G: 2.54889, Loss_KL: 0.36397\n",
      "[315/400] [100/349] Loss_D: 0.25981, Loss_G: 2.96312, Loss_KL: 0.31403\n",
      "[315/400] [200/349] Loss_D: 0.65939, Loss_G: 2.59470, Loss_KL: 0.26144\n",
      "[315/400] [300/349] Loss_D: 0.41652, Loss_G: 2.84220, Loss_KL: 0.32372\n",
      "[315/400] Loss_D: 0.59130, Loss_G: 2.77763, Loss_KL: 0.33771\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[316/400] [0/349] Loss_D: 0.58239, Loss_G: 2.21510, Loss_KL: 0.31259\n",
      "[316/400] [100/349] Loss_D: 0.35527, Loss_G: 2.96845, Loss_KL: 0.33200\n",
      "[316/400] [200/349] Loss_D: 0.73741, Loss_G: 3.45911, Loss_KL: 0.35145\n",
      "[316/400] [300/349] Loss_D: 0.84405, Loss_G: 2.46749, Loss_KL: 0.28196\n",
      "[316/400] Loss_D: 0.60274, Loss_G: 2.78699, Loss_KL: 0.34089\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[317/400] [0/349] Loss_D: 0.45218, Loss_G: 2.46697, Loss_KL: 0.31775\n",
      "[317/400] [100/349] Loss_D: 0.41877, Loss_G: 2.76593, Loss_KL: 0.42026\n",
      "[317/400] [200/349] Loss_D: 0.71591, Loss_G: 2.50169, Loss_KL: 0.38666\n",
      "[317/400] [300/349] Loss_D: 0.33786, Loss_G: 2.38216, Loss_KL: 0.36301\n",
      "[317/400] Loss_D: 0.60656, Loss_G: 2.75485, Loss_KL: 0.33614\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[318/400] [0/349] Loss_D: 0.58569, Loss_G: 3.04740, Loss_KL: 0.25624\n",
      "[318/400] [100/349] Loss_D: 0.52527, Loss_G: 3.99582, Loss_KL: 0.36765\n",
      "[318/400] [200/349] Loss_D: 0.29872, Loss_G: 3.28355, Loss_KL: 0.28691\n",
      "[318/400] [300/349] Loss_D: 0.68261, Loss_G: 2.48299, Loss_KL: 0.38045\n",
      "[318/400] Loss_D: 0.60525, Loss_G: 2.79433, Loss_KL: 0.33724\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[319/400] [0/349] Loss_D: 0.33357, Loss_G: 2.39362, Loss_KL: 0.32253\n",
      "[319/400] [100/349] Loss_D: 0.73820, Loss_G: 3.21382, Loss_KL: 0.36443\n",
      "[319/400] [200/349] Loss_D: 0.49600, Loss_G: 2.28271, Loss_KL: 0.43818\n",
      "[319/400] [300/349] Loss_D: 0.23253, Loss_G: 3.17357, Loss_KL: 0.33876\n",
      "[319/400] Loss_D: 0.61374, Loss_G: 2.75262, Loss_KL: 0.34045\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[320/400] [0/349] Loss_D: 0.67270, Loss_G: 2.60841, Loss_KL: 0.29326\n",
      "[320/400] [100/349] Loss_D: 0.45858, Loss_G: 3.29378, Loss_KL: 0.41629\n",
      "[320/400] [200/349] Loss_D: 0.71979, Loss_G: 2.29219, Loss_KL: 0.31576\n",
      "[320/400] [300/349] Loss_D: 0.69623, Loss_G: 2.55939, Loss_KL: 0.37905\n",
      "[320/400] Loss_D: 0.60933, Loss_G: 2.77473, Loss_KL: 0.34446\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[321/400] [0/349] Loss_D: 0.70440, Loss_G: 2.43055, Loss_KL: 0.39084\n",
      "[321/400] [100/349] Loss_D: 0.41613, Loss_G: 2.81947, Loss_KL: 0.33821\n",
      "[321/400] [200/349] Loss_D: 0.60332, Loss_G: 2.87017, Loss_KL: 0.39104\n",
      "[321/400] [300/349] Loss_D: 0.44068, Loss_G: 2.78558, Loss_KL: 0.30869\n",
      "[321/400] Loss_D: 0.60793, Loss_G: 2.79179, Loss_KL: 0.35190\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[322/400] [0/349] Loss_D: 0.61673, Loss_G: 2.45711, Loss_KL: 0.33994\n",
      "[322/400] [100/349] Loss_D: 0.38564, Loss_G: 3.26762, Loss_KL: 0.31745\n",
      "[322/400] [200/349] Loss_D: 0.70925, Loss_G: 2.73882, Loss_KL: 0.32553\n",
      "[322/400] [300/349] Loss_D: 0.59937, Loss_G: 2.38235, Loss_KL: 0.30287\n",
      "[322/400] Loss_D: 0.60315, Loss_G: 2.83030, Loss_KL: 0.35605\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[323/400] [0/349] Loss_D: 0.62810, Loss_G: 2.86813, Loss_KL: 0.38430\n",
      "[323/400] [100/349] Loss_D: 0.71563, Loss_G: 3.34891, Loss_KL: 0.38106\n",
      "[323/400] [200/349] Loss_D: 0.65825, Loss_G: 3.48916, Loss_KL: 0.36866\n",
      "[323/400] [300/349] Loss_D: 0.44150, Loss_G: 2.38711, Loss_KL: 0.26901\n",
      "[323/400] Loss_D: 0.61152, Loss_G: 2.78145, Loss_KL: 0.35103\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[324/400] [0/349] Loss_D: 0.63361, Loss_G: 2.32274, Loss_KL: 0.32010\n",
      "[324/400] [100/349] Loss_D: 0.50878, Loss_G: 3.02387, Loss_KL: 0.32366\n",
      "[324/400] [200/349] Loss_D: 0.36596, Loss_G: 2.32866, Loss_KL: 0.34178\n",
      "[324/400] [300/349] Loss_D: 0.73957, Loss_G: 2.21757, Loss_KL: 0.39873\n",
      "[324/400] Loss_D: 0.59605, Loss_G: 2.83701, Loss_KL: 0.34349\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[325/400] [0/349] Loss_D: 0.74108, Loss_G: 3.18780, Loss_KL: 0.34992\n",
      "[325/400] [100/349] Loss_D: 0.25082, Loss_G: 3.21317, Loss_KL: 0.33855\n",
      "[325/400] [200/349] Loss_D: 0.67395, Loss_G: 2.67157, Loss_KL: 0.34785\n",
      "[325/400] [300/349] Loss_D: 0.63459, Loss_G: 2.86124, Loss_KL: 0.29342\n",
      "[325/400] Loss_D: 0.59918, Loss_G: 2.81160, Loss_KL: 0.34260\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[326/400] [0/349] Loss_D: 0.47982, Loss_G: 3.04566, Loss_KL: 0.34664\n",
      "[326/400] [100/349] Loss_D: 0.57142, Loss_G: 2.47429, Loss_KL: 0.32532\n",
      "[326/400] [200/349] Loss_D: 0.69162, Loss_G: 2.14215, Loss_KL: 0.33964\n",
      "[326/400] [300/349] Loss_D: 0.36768, Loss_G: 2.99736, Loss_KL: 0.33986\n",
      "[326/400] Loss_D: 0.59125, Loss_G: 2.83088, Loss_KL: 0.34048\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[327/400] [0/349] Loss_D: 0.84008, Loss_G: 2.83700, Loss_KL: 0.38269\n",
      "[327/400] [100/349] Loss_D: 0.42497, Loss_G: 2.95327, Loss_KL: 0.26439\n",
      "[327/400] [200/349] Loss_D: 0.44753, Loss_G: 3.12867, Loss_KL: 0.33233\n",
      "[327/400] [300/349] Loss_D: 0.87577, Loss_G: 2.81700, Loss_KL: 0.38217\n",
      "[327/400] Loss_D: 0.59923, Loss_G: 2.81624, Loss_KL: 0.33695\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[328/400] [0/349] Loss_D: 0.65837, Loss_G: 2.49781, Loss_KL: 0.34534\n",
      "[328/400] [100/349] Loss_D: 0.72630, Loss_G: 2.66830, Loss_KL: 0.28319\n",
      "[328/400] [200/349] Loss_D: 0.65991, Loss_G: 2.66320, Loss_KL: 0.35735\n",
      "[328/400] [300/349] Loss_D: 0.55286, Loss_G: 3.03477, Loss_KL: 0.34239\n",
      "[328/400] Loss_D: 0.59774, Loss_G: 2.79134, Loss_KL: 0.33446\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[329/400] [0/349] Loss_D: 0.49610, Loss_G: 3.71072, Loss_KL: 0.35780\n",
      "[329/400] [100/349] Loss_D: 0.67498, Loss_G: 2.57479, Loss_KL: 0.32280\n",
      "[329/400] [200/349] Loss_D: 0.67661, Loss_G: 2.87033, Loss_KL: 0.31111\n",
      "[329/400] [300/349] Loss_D: 0.84327, Loss_G: 2.62078, Loss_KL: 0.31399\n",
      "[329/400] Loss_D: 0.60794, Loss_G: 2.78135, Loss_KL: 0.33892\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[330/400] [0/349] Loss_D: 0.45415, Loss_G: 2.97090, Loss_KL: 0.36438\n",
      "[330/400] [100/349] Loss_D: 0.30537, Loss_G: 3.15771, Loss_KL: 0.30448\n",
      "[330/400] [200/349] Loss_D: 1.00225, Loss_G: 2.16511, Loss_KL: 0.28833\n",
      "[330/400] [300/349] Loss_D: 0.66581, Loss_G: 3.08020, Loss_KL: 0.35971\n",
      "[330/400] Loss_D: 0.60099, Loss_G: 2.86556, Loss_KL: 0.33729\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[331/400] [0/349] Loss_D: 0.73662, Loss_G: 2.42594, Loss_KL: 0.32034\n",
      "[331/400] [100/349] Loss_D: 0.75071, Loss_G: 2.48171, Loss_KL: 0.30437\n",
      "[331/400] [200/349] Loss_D: 0.45594, Loss_G: 3.10576, Loss_KL: 0.29239\n",
      "[331/400] [300/349] Loss_D: 0.34556, Loss_G: 2.94078, Loss_KL: 0.35378\n",
      "[331/400] Loss_D: 0.59069, Loss_G: 2.87577, Loss_KL: 0.33372\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[332/400] [0/349] Loss_D: 0.71777, Loss_G: 2.71751, Loss_KL: 0.32369\n",
      "[332/400] [100/349] Loss_D: 0.46632, Loss_G: 3.18094, Loss_KL: 0.39896\n",
      "[332/400] [200/349] Loss_D: 0.75689, Loss_G: 2.81481, Loss_KL: 0.47159\n",
      "[332/400] [300/349] Loss_D: 0.69859, Loss_G: 2.74795, Loss_KL: 0.33052\n",
      "[332/400] Loss_D: 0.60259, Loss_G: 2.79130, Loss_KL: 0.33879\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[333/400] [0/349] Loss_D: 0.83469, Loss_G: 3.19080, Loss_KL: 0.29072\n",
      "[333/400] [100/349] Loss_D: 0.78145, Loss_G: 2.92503, Loss_KL: 0.32531\n",
      "[333/400] [200/349] Loss_D: 0.44473, Loss_G: 2.62124, Loss_KL: 0.31097\n",
      "[333/400] [300/349] Loss_D: 0.42623, Loss_G: 3.04967, Loss_KL: 0.45511\n",
      "[333/400] Loss_D: 0.59791, Loss_G: 2.83226, Loss_KL: 0.33768\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[334/400] [0/349] Loss_D: 0.66533, Loss_G: 3.30101, Loss_KL: 0.32433\n",
      "[334/400] [100/349] Loss_D: 0.42276, Loss_G: 2.57931, Loss_KL: 0.27176\n",
      "[334/400] [200/349] Loss_D: 0.57119, Loss_G: 2.74936, Loss_KL: 0.31481\n",
      "[334/400] [300/349] Loss_D: 0.46361, Loss_G: 3.01735, Loss_KL: 0.27747\n",
      "[334/400] Loss_D: 0.60312, Loss_G: 2.82565, Loss_KL: 0.33532\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[335/400] [0/349] Loss_D: 0.46589, Loss_G: 1.88382, Loss_KL: 0.33557\n",
      "[335/400] [100/349] Loss_D: 0.72248, Loss_G: 2.81472, Loss_KL: 0.31093\n",
      "[335/400] [200/349] Loss_D: 0.49789, Loss_G: 3.14818, Loss_KL: 0.35130\n",
      "[335/400] [300/349] Loss_D: 0.81752, Loss_G: 3.21370, Loss_KL: 0.38487\n",
      "[335/400] Loss_D: 0.59304, Loss_G: 2.82077, Loss_KL: 0.33453\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[336/400] [0/349] Loss_D: 0.63242, Loss_G: 2.82889, Loss_KL: 0.30123\n",
      "[336/400] [100/349] Loss_D: 0.64656, Loss_G: 2.45524, Loss_KL: 0.34857\n",
      "[336/400] [200/349] Loss_D: 0.68180, Loss_G: 2.54373, Loss_KL: 0.34586\n",
      "[336/400] [300/349] Loss_D: 0.71595, Loss_G: 2.49677, Loss_KL: 0.33230\n",
      "[336/400] Loss_D: 0.61841, Loss_G: 2.77273, Loss_KL: 0.33325\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[337/400] [0/349] Loss_D: 0.43516, Loss_G: 2.87675, Loss_KL: 0.29281\n",
      "[337/400] [100/349] Loss_D: 0.64732, Loss_G: 3.13979, Loss_KL: 0.33999\n",
      "[337/400] [200/349] Loss_D: 0.28797, Loss_G: 3.01521, Loss_KL: 0.28416\n",
      "[337/400] [300/349] Loss_D: 0.80183, Loss_G: 2.17211, Loss_KL: 0.31374\n",
      "[337/400] Loss_D: 0.60562, Loss_G: 2.74977, Loss_KL: 0.32580\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[338/400] [0/349] Loss_D: 0.71474, Loss_G: 2.79430, Loss_KL: 0.35124\n",
      "[338/400] [100/349] Loss_D: 0.86371, Loss_G: 2.99871, Loss_KL: 0.30742\n",
      "[338/400] [200/349] Loss_D: 0.72391, Loss_G: 2.99129, Loss_KL: 0.31695\n",
      "[338/400] [300/349] Loss_D: 0.37010, Loss_G: 2.89890, Loss_KL: 0.33998\n",
      "[338/400] Loss_D: 0.60199, Loss_G: 2.83254, Loss_KL: 0.33507\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[339/400] [0/349] Loss_D: 0.78417, Loss_G: 2.77857, Loss_KL: 0.36200\n",
      "[339/400] [100/349] Loss_D: 0.81423, Loss_G: 2.85237, Loss_KL: 0.35420\n",
      "[339/400] [200/349] Loss_D: 0.41763, Loss_G: 2.00096, Loss_KL: 0.35341\n",
      "[339/400] [300/349] Loss_D: 0.50141, Loss_G: 3.13715, Loss_KL: 0.36958\n",
      "[339/400] Loss_D: 0.58969, Loss_G: 2.77995, Loss_KL: 0.33291\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[340/400] [0/349] Loss_D: 0.37661, Loss_G: 2.29719, Loss_KL: 0.32952\n",
      "[340/400] [100/349] Loss_D: 0.75252, Loss_G: 3.00882, Loss_KL: 0.29429\n",
      "[340/400] [200/349] Loss_D: 0.62407, Loss_G: 2.62703, Loss_KL: 0.37841\n",
      "[340/400] [300/349] Loss_D: 0.74601, Loss_G: 2.71299, Loss_KL: 0.34431\n",
      "[340/400] Loss_D: 0.60957, Loss_G: 2.82868, Loss_KL: 0.32956\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[341/400] [0/349] Loss_D: 0.63366, Loss_G: 3.28381, Loss_KL: 0.37514\n",
      "[341/400] [100/349] Loss_D: 0.45596, Loss_G: 3.39056, Loss_KL: 0.36635\n",
      "[341/400] [200/349] Loss_D: 0.56787, Loss_G: 2.89358, Loss_KL: 0.32571\n",
      "[341/400] [300/349] Loss_D: 0.48039, Loss_G: 2.36245, Loss_KL: 0.30797\n",
      "[341/400] Loss_D: 0.59759, Loss_G: 2.80171, Loss_KL: 0.33135\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[342/400] [0/349] Loss_D: 0.50492, Loss_G: 3.13306, Loss_KL: 0.35198\n",
      "[342/400] [100/349] Loss_D: 0.33836, Loss_G: 3.16069, Loss_KL: 0.33235\n",
      "[342/400] [200/349] Loss_D: 0.33493, Loss_G: 3.26053, Loss_KL: 0.29579\n",
      "[342/400] [300/349] Loss_D: 0.80448, Loss_G: 2.21594, Loss_KL: 0.31344\n",
      "[342/400] Loss_D: 0.58632, Loss_G: 2.80206, Loss_KL: 0.33411\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[343/400] [0/349] Loss_D: 0.48249, Loss_G: 2.65160, Loss_KL: 0.29871\n",
      "[343/400] [100/349] Loss_D: 0.38645, Loss_G: 2.62842, Loss_KL: 0.31548\n",
      "[343/400] [200/349] Loss_D: 0.68960, Loss_G: 3.82245, Loss_KL: 0.24997\n",
      "[343/400] [300/349] Loss_D: 0.63635, Loss_G: 2.53077, Loss_KL: 0.39104\n",
      "[343/400] Loss_D: 0.61013, Loss_G: 2.79478, Loss_KL: 0.33445\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[344/400] [0/349] Loss_D: 0.64589, Loss_G: 2.64499, Loss_KL: 0.31352\n",
      "[344/400] [100/349] Loss_D: 0.76715, Loss_G: 2.96300, Loss_KL: 0.36154\n",
      "[344/400] [200/349] Loss_D: 0.27725, Loss_G: 2.76821, Loss_KL: 0.35687\n",
      "[344/400] [300/349] Loss_D: 0.80770, Loss_G: 2.09575, Loss_KL: 0.27092\n",
      "[344/400] Loss_D: 0.59786, Loss_G: 2.81638, Loss_KL: 0.33275\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[345/400] [0/349] Loss_D: 0.71099, Loss_G: 2.69509, Loss_KL: 0.30201\n",
      "[345/400] [100/349] Loss_D: 0.30090, Loss_G: 2.63110, Loss_KL: 0.30961\n",
      "[345/400] [200/349] Loss_D: 0.62406, Loss_G: 3.02240, Loss_KL: 0.35775\n",
      "[345/400] [300/349] Loss_D: 0.72854, Loss_G: 2.37859, Loss_KL: 0.38094\n",
      "[345/400] Loss_D: 0.58918, Loss_G: 2.84082, Loss_KL: 0.32978\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[346/400] [0/349] Loss_D: 0.88437, Loss_G: 3.42786, Loss_KL: 0.22791\n",
      "[346/400] [100/349] Loss_D: 0.74949, Loss_G: 2.33738, Loss_KL: 0.32194\n",
      "[346/400] [200/349] Loss_D: 0.78381, Loss_G: 3.23407, Loss_KL: 0.30117\n",
      "[346/400] [300/349] Loss_D: 0.68435, Loss_G: 2.69378, Loss_KL: 0.30746\n",
      "[346/400] Loss_D: 0.59043, Loss_G: 2.83467, Loss_KL: 0.33073\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[347/400] [0/349] Loss_D: 0.85963, Loss_G: 2.59938, Loss_KL: 0.37004\n",
      "[347/400] [100/349] Loss_D: 0.71524, Loss_G: 2.91448, Loss_KL: 0.33116\n",
      "[347/400] [200/349] Loss_D: 0.63233, Loss_G: 2.28871, Loss_KL: 0.27396\n",
      "[347/400] [300/349] Loss_D: 0.73642, Loss_G: 2.66893, Loss_KL: 0.38057\n",
      "[347/400] Loss_D: 0.59860, Loss_G: 2.81724, Loss_KL: 0.33009\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[348/400] [0/349] Loss_D: 0.63040, Loss_G: 3.46117, Loss_KL: 0.29184\n",
      "[348/400] [100/349] Loss_D: 0.67802, Loss_G: 3.22531, Loss_KL: 0.32153\n",
      "[348/400] [200/349] Loss_D: 0.84824, Loss_G: 2.31147, Loss_KL: 0.24920\n",
      "[348/400] [300/349] Loss_D: 0.48604, Loss_G: 2.61014, Loss_KL: 0.40978\n",
      "[348/400] Loss_D: 0.60404, Loss_G: 2.81263, Loss_KL: 0.33180\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[349/400] [0/349] Loss_D: 0.41720, Loss_G: 2.92988, Loss_KL: 0.27562\n",
      "[349/400] [100/349] Loss_D: 0.74355, Loss_G: 3.03074, Loss_KL: 0.32504\n",
      "[349/400] [200/349] Loss_D: 0.76559, Loss_G: 2.63704, Loss_KL: 0.41203\n",
      "[349/400] [300/349] Loss_D: 0.46170, Loss_G: 2.69983, Loss_KL: 0.32792\n",
      "[349/400] Loss_D: 0.61692, Loss_G: 2.79912, Loss_KL: 0.32848\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[350/400] [0/349] Loss_D: 0.72088, Loss_G: 2.67620, Loss_KL: 0.31102\n",
      "[350/400] [100/349] Loss_D: 0.53658, Loss_G: 3.51674, Loss_KL: 0.27152\n",
      "[350/400] [200/349] Loss_D: 0.56746, Loss_G: 2.28594, Loss_KL: 0.35552\n",
      "[350/400] [300/349] Loss_D: 0.61167, Loss_G: 2.77909, Loss_KL: 0.27200\n",
      "[350/400] Loss_D: 0.59316, Loss_G: 2.81710, Loss_KL: 0.33469\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[351/400] [0/349] Loss_D: 0.63191, Loss_G: 3.09252, Loss_KL: 0.41076\n",
      "[351/400] [100/349] Loss_D: 0.67794, Loss_G: 2.32994, Loss_KL: 0.33301\n",
      "[351/400] [200/349] Loss_D: 0.65695, Loss_G: 3.23246, Loss_KL: 0.38466\n",
      "[351/400] [300/349] Loss_D: 0.64652, Loss_G: 3.24573, Loss_KL: 0.34725\n",
      "[351/400] Loss_D: 0.59426, Loss_G: 2.83825, Loss_KL: 0.33215\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[352/400] [0/349] Loss_D: 0.57287, Loss_G: 3.28414, Loss_KL: 0.32448\n",
      "[352/400] [100/349] Loss_D: 0.58702, Loss_G: 2.34421, Loss_KL: 0.31526\n",
      "[352/400] [200/349] Loss_D: 0.38446, Loss_G: 2.71506, Loss_KL: 0.30305\n",
      "[352/400] [300/349] Loss_D: 0.65374, Loss_G: 2.60839, Loss_KL: 0.37309\n",
      "[352/400] Loss_D: 0.60871, Loss_G: 2.80177, Loss_KL: 0.33333\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[353/400] [0/349] Loss_D: 0.86159, Loss_G: 2.47637, Loss_KL: 0.26156\n",
      "[353/400] [100/349] Loss_D: 0.69671, Loss_G: 2.60245, Loss_KL: 0.29727\n",
      "[353/400] [200/349] Loss_D: 0.71089, Loss_G: 2.16426, Loss_KL: 0.38609\n",
      "[353/400] [300/349] Loss_D: 0.70362, Loss_G: 2.67018, Loss_KL: 0.29114\n",
      "[353/400] Loss_D: 0.59899, Loss_G: 2.82173, Loss_KL: 0.33546\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[354/400] [0/349] Loss_D: 0.78100, Loss_G: 3.13822, Loss_KL: 0.30666\n",
      "[354/400] [100/349] Loss_D: 0.63744, Loss_G: 3.47948, Loss_KL: 0.32522\n",
      "[354/400] [200/349] Loss_D: 0.41435, Loss_G: 3.16900, Loss_KL: 0.35502\n",
      "[354/400] [300/349] Loss_D: 0.84029, Loss_G: 2.20160, Loss_KL: 0.34303\n",
      "[354/400] Loss_D: 0.60936, Loss_G: 2.83782, Loss_KL: 0.33792\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[355/400] [0/349] Loss_D: 0.85638, Loss_G: 3.08991, Loss_KL: 0.29859\n",
      "[355/400] [100/349] Loss_D: 0.75198, Loss_G: 2.16766, Loss_KL: 0.31690\n",
      "[355/400] [200/349] Loss_D: 0.25930, Loss_G: 2.25623, Loss_KL: 0.31671\n",
      "[355/400] [300/349] Loss_D: 0.74166, Loss_G: 3.60311, Loss_KL: 0.38495\n",
      "[355/400] Loss_D: 0.60695, Loss_G: 2.80098, Loss_KL: 0.33201\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[356/400] [0/349] Loss_D: 0.43019, Loss_G: 2.46054, Loss_KL: 0.31350\n",
      "[356/400] [100/349] Loss_D: 0.45586, Loss_G: 3.17337, Loss_KL: 0.34638\n",
      "[356/400] [200/349] Loss_D: 0.83557, Loss_G: 2.52726, Loss_KL: 0.28918\n",
      "[356/400] [300/349] Loss_D: 0.86306, Loss_G: 3.10781, Loss_KL: 0.32154\n",
      "[356/400] Loss_D: 0.59189, Loss_G: 2.83994, Loss_KL: 0.33620\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[357/400] [0/349] Loss_D: 0.20089, Loss_G: 2.88419, Loss_KL: 0.33582\n",
      "[357/400] [100/349] Loss_D: 0.54175, Loss_G: 3.01237, Loss_KL: 0.33527\n",
      "[357/400] [200/349] Loss_D: 0.62203, Loss_G: 2.95621, Loss_KL: 0.29816\n",
      "[357/400] [300/349] Loss_D: 0.58607, Loss_G: 2.34054, Loss_KL: 0.36298\n",
      "[357/400] Loss_D: 0.59379, Loss_G: 2.85374, Loss_KL: 0.33741\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[358/400] [0/349] Loss_D: 0.67795, Loss_G: 2.19177, Loss_KL: 0.29965\n",
      "[358/400] [100/349] Loss_D: 0.76919, Loss_G: 2.55782, Loss_KL: 0.34237\n",
      "[358/400] [200/349] Loss_D: 0.34708, Loss_G: 3.85010, Loss_KL: 0.35921\n",
      "[358/400] [300/349] Loss_D: 0.44178, Loss_G: 2.86431, Loss_KL: 0.35020\n",
      "[358/400] Loss_D: 0.60964, Loss_G: 2.77140, Loss_KL: 0.33127\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[359/400] [0/349] Loss_D: 0.26312, Loss_G: 3.09309, Loss_KL: 0.41385\n",
      "[359/400] [100/349] Loss_D: 0.76201, Loss_G: 1.91649, Loss_KL: 0.28096\n",
      "[359/400] [200/349] Loss_D: 0.75656, Loss_G: 3.48803, Loss_KL: 0.33011\n",
      "[359/400] [300/349] Loss_D: 0.69790, Loss_G: 3.23157, Loss_KL: 0.40145\n",
      "[359/400] Loss_D: 0.60957, Loss_G: 2.84088, Loss_KL: 0.33216\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[360/400] [0/349] Loss_D: 0.66619, Loss_G: 2.55559, Loss_KL: 0.27098\n",
      "[360/400] [100/349] Loss_D: 0.82876, Loss_G: 2.97070, Loss_KL: 0.36430\n",
      "[360/400] [200/349] Loss_D: 0.72526, Loss_G: 3.06684, Loss_KL: 0.32445\n",
      "[360/400] [300/349] Loss_D: 0.47323, Loss_G: 2.98579, Loss_KL: 0.38162\n",
      "[360/400] Loss_D: 0.59936, Loss_G: 2.86423, Loss_KL: 0.33237\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[361/400] [0/349] Loss_D: 0.70783, Loss_G: 2.98066, Loss_KL: 0.39273\n",
      "[361/400] [100/349] Loss_D: 0.70274, Loss_G: 2.86183, Loss_KL: 0.27844\n",
      "[361/400] [200/349] Loss_D: 0.82650, Loss_G: 3.38368, Loss_KL: 0.25677\n",
      "[361/400] [300/349] Loss_D: 0.73570, Loss_G: 2.35890, Loss_KL: 0.25856\n",
      "[361/400] Loss_D: 0.59333, Loss_G: 2.81688, Loss_KL: 0.33609\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[362/400] [0/349] Loss_D: 0.57042, Loss_G: 2.75774, Loss_KL: 0.31757\n",
      "[362/400] [100/349] Loss_D: 0.82308, Loss_G: 2.47688, Loss_KL: 0.25407\n",
      "[362/400] [200/349] Loss_D: 0.71973, Loss_G: 2.91617, Loss_KL: 0.34989\n",
      "[362/400] [300/349] Loss_D: 0.63984, Loss_G: 3.15215, Loss_KL: 0.34426\n",
      "[362/400] Loss_D: 0.61766, Loss_G: 2.78130, Loss_KL: 0.33342\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[363/400] [0/349] Loss_D: 0.59621, Loss_G: 1.98179, Loss_KL: 0.35176\n",
      "[363/400] [100/349] Loss_D: 0.79157, Loss_G: 3.86470, Loss_KL: 0.35206\n",
      "[363/400] [200/349] Loss_D: 0.62941, Loss_G: 2.60399, Loss_KL: 0.29641\n",
      "[363/400] [300/349] Loss_D: 0.23547, Loss_G: 3.27047, Loss_KL: 0.37534\n",
      "[363/400] Loss_D: 0.58748, Loss_G: 2.86701, Loss_KL: 0.33485\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[364/400] [0/349] Loss_D: 0.54935, Loss_G: 2.17619, Loss_KL: 0.32029\n",
      "[364/400] [100/349] Loss_D: 0.33120, Loss_G: 3.55875, Loss_KL: 0.33830\n",
      "[364/400] [200/349] Loss_D: 0.71401, Loss_G: 2.70424, Loss_KL: 0.34902\n",
      "[364/400] [300/349] Loss_D: 0.71964, Loss_G: 2.30907, Loss_KL: 0.36224\n",
      "[364/400] Loss_D: 0.60429, Loss_G: 2.80487, Loss_KL: 0.33367\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[365/400] [0/349] Loss_D: 0.80880, Loss_G: 3.91515, Loss_KL: 0.29723\n",
      "[365/400] [100/349] Loss_D: 0.84810, Loss_G: 3.18942, Loss_KL: 0.31903\n",
      "[365/400] [200/349] Loss_D: 0.78309, Loss_G: 2.85580, Loss_KL: 0.39044\n",
      "[365/400] [300/349] Loss_D: 0.69461, Loss_G: 2.58053, Loss_KL: 0.31133\n",
      "[365/400] Loss_D: 0.59439, Loss_G: 2.79570, Loss_KL: 0.33563\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[366/400] [0/349] Loss_D: 0.64176, Loss_G: 2.71936, Loss_KL: 0.28544\n",
      "[366/400] [100/349] Loss_D: 0.69553, Loss_G: 3.00798, Loss_KL: 0.31018\n",
      "[366/400] [200/349] Loss_D: 0.55866, Loss_G: 2.65291, Loss_KL: 0.30348\n",
      "[366/400] [300/349] Loss_D: 0.83616, Loss_G: 3.02535, Loss_KL: 0.35607\n",
      "[366/400] Loss_D: 0.60367, Loss_G: 2.79936, Loss_KL: 0.33614\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[367/400] [0/349] Loss_D: 0.72196, Loss_G: 3.24905, Loss_KL: 0.30865\n",
      "[367/400] [100/349] Loss_D: 0.71387, Loss_G: 2.80172, Loss_KL: 0.29949\n",
      "[367/400] [200/349] Loss_D: 0.47171, Loss_G: 2.52485, Loss_KL: 0.32545\n",
      "[367/400] [300/349] Loss_D: 0.37260, Loss_G: 2.77659, Loss_KL: 0.31213\n",
      "[367/400] Loss_D: 0.60071, Loss_G: 2.82440, Loss_KL: 0.33673\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[368/400] [0/349] Loss_D: 0.64289, Loss_G: 2.91245, Loss_KL: 0.54124\n",
      "[368/400] [100/349] Loss_D: 0.56124, Loss_G: 3.11092, Loss_KL: 0.28901\n",
      "[368/400] [200/349] Loss_D: 0.74390, Loss_G: 2.59638, Loss_KL: 0.34714\n",
      "[368/400] [300/349] Loss_D: 0.32220, Loss_G: 3.16212, Loss_KL: 0.30047\n",
      "[368/400] Loss_D: 0.59502, Loss_G: 2.82788, Loss_KL: 0.33736\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[369/400] [0/349] Loss_D: 0.76949, Loss_G: 2.88497, Loss_KL: 0.32502\n",
      "[369/400] [100/349] Loss_D: 0.43450, Loss_G: 2.78448, Loss_KL: 0.44555\n",
      "[369/400] [200/349] Loss_D: 0.83868, Loss_G: 2.35930, Loss_KL: 0.28019\n",
      "[369/400] [300/349] Loss_D: 0.40104, Loss_G: 2.78787, Loss_KL: 0.30693\n",
      "[369/400] Loss_D: 0.60345, Loss_G: 2.83585, Loss_KL: 0.33390\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[370/400] [0/349] Loss_D: 0.39272, Loss_G: 2.53444, Loss_KL: 0.39848\n",
      "[370/400] [100/349] Loss_D: 0.46189, Loss_G: 2.77937, Loss_KL: 0.31757\n",
      "[370/400] [200/349] Loss_D: 0.57380, Loss_G: 2.38997, Loss_KL: 0.35046\n",
      "[370/400] [300/349] Loss_D: 0.48495, Loss_G: 2.23462, Loss_KL: 0.25560\n",
      "[370/400] Loss_D: 0.60166, Loss_G: 2.82518, Loss_KL: 0.33052\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[371/400] [0/349] Loss_D: 0.64115, Loss_G: 2.28606, Loss_KL: 0.38168\n",
      "[371/400] [100/349] Loss_D: 0.39954, Loss_G: 3.30140, Loss_KL: 0.34044\n",
      "[371/400] [200/349] Loss_D: 0.81159, Loss_G: 2.53142, Loss_KL: 0.26569\n",
      "[371/400] [300/349] Loss_D: 0.43147, Loss_G: 2.55820, Loss_KL: 0.27994\n",
      "[371/400] Loss_D: 0.59988, Loss_G: 2.79252, Loss_KL: 0.33335\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[372/400] [0/349] Loss_D: 0.54355, Loss_G: 3.06363, Loss_KL: 0.28459\n",
      "[372/400] [100/349] Loss_D: 0.70622, Loss_G: 2.72360, Loss_KL: 0.37357\n",
      "[372/400] [200/349] Loss_D: 0.33418, Loss_G: 2.87748, Loss_KL: 0.39821\n",
      "[372/400] [300/349] Loss_D: 0.63711, Loss_G: 2.38996, Loss_KL: 0.34714\n",
      "[372/400] Loss_D: 0.60128, Loss_G: 2.85609, Loss_KL: 0.33629\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[373/400] [0/349] Loss_D: 0.58787, Loss_G: 3.65273, Loss_KL: 0.37428\n",
      "[373/400] [100/349] Loss_D: 0.69479, Loss_G: 2.12135, Loss_KL: 0.40209\n",
      "[373/400] [200/349] Loss_D: 0.56021, Loss_G: 2.62235, Loss_KL: 0.31474\n",
      "[373/400] [300/349] Loss_D: 0.67299, Loss_G: 2.90885, Loss_KL: 0.38056\n",
      "[373/400] Loss_D: 0.59579, Loss_G: 2.81628, Loss_KL: 0.33786\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[374/400] [0/349] Loss_D: 0.41106, Loss_G: 2.32510, Loss_KL: 0.31318\n",
      "[374/400] [100/349] Loss_D: 0.62903, Loss_G: 2.67932, Loss_KL: 0.29508\n",
      "[374/400] [200/349] Loss_D: 0.29717, Loss_G: 2.97016, Loss_KL: 0.35322\n",
      "[374/400] [300/349] Loss_D: 0.89464, Loss_G: 1.99517, Loss_KL: 0.34407\n",
      "[374/400] Loss_D: 0.59874, Loss_G: 2.79379, Loss_KL: 0.33492\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[375/400] [0/349] Loss_D: 0.24548, Loss_G: 2.67317, Loss_KL: 0.33002\n",
      "[375/400] [100/349] Loss_D: 0.38304, Loss_G: 2.73332, Loss_KL: 0.32039\n",
      "[375/400] [200/349] Loss_D: 0.24533, Loss_G: 2.41463, Loss_KL: 0.31426\n",
      "[375/400] [300/349] Loss_D: 0.64995, Loss_G: 3.38154, Loss_KL: 0.28584\n",
      "[375/400] Loss_D: 0.58632, Loss_G: 2.87088, Loss_KL: 0.33706\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[376/400] [0/349] Loss_D: 0.63228, Loss_G: 2.77878, Loss_KL: 0.25870\n",
      "[376/400] [100/349] Loss_D: 0.91740, Loss_G: 3.44900, Loss_KL: 0.33202\n",
      "[376/400] [200/349] Loss_D: 0.72298, Loss_G: 3.56396, Loss_KL: 0.29111\n",
      "[376/400] [300/349] Loss_D: 0.55181, Loss_G: 3.17159, Loss_KL: 0.35064\n",
      "[376/400] Loss_D: 0.60337, Loss_G: 2.79730, Loss_KL: 0.33782\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[377/400] [0/349] Loss_D: 0.49208, Loss_G: 3.81257, Loss_KL: 0.27001\n",
      "[377/400] [100/349] Loss_D: 0.74243, Loss_G: 2.96697, Loss_KL: 0.27326\n",
      "[377/400] [200/349] Loss_D: 0.41633, Loss_G: 3.57627, Loss_KL: 0.39888\n",
      "[377/400] [300/349] Loss_D: 0.50183, Loss_G: 2.84929, Loss_KL: 0.30355\n",
      "[377/400] Loss_D: 0.58841, Loss_G: 2.84723, Loss_KL: 0.34189\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[378/400] [0/349] Loss_D: 0.69914, Loss_G: 2.33778, Loss_KL: 0.45726\n",
      "[378/400] [100/349] Loss_D: 0.51775, Loss_G: 2.57117, Loss_KL: 0.31688\n",
      "[378/400] [200/349] Loss_D: 0.20080, Loss_G: 3.07078, Loss_KL: 0.33906\n",
      "[378/400] [300/349] Loss_D: 0.44265, Loss_G: 2.28775, Loss_KL: 0.34462\n",
      "[378/400] Loss_D: 0.60468, Loss_G: 2.82471, Loss_KL: 0.34127\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[379/400] [0/349] Loss_D: 0.66428, Loss_G: 2.62349, Loss_KL: 0.33756\n",
      "[379/400] [100/349] Loss_D: 0.31203, Loss_G: 2.67330, Loss_KL: 0.42488\n",
      "[379/400] [200/349] Loss_D: 0.46212, Loss_G: 2.53193, Loss_KL: 0.37630\n",
      "[379/400] [300/349] Loss_D: 0.47448, Loss_G: 2.97902, Loss_KL: 0.27478\n",
      "[379/400] Loss_D: 0.59148, Loss_G: 2.82873, Loss_KL: 0.33310\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[380/400] [0/349] Loss_D: 0.42363, Loss_G: 2.89553, Loss_KL: 0.32387\n",
      "[380/400] [100/349] Loss_D: 0.26903, Loss_G: 3.42180, Loss_KL: 0.36434\n",
      "[380/400] [200/349] Loss_D: 0.53874, Loss_G: 3.12830, Loss_KL: 0.31758\n",
      "[380/400] [300/349] Loss_D: 0.48038, Loss_G: 2.48268, Loss_KL: 0.33717\n",
      "[380/400] Loss_D: 0.60511, Loss_G: 2.84029, Loss_KL: 0.32965\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[381/400] [0/349] Loss_D: 0.68881, Loss_G: 3.49684, Loss_KL: 0.30358\n",
      "[381/400] [100/349] Loss_D: 0.74853, Loss_G: 2.69603, Loss_KL: 0.38063\n",
      "[381/400] [200/349] Loss_D: 0.44304, Loss_G: 2.54323, Loss_KL: 0.35391\n",
      "[381/400] [300/349] Loss_D: 0.77348, Loss_G: 2.38795, Loss_KL: 0.44918\n",
      "[381/400] Loss_D: 0.61450, Loss_G: 2.77917, Loss_KL: 0.32580\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[382/400] [0/349] Loss_D: 0.42328, Loss_G: 2.58493, Loss_KL: 0.32130\n",
      "[382/400] [100/349] Loss_D: 0.46396, Loss_G: 2.27822, Loss_KL: 0.34185\n",
      "[382/400] [200/349] Loss_D: 0.42321, Loss_G: 3.04477, Loss_KL: 0.35804\n",
      "[382/400] [300/349] Loss_D: 0.73559, Loss_G: 2.54289, Loss_KL: 0.25042\n",
      "[382/400] Loss_D: 0.61125, Loss_G: 2.79495, Loss_KL: 0.32649\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[383/400] [0/349] Loss_D: 0.76846, Loss_G: 2.71422, Loss_KL: 0.25816\n",
      "[383/400] [100/349] Loss_D: 0.23800, Loss_G: 3.41393, Loss_KL: 0.32861\n",
      "[383/400] [200/349] Loss_D: 0.64863, Loss_G: 3.91112, Loss_KL: 0.30121\n",
      "[383/400] [300/349] Loss_D: 0.78865, Loss_G: 3.55299, Loss_KL: 0.28885\n",
      "[383/400] Loss_D: 0.59073, Loss_G: 2.83453, Loss_KL: 0.32741\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[384/400] [0/349] Loss_D: 0.70951, Loss_G: 2.80402, Loss_KL: 0.35683\n",
      "[384/400] [100/349] Loss_D: 0.26262, Loss_G: 2.98129, Loss_KL: 0.32884\n",
      "[384/400] [200/349] Loss_D: 0.33639, Loss_G: 2.64938, Loss_KL: 0.37998\n",
      "[384/400] [300/349] Loss_D: 0.61642, Loss_G: 2.72727, Loss_KL: 0.33880\n",
      "[384/400] Loss_D: 0.59606, Loss_G: 2.79616, Loss_KL: 0.32247\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[385/400] [0/349] Loss_D: 0.24311, Loss_G: 2.84095, Loss_KL: 0.28992\n",
      "[385/400] [100/349] Loss_D: 0.55733, Loss_G: 3.35328, Loss_KL: 0.28179\n",
      "[385/400] [200/349] Loss_D: 0.53165, Loss_G: 2.67500, Loss_KL: 0.32325\n",
      "[385/400] [300/349] Loss_D: 0.56783, Loss_G: 2.84245, Loss_KL: 0.34850\n",
      "[385/400] Loss_D: 0.61042, Loss_G: 2.78769, Loss_KL: 0.32405\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[386/400] [0/349] Loss_D: 0.63187, Loss_G: 2.59616, Loss_KL: 0.35631\n",
      "[386/400] [100/349] Loss_D: 0.51892, Loss_G: 3.00215, Loss_KL: 0.33212\n",
      "[386/400] [200/349] Loss_D: 0.37449, Loss_G: 3.70265, Loss_KL: 0.28546\n",
      "[386/400] [300/349] Loss_D: 0.69894, Loss_G: 2.74310, Loss_KL: 0.33269\n",
      "[386/400] Loss_D: 0.58653, Loss_G: 2.83321, Loss_KL: 0.32283\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[387/400] [0/349] Loss_D: 0.71983, Loss_G: 2.59188, Loss_KL: 0.29589\n",
      "[387/400] [100/349] Loss_D: 0.51459, Loss_G: 1.93916, Loss_KL: 0.25244\n",
      "[387/400] [200/349] Loss_D: 0.73082, Loss_G: 2.79578, Loss_KL: 0.32575\n",
      "[387/400] [300/349] Loss_D: 0.75421, Loss_G: 2.39860, Loss_KL: 0.34462\n",
      "[387/400] Loss_D: 0.60081, Loss_G: 2.78514, Loss_KL: 0.32323\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[388/400] [0/349] Loss_D: 0.39073, Loss_G: 2.95959, Loss_KL: 0.32715\n",
      "[388/400] [100/349] Loss_D: 0.49318, Loss_G: 2.86786, Loss_KL: 0.29521\n",
      "[388/400] [200/349] Loss_D: 0.73268, Loss_G: 2.83693, Loss_KL: 0.36176\n",
      "[388/400] [300/349] Loss_D: 0.69052, Loss_G: 2.25148, Loss_KL: 0.39964\n",
      "[388/400] Loss_D: 0.60719, Loss_G: 2.78561, Loss_KL: 0.31987\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[389/400] [0/349] Loss_D: 0.45596, Loss_G: 2.43570, Loss_KL: 0.29265\n",
      "[389/400] [100/349] Loss_D: 0.57814, Loss_G: 3.02673, Loss_KL: 0.31318\n",
      "[389/400] [200/349] Loss_D: 0.72236, Loss_G: 2.28981, Loss_KL: 0.29583\n",
      "[389/400] [300/349] Loss_D: 0.47705, Loss_G: 2.22865, Loss_KL: 0.31740\n",
      "[389/400] Loss_D: 0.60431, Loss_G: 2.80221, Loss_KL: 0.32482\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[390/400] [0/349] Loss_D: 0.78890, Loss_G: 2.40978, Loss_KL: 0.37057\n",
      "[390/400] [100/349] Loss_D: 0.57354, Loss_G: 2.42517, Loss_KL: 0.32122\n",
      "[390/400] [200/349] Loss_D: 0.79439, Loss_G: 2.80339, Loss_KL: 0.33553\n",
      "[390/400] [300/349] Loss_D: 0.27690, Loss_G: 2.87602, Loss_KL: 0.35449\n",
      "[390/400] Loss_D: 0.59932, Loss_G: 2.80482, Loss_KL: 0.32619\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[391/400] [0/349] Loss_D: 0.41467, Loss_G: 2.90465, Loss_KL: 0.47548\n",
      "[391/400] [100/349] Loss_D: 0.62153, Loss_G: 2.46587, Loss_KL: 0.27074\n",
      "[391/400] [200/349] Loss_D: 0.76443, Loss_G: 2.95975, Loss_KL: 0.34625\n",
      "[391/400] [300/349] Loss_D: 0.68510, Loss_G: 2.83061, Loss_KL: 0.31232\n",
      "[391/400] Loss_D: 0.60110, Loss_G: 2.80477, Loss_KL: 0.32468\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[392/400] [0/349] Loss_D: 0.40355, Loss_G: 2.33962, Loss_KL: 0.28795\n",
      "[392/400] [100/349] Loss_D: 0.75051, Loss_G: 2.90655, Loss_KL: 0.29176\n",
      "[392/400] [200/349] Loss_D: 0.76029, Loss_G: 2.95437, Loss_KL: 0.38052\n",
      "[392/400] [300/349] Loss_D: 0.80038, Loss_G: 2.90938, Loss_KL: 0.28530\n",
      "[392/400] Loss_D: 0.59703, Loss_G: 2.85275, Loss_KL: 0.33497\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[393/400] [0/349] Loss_D: 0.51767, Loss_G: 2.32068, Loss_KL: 0.29559\n",
      "[393/400] [100/349] Loss_D: 0.41134, Loss_G: 3.12715, Loss_KL: 0.37462\n",
      "[393/400] [200/349] Loss_D: 0.50945, Loss_G: 2.72929, Loss_KL: 0.32252\n",
      "[393/400] [300/349] Loss_D: 0.85363, Loss_G: 2.01292, Loss_KL: 0.32116\n",
      "[393/400] Loss_D: 0.61367, Loss_G: 2.79556, Loss_KL: 0.32971\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[394/400] [0/349] Loss_D: 0.37313, Loss_G: 2.74610, Loss_KL: 0.29057\n",
      "[394/400] [100/349] Loss_D: 0.78039, Loss_G: 2.75522, Loss_KL: 0.25215\n",
      "[394/400] [200/349] Loss_D: 0.74251, Loss_G: 2.14254, Loss_KL: 0.28079\n",
      "[394/400] [300/349] Loss_D: 0.36086, Loss_G: 2.71117, Loss_KL: 0.36591\n",
      "[394/400] Loss_D: 0.59101, Loss_G: 2.80570, Loss_KL: 0.32742\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[395/400] [0/349] Loss_D: 0.65018, Loss_G: 3.38828, Loss_KL: 0.31695\n",
      "[395/400] [100/349] Loss_D: 0.89284, Loss_G: 2.95894, Loss_KL: 0.37215\n",
      "[395/400] [200/349] Loss_D: 0.71842, Loss_G: 3.11444, Loss_KL: 0.32078\n",
      "[395/400] [300/349] Loss_D: 0.30026, Loss_G: 2.69162, Loss_KL: 0.30170\n",
      "[395/400] Loss_D: 0.60876, Loss_G: 2.79160, Loss_KL: 0.33052\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[396/400] [0/349] Loss_D: 0.79060, Loss_G: 2.80126, Loss_KL: 0.28964\n",
      "[396/400] [100/349] Loss_D: 0.45240, Loss_G: 2.59450, Loss_KL: 0.34793\n",
      "[396/400] [200/349] Loss_D: 0.58115, Loss_G: 2.87798, Loss_KL: 0.40698\n",
      "[396/400] [300/349] Loss_D: 0.31283, Loss_G: 4.00354, Loss_KL: 0.33498\n",
      "[396/400] Loss_D: 0.61008, Loss_G: 2.82118, Loss_KL: 0.33422\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[397/400] [0/349] Loss_D: 0.64226, Loss_G: 2.67340, Loss_KL: 0.35050\n",
      "[397/400] [100/349] Loss_D: 0.70037, Loss_G: 3.19863, Loss_KL: 0.28943\n",
      "[397/400] [200/349] Loss_D: 0.43199, Loss_G: 2.66475, Loss_KL: 0.25348\n",
      "[397/400] [300/349] Loss_D: 0.34927, Loss_G: 2.24043, Loss_KL: 0.34641\n",
      "[397/400] Loss_D: 0.60332, Loss_G: 2.79888, Loss_KL: 0.32640\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[398/400] [0/349] Loss_D: 0.71583, Loss_G: 3.34990, Loss_KL: 0.33862\n",
      "[398/400] [100/349] Loss_D: 0.70927, Loss_G: 2.94417, Loss_KL: 0.33294\n",
      "[398/400] [200/349] Loss_D: 0.58702, Loss_G: 2.27659, Loss_KL: 0.31028\n",
      "[398/400] [300/349] Loss_D: 0.74274, Loss_G: 2.75927, Loss_KL: 0.32697\n",
      "[398/400] Loss_D: 0.61643, Loss_G: 2.82607, Loss_KL: 0.32725\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[399/400] [0/349] Loss_D: 0.35560, Loss_G: 2.82245, Loss_KL: 0.28032\n",
      "[399/400] [100/349] Loss_D: 0.58253, Loss_G: 2.30366, Loss_KL: 0.32145\n",
      "[399/400] [200/349] Loss_D: 0.68147, Loss_G: 2.85259, Loss_KL: 0.30032\n",
      "[399/400] [300/349] Loss_D: 0.59998, Loss_G: 3.00393, Loss_KL: 0.35959\n",
      "[399/400] Loss_D: 0.59319, Loss_G: 2.80389, Loss_KL: 0.31831\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[400/400] [0/349] Loss_D: 0.41502, Loss_G: 2.68477, Loss_KL: 0.30575\n",
      "[400/400] [100/349] Loss_D: 0.43192, Loss_G: 3.16980, Loss_KL: 0.37040\n",
      "[400/400] [200/349] Loss_D: 0.44642, Loss_G: 2.58257, Loss_KL: 0.39701\n",
      "[400/400] [300/349] Loss_D: 0.49419, Loss_G: 3.73664, Loss_KL: 0.30735\n",
      "[400/400] Loss_D: 0.60216, Loss_G: 2.84654, Loss_KL: 0.32105\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Save G1/D1 models\n"
     ]
    }
   ],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 256\n",
      "using openai/clip-vit-base-patch32 as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 11199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=2\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loaded from clip/clip_out_VI/checkpoint_s1_clip_onels_1biasF/netG1_epoch_400.pth, starting at epoch: 400\n",
      "Initialized stage2 Generator\n",
      "Initialized, stage 2 discriminator\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Traininig Stage: 2, outputs at: clip/clip_out_VI/tensorboard_s2_clip_onels_1biasF, clip/clip_out_VI/results_s2_clip_onels_1biasF, clip/clip_out_VI/checkpoint_s2_clip_onels_1biasF\n",
      "[1/400] [0/349] Loss_D: 1.72663, Loss_G: 0.34386, Loss_KL: 0.01061\n",
      "[1/400] [100/349] Loss_D: 1.49905, Loss_G: 1.12112, Loss_KL: 0.00292\n",
      "[1/400] [200/349] Loss_D: 1.33693, Loss_G: 1.21798, Loss_KL: 0.00101\n",
      "[1/400] [300/349] Loss_D: 1.33372, Loss_G: 1.20604, Loss_KL: 0.00063\n",
      "[1/400] Loss_D: 1.57913, Loss_G: 1.52084, Loss_KL: 0.00256\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[2/400] [0/349] Loss_D: 1.37315, Loss_G: 1.21679, Loss_KL: 0.00089\n",
      "[2/400] [100/349] Loss_D: 1.41355, Loss_G: 1.55463, Loss_KL: 0.00299\n",
      "[2/400] [200/349] Loss_D: 1.31377, Loss_G: 1.48482, Loss_KL: 0.00887\n",
      "[2/400] [300/349] Loss_D: 1.21102, Loss_G: 1.01128, Loss_KL: 0.01684\n",
      "[2/400] Loss_D: 1.35218, Loss_G: 1.17740, Loss_KL: 0.00903\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[3/400] [0/349] Loss_D: 1.31796, Loss_G: 1.02347, Loss_KL: 0.01796\n",
      "[3/400] [100/349] Loss_D: 1.33023, Loss_G: 0.86831, Loss_KL: 0.02081\n",
      "[3/400] [200/349] Loss_D: 1.29675, Loss_G: 0.77151, Loss_KL: 0.02211\n",
      "[3/400] [300/349] Loss_D: 1.12949, Loss_G: 1.01811, Loss_KL: 0.02113\n",
      "[3/400] Loss_D: 1.32698, Loss_G: 1.13140, Loss_KL: 0.02046\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[4/400] [0/349] Loss_D: 1.22021, Loss_G: 1.28865, Loss_KL: 0.02142\n",
      "[4/400] [100/349] Loss_D: 1.37056, Loss_G: 0.88237, Loss_KL: 0.02010\n",
      "[4/400] [200/349] Loss_D: 1.39543, Loss_G: 1.05496, Loss_KL: 0.02415\n",
      "[4/400] [300/349] Loss_D: 1.30712, Loss_G: 0.70463, Loss_KL: 0.01845\n",
      "[4/400] Loss_D: 1.28668, Loss_G: 1.23017, Loss_KL: 0.02036\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[5/400] [0/349] Loss_D: 1.14199, Loss_G: 1.29551, Loss_KL: 0.01879\n",
      "[5/400] [100/349] Loss_D: 1.35582, Loss_G: 1.28042, Loss_KL: 0.01952\n",
      "[5/400] [200/349] Loss_D: 1.28805, Loss_G: 1.74776, Loss_KL: 0.03019\n",
      "[5/400] [300/349] Loss_D: 1.08899, Loss_G: 1.37956, Loss_KL: 0.03666\n",
      "[5/400] Loss_D: 1.27395, Loss_G: 1.22420, Loss_KL: 0.02936\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[6/400] [0/349] Loss_D: 1.67144, Loss_G: 0.38453, Loss_KL: 0.02115\n",
      "[6/400] [100/349] Loss_D: 1.17273, Loss_G: 1.90615, Loss_KL: 0.02679\n",
      "[6/400] [200/349] Loss_D: 1.26143, Loss_G: 1.01227, Loss_KL: 0.02973\n",
      "[6/400] [300/349] Loss_D: 1.25760, Loss_G: 1.46415, Loss_KL: 0.03237\n",
      "[6/400] Loss_D: 1.26637, Loss_G: 1.17891, Loss_KL: 0.02711\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[7/400] [0/349] Loss_D: 1.10692, Loss_G: 1.37604, Loss_KL: 0.03149\n",
      "[7/400] [100/349] Loss_D: 1.68557, Loss_G: 1.35948, Loss_KL: 0.02628\n",
      "[7/400] [200/349] Loss_D: 1.14022, Loss_G: 0.96839, Loss_KL: 0.02496\n",
      "[7/400] [300/349] Loss_D: 1.19316, Loss_G: 1.91210, Loss_KL: 0.02783\n",
      "[7/400] Loss_D: 1.23890, Loss_G: 1.25092, Loss_KL: 0.02885\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[8/400] [0/349] Loss_D: 1.01963, Loss_G: 1.42170, Loss_KL: 0.04002\n",
      "[8/400] [100/349] Loss_D: 1.24298, Loss_G: 0.96416, Loss_KL: 0.02752\n",
      "[8/400] [200/349] Loss_D: 1.80409, Loss_G: 1.70047, Loss_KL: 0.02410\n",
      "[8/400] [300/349] Loss_D: 1.34379, Loss_G: 0.47321, Loss_KL: 0.03282\n",
      "[8/400] Loss_D: 1.23128, Loss_G: 1.29991, Loss_KL: 0.03122\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[9/400] [0/349] Loss_D: 1.09588, Loss_G: 0.83019, Loss_KL: 0.02190\n",
      "[9/400] [100/349] Loss_D: 0.96344, Loss_G: 1.81298, Loss_KL: 0.02764\n",
      "[9/400] [200/349] Loss_D: 1.16996, Loss_G: 1.58182, Loss_KL: 0.02563\n",
      "[9/400] [300/349] Loss_D: 1.16696, Loss_G: 0.82434, Loss_KL: 0.03829\n",
      "[9/400] Loss_D: 1.19432, Loss_G: 1.40068, Loss_KL: 0.02831\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[10/400] [0/349] Loss_D: 1.28496, Loss_G: 2.77255, Loss_KL: 0.03382\n",
      "[10/400] [100/349] Loss_D: 1.48361, Loss_G: 0.53416, Loss_KL: 0.02422\n",
      "[10/400] [200/349] Loss_D: 1.12710, Loss_G: 2.09322, Loss_KL: 0.04120\n",
      "[10/400] [300/349] Loss_D: 1.08946, Loss_G: 1.04291, Loss_KL: 0.03832\n",
      "[10/400] Loss_D: 1.17045, Loss_G: 1.50293, Loss_KL: 0.03268\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[11/400] [0/349] Loss_D: 1.22181, Loss_G: 0.64264, Loss_KL: 0.02896\n",
      "[11/400] [100/349] Loss_D: 0.97508, Loss_G: 1.72832, Loss_KL: 0.04363\n",
      "[11/400] [200/349] Loss_D: 1.32882, Loss_G: 0.88607, Loss_KL: 0.02941\n",
      "[11/400] [300/349] Loss_D: 1.09386, Loss_G: 1.04248, Loss_KL: 0.04395\n",
      "[11/400] Loss_D: 1.18017, Loss_G: 1.40284, Loss_KL: 0.03940\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[12/400] [0/349] Loss_D: 1.55423, Loss_G: 2.58259, Loss_KL: 0.04827\n",
      "[12/400] [100/349] Loss_D: 1.12124, Loss_G: 1.73300, Loss_KL: 0.04860\n",
      "[12/400] [200/349] Loss_D: 1.32927, Loss_G: 0.86781, Loss_KL: 0.02766\n",
      "[12/400] [300/349] Loss_D: 0.93249, Loss_G: 2.84828, Loss_KL: 0.03805\n",
      "[12/400] Loss_D: 1.14342, Loss_G: 1.60237, Loss_KL: 0.03955\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[13/400] [0/349] Loss_D: 0.85888, Loss_G: 1.36972, Loss_KL: 0.03361\n",
      "[13/400] [100/349] Loss_D: 1.17352, Loss_G: 1.36852, Loss_KL: 0.04746\n",
      "[13/400] [200/349] Loss_D: 1.29030, Loss_G: 1.11325, Loss_KL: 0.03759\n",
      "[13/400] [300/349] Loss_D: 0.87609, Loss_G: 2.05953, Loss_KL: 0.04067\n",
      "[13/400] Loss_D: 1.13583, Loss_G: 1.59883, Loss_KL: 0.04016\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[14/400] [0/349] Loss_D: 1.13007, Loss_G: 1.19357, Loss_KL: 0.04647\n",
      "[14/400] [100/349] Loss_D: 1.10411, Loss_G: 1.98537, Loss_KL: 0.03746\n",
      "[14/400] [200/349] Loss_D: 1.11273, Loss_G: 2.47581, Loss_KL: 0.04461\n",
      "[14/400] [300/349] Loss_D: 0.69088, Loss_G: 2.15439, Loss_KL: 0.02931\n",
      "[14/400] Loss_D: 1.08883, Loss_G: 1.78359, Loss_KL: 0.04209\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[15/400] [0/349] Loss_D: 1.01049, Loss_G: 1.65745, Loss_KL: 0.04718\n",
      "[15/400] [100/349] Loss_D: 1.19217, Loss_G: 1.32866, Loss_KL: 0.03807\n",
      "[15/400] [200/349] Loss_D: 1.10983, Loss_G: 2.89649, Loss_KL: 0.06117\n",
      "[15/400] [300/349] Loss_D: 0.98107, Loss_G: 2.32824, Loss_KL: 0.05174\n",
      "[15/400] Loss_D: 1.07263, Loss_G: 1.86640, Loss_KL: 0.04476\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[16/400] [0/349] Loss_D: 1.22616, Loss_G: 0.98857, Loss_KL: 0.04750\n",
      "[16/400] [100/349] Loss_D: 1.11797, Loss_G: 1.17217, Loss_KL: 0.03859\n",
      "[16/400] [200/349] Loss_D: 0.66717, Loss_G: 1.93005, Loss_KL: 0.04934\n",
      "[16/400] [300/349] Loss_D: 0.97589, Loss_G: 1.35230, Loss_KL: 0.03954\n",
      "[16/400] Loss_D: 1.05546, Loss_G: 1.89617, Loss_KL: 0.04598\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[17/400] [0/349] Loss_D: 0.99746, Loss_G: 2.65207, Loss_KL: 0.03905\n",
      "[17/400] [100/349] Loss_D: 0.91784, Loss_G: 3.56749, Loss_KL: 0.03123\n",
      "[17/400] [200/349] Loss_D: 1.05432, Loss_G: 2.46530, Loss_KL: 0.03599\n",
      "[17/400] [300/349] Loss_D: 0.84290, Loss_G: 2.36315, Loss_KL: 0.03774\n",
      "[17/400] Loss_D: 1.01795, Loss_G: 2.06637, Loss_KL: 0.04355\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[18/400] [0/349] Loss_D: 0.78490, Loss_G: 2.42386, Loss_KL: 0.03724\n",
      "[18/400] [100/349] Loss_D: 1.08439, Loss_G: 1.49396, Loss_KL: 0.04788\n",
      "[18/400] [200/349] Loss_D: 1.01157, Loss_G: 2.88344, Loss_KL: 0.06402\n",
      "[18/400] [300/349] Loss_D: 1.78787, Loss_G: 0.57064, Loss_KL: 0.04591\n",
      "[18/400] Loss_D: 1.02359, Loss_G: 2.08966, Loss_KL: 0.04566\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[19/400] [0/349] Loss_D: 0.92246, Loss_G: 3.00611, Loss_KL: 0.05168\n",
      "[19/400] [100/349] Loss_D: 0.94201, Loss_G: 1.63467, Loss_KL: 0.05931\n",
      "[19/400] [200/349] Loss_D: 1.56696, Loss_G: 0.67308, Loss_KL: 0.05293\n",
      "[19/400] [300/349] Loss_D: 1.02887, Loss_G: 3.15299, Loss_KL: 0.05567\n",
      "[19/400] Loss_D: 1.00998, Loss_G: 2.11489, Loss_KL: 0.05243\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[20/400] [0/349] Loss_D: 1.06768, Loss_G: 2.14867, Loss_KL: 0.05867\n",
      "[20/400] [100/349] Loss_D: 1.54338, Loss_G: 2.99950, Loss_KL: 0.03368\n",
      "[20/400] [200/349] Loss_D: 1.08094, Loss_G: 2.32579, Loss_KL: 0.04368\n",
      "[20/400] [300/349] Loss_D: 0.95977, Loss_G: 2.06706, Loss_KL: 0.04622\n",
      "[20/400] Loss_D: 1.01964, Loss_G: 2.11202, Loss_KL: 0.04423\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[21/400] [0/349] Loss_D: 0.82827, Loss_G: 2.32817, Loss_KL: 0.04858\n",
      "[21/400] [100/349] Loss_D: 0.51095, Loss_G: 1.93877, Loss_KL: 0.04035\n",
      "[21/400] [200/349] Loss_D: 0.99448, Loss_G: 3.70346, Loss_KL: 0.03781\n",
      "[21/400] [300/349] Loss_D: 0.99957, Loss_G: 1.61994, Loss_KL: 0.04734\n",
      "[21/400] Loss_D: 0.99994, Loss_G: 2.16258, Loss_KL: 0.04566\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[22/400] [0/349] Loss_D: 0.80648, Loss_G: 3.10414, Loss_KL: 0.03592\n",
      "[22/400] [100/349] Loss_D: 1.10554, Loss_G: 4.83767, Loss_KL: 0.03912\n",
      "[22/400] [200/349] Loss_D: 0.82982, Loss_G: 1.55530, Loss_KL: 0.04841\n",
      "[22/400] [300/349] Loss_D: 0.88273, Loss_G: 1.69578, Loss_KL: 0.03990\n",
      "[22/400] Loss_D: 0.94974, Loss_G: 2.33726, Loss_KL: 0.04391\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[23/400] [0/349] Loss_D: 0.79639, Loss_G: 3.04737, Loss_KL: 0.05676\n",
      "[23/400] [100/349] Loss_D: 0.96099, Loss_G: 1.83376, Loss_KL: 0.04411\n",
      "[23/400] [200/349] Loss_D: 0.90086, Loss_G: 3.11147, Loss_KL: 0.02554\n",
      "[23/400] [300/349] Loss_D: 0.98511, Loss_G: 3.33596, Loss_KL: 0.03164\n",
      "[23/400] Loss_D: 0.96302, Loss_G: 2.31992, Loss_KL: 0.04059\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[24/400] [0/349] Loss_D: 1.12748, Loss_G: 2.72426, Loss_KL: 0.02928\n",
      "[24/400] [100/349] Loss_D: 0.83157, Loss_G: 2.21884, Loss_KL: 0.03358\n",
      "[24/400] [200/349] Loss_D: 0.83001, Loss_G: 2.74588, Loss_KL: 0.04529\n",
      "[24/400] [300/349] Loss_D: 0.63209, Loss_G: 2.18194, Loss_KL: 0.04864\n",
      "[24/400] Loss_D: 0.93585, Loss_G: 2.43022, Loss_KL: 0.04441\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[25/400] [0/349] Loss_D: 0.75423, Loss_G: 2.22988, Loss_KL: 0.05174\n",
      "[25/400] [100/349] Loss_D: 0.82326, Loss_G: 2.66324, Loss_KL: 0.05277\n",
      "[25/400] [200/349] Loss_D: 0.92490, Loss_G: 3.42532, Loss_KL: 0.05224\n",
      "[25/400] [300/349] Loss_D: 0.56295, Loss_G: 3.01557, Loss_KL: 0.03306\n",
      "[25/400] Loss_D: 0.95816, Loss_G: 2.38635, Loss_KL: 0.04732\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[26/400] [0/349] Loss_D: 1.02635, Loss_G: 1.97054, Loss_KL: 0.04105\n",
      "[26/400] [100/349] Loss_D: 0.86415, Loss_G: 1.66048, Loss_KL: 0.04777\n",
      "[26/400] [200/349] Loss_D: 1.26414, Loss_G: 0.93081, Loss_KL: 0.04924\n",
      "[26/400] [300/349] Loss_D: 1.02520, Loss_G: 1.77492, Loss_KL: 0.04094\n",
      "[26/400] Loss_D: 0.92239, Loss_G: 2.48996, Loss_KL: 0.04669\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[27/400] [0/349] Loss_D: 0.39671, Loss_G: 2.69637, Loss_KL: 0.06669\n",
      "[27/400] [100/349] Loss_D: 0.93306, Loss_G: 3.88486, Loss_KL: 0.05315\n",
      "[27/400] [200/349] Loss_D: 0.84536, Loss_G: 2.07069, Loss_KL: 0.05408\n",
      "[27/400] [300/349] Loss_D: 1.25156, Loss_G: 0.58581, Loss_KL: 0.05222\n",
      "[27/400] Loss_D: 0.88793, Loss_G: 2.61821, Loss_KL: 0.05260\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[28/400] [0/349] Loss_D: 0.90430, Loss_G: 1.54731, Loss_KL: 0.05946\n",
      "[28/400] [100/349] Loss_D: 0.77636, Loss_G: 1.89841, Loss_KL: 0.04958\n",
      "[28/400] [200/349] Loss_D: 0.85528, Loss_G: 1.17909, Loss_KL: 0.03650\n",
      "[28/400] [300/349] Loss_D: 1.01412, Loss_G: 4.35778, Loss_KL: 0.04485\n",
      "[28/400] Loss_D: 0.90746, Loss_G: 2.51154, Loss_KL: 0.04828\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[29/400] [0/349] Loss_D: 0.99924, Loss_G: 2.34019, Loss_KL: 0.04220\n",
      "[29/400] [100/349] Loss_D: 0.52855, Loss_G: 3.44803, Loss_KL: 0.05270\n",
      "[29/400] [200/349] Loss_D: 1.55320, Loss_G: 0.89858, Loss_KL: 0.05918\n",
      "[29/400] [300/349] Loss_D: 0.82795, Loss_G: 2.17250, Loss_KL: 0.03773\n",
      "[29/400] Loss_D: 0.88284, Loss_G: 2.58521, Loss_KL: 0.04929\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[30/400] [0/349] Loss_D: 1.25763, Loss_G: 1.38365, Loss_KL: 0.03599\n",
      "[30/400] [100/349] Loss_D: 0.81429, Loss_G: 2.33315, Loss_KL: 0.03596\n",
      "[30/400] [200/349] Loss_D: 1.32972, Loss_G: 2.47545, Loss_KL: 0.04218\n",
      "[30/400] [300/349] Loss_D: 0.70102, Loss_G: 2.91502, Loss_KL: 0.04754\n",
      "[30/400] Loss_D: 0.88856, Loss_G: 2.68361, Loss_KL: 0.04581\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[31/400] [0/349] Loss_D: 0.80946, Loss_G: 4.38879, Loss_KL: 0.05026\n",
      "[31/400] [100/349] Loss_D: 0.68469, Loss_G: 2.73691, Loss_KL: 0.04929\n",
      "[31/400] [200/349] Loss_D: 0.86107, Loss_G: 0.80866, Loss_KL: 0.04392\n",
      "[31/400] [300/349] Loss_D: 0.78563, Loss_G: 3.85090, Loss_KL: 0.04000\n",
      "[31/400] Loss_D: 0.86692, Loss_G: 2.67089, Loss_KL: 0.04740\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[32/400] [0/349] Loss_D: 0.57431, Loss_G: 2.74649, Loss_KL: 0.04322\n",
      "[32/400] [100/349] Loss_D: 0.79636, Loss_G: 2.63696, Loss_KL: 0.05253\n",
      "[32/400] [200/349] Loss_D: 0.98015, Loss_G: 5.62025, Loss_KL: 0.05048\n",
      "[32/400] [300/349] Loss_D: 1.25250, Loss_G: 3.12364, Loss_KL: 0.04893\n",
      "[32/400] Loss_D: 0.83906, Loss_G: 2.82652, Loss_KL: 0.05226\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[33/400] [0/349] Loss_D: 0.83960, Loss_G: 2.74031, Loss_KL: 0.05518\n",
      "[33/400] [100/349] Loss_D: 1.06090, Loss_G: 2.01004, Loss_KL: 0.05171\n",
      "[33/400] [200/349] Loss_D: 0.37137, Loss_G: 4.26465, Loss_KL: 0.04548\n",
      "[33/400] [300/349] Loss_D: 1.08743, Loss_G: 2.39419, Loss_KL: 0.05618\n",
      "[33/400] Loss_D: 0.85996, Loss_G: 2.73355, Loss_KL: 0.04995\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[34/400] [0/349] Loss_D: 0.93576, Loss_G: 3.24217, Loss_KL: 0.06420\n",
      "[34/400] [100/349] Loss_D: 0.79469, Loss_G: 3.68528, Loss_KL: 0.05264\n",
      "[34/400] [200/349] Loss_D: 0.83346, Loss_G: 1.77266, Loss_KL: 0.04462\n",
      "[34/400] [300/349] Loss_D: 0.67319, Loss_G: 2.04210, Loss_KL: 0.04594\n",
      "[34/400] Loss_D: 0.85504, Loss_G: 2.78216, Loss_KL: 0.04858\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[35/400] [0/349] Loss_D: 0.90250, Loss_G: 1.71969, Loss_KL: 0.04516\n",
      "[35/400] [100/349] Loss_D: 0.91970, Loss_G: 2.18044, Loss_KL: 0.03468\n",
      "[35/400] [200/349] Loss_D: 0.73934, Loss_G: 1.29640, Loss_KL: 0.04944\n",
      "[35/400] [300/349] Loss_D: 0.60333, Loss_G: 3.32634, Loss_KL: 0.04831\n",
      "[35/400] Loss_D: 0.83315, Loss_G: 2.92613, Loss_KL: 0.05134\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[36/400] [0/349] Loss_D: 0.59535, Loss_G: 3.35576, Loss_KL: 0.04131\n",
      "[36/400] [100/349] Loss_D: 0.75624, Loss_G: 1.81335, Loss_KL: 0.04603\n",
      "[36/400] [200/349] Loss_D: 0.58448, Loss_G: 2.34448, Loss_KL: 0.06324\n",
      "[36/400] [300/349] Loss_D: 0.92991, Loss_G: 2.98880, Loss_KL: 0.03838\n",
      "[36/400] Loss_D: 0.84266, Loss_G: 2.83222, Loss_KL: 0.05379\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[37/400] [0/349] Loss_D: 1.20867, Loss_G: 4.92024, Loss_KL: 0.04727\n",
      "[37/400] [100/349] Loss_D: 0.61339, Loss_G: 5.08410, Loss_KL: 0.05585\n",
      "[37/400] [200/349] Loss_D: 0.37818, Loss_G: 3.91810, Loss_KL: 0.06873\n",
      "[37/400] [300/349] Loss_D: 0.97670, Loss_G: 2.14337, Loss_KL: 0.05503\n",
      "[37/400] Loss_D: 0.82979, Loss_G: 2.92093, Loss_KL: 0.05664\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[38/400] [0/349] Loss_D: 0.67730, Loss_G: 3.78081, Loss_KL: 0.04971\n",
      "[38/400] [100/349] Loss_D: 0.74934, Loss_G: 3.42554, Loss_KL: 0.04152\n",
      "[38/400] [200/349] Loss_D: 0.74471, Loss_G: 2.34358, Loss_KL: 0.05299\n",
      "[38/400] [300/349] Loss_D: 0.84719, Loss_G: 4.21061, Loss_KL: 0.03695\n",
      "[38/400] Loss_D: 0.80465, Loss_G: 2.92752, Loss_KL: 0.05200\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[39/400] [0/349] Loss_D: 0.76340, Loss_G: 3.76689, Loss_KL: 0.05741\n",
      "[39/400] [100/349] Loss_D: 0.82904, Loss_G: 3.27557, Loss_KL: 0.05521\n",
      "[39/400] [200/349] Loss_D: 0.85892, Loss_G: 2.92043, Loss_KL: 0.05909\n",
      "[39/400] [300/349] Loss_D: 0.93468, Loss_G: 4.40853, Loss_KL: 0.04557\n",
      "[39/400] Loss_D: 0.80868, Loss_G: 3.02038, Loss_KL: 0.05795\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[40/400] [0/349] Loss_D: 0.79291, Loss_G: 2.19107, Loss_KL: 0.05818\n",
      "[40/400] [100/349] Loss_D: 0.78650, Loss_G: 3.01646, Loss_KL: 0.05306\n",
      "[40/400] [200/349] Loss_D: 1.01829, Loss_G: 1.43080, Loss_KL: 0.04438\n",
      "[40/400] [300/349] Loss_D: 0.52763, Loss_G: 2.86838, Loss_KL: 0.06841\n",
      "[40/400] Loss_D: 0.82755, Loss_G: 2.93728, Loss_KL: 0.05454\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[41/400] [0/349] Loss_D: 0.35577, Loss_G: 3.13104, Loss_KL: 0.05354\n",
      "[41/400] [100/349] Loss_D: 0.74485, Loss_G: 2.19028, Loss_KL: 0.05041\n",
      "[41/400] [200/349] Loss_D: 0.50804, Loss_G: 3.40674, Loss_KL: 0.06243\n",
      "[41/400] [300/349] Loss_D: 0.88024, Loss_G: 2.62675, Loss_KL: 0.04506\n",
      "[41/400] Loss_D: 0.76248, Loss_G: 3.25043, Loss_KL: 0.06080\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[42/400] [0/349] Loss_D: 0.52929, Loss_G: 4.37320, Loss_KL: 0.07082\n",
      "[42/400] [100/349] Loss_D: 0.97213, Loss_G: 2.65797, Loss_KL: 0.04893\n",
      "[42/400] [200/349] Loss_D: 0.52170, Loss_G: 3.20340, Loss_KL: 0.04692\n",
      "[42/400] [300/349] Loss_D: 1.06348, Loss_G: 1.64806, Loss_KL: 0.06735\n",
      "[42/400] Loss_D: 0.79609, Loss_G: 3.14489, Loss_KL: 0.06529\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[43/400] [0/349] Loss_D: 0.93221, Loss_G: 2.66355, Loss_KL: 0.08617\n",
      "[43/400] [100/349] Loss_D: 0.80937, Loss_G: 2.56523, Loss_KL: 0.06306\n",
      "[43/400] [200/349] Loss_D: 0.84921, Loss_G: 3.16539, Loss_KL: 0.05660\n",
      "[43/400] [300/349] Loss_D: 0.90090, Loss_G: 4.08699, Loss_KL: 0.06918\n",
      "[43/400] Loss_D: 0.77969, Loss_G: 3.16556, Loss_KL: 0.06057\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[44/400] [0/349] Loss_D: 0.79188, Loss_G: 2.97992, Loss_KL: 0.04159\n",
      "[44/400] [100/349] Loss_D: 1.19317, Loss_G: 0.75224, Loss_KL: 0.06769\n",
      "[44/400] [200/349] Loss_D: 1.28501, Loss_G: 5.63684, Loss_KL: 0.05807\n",
      "[44/400] [300/349] Loss_D: 0.77858, Loss_G: 2.27483, Loss_KL: 0.05181\n",
      "[44/400] Loss_D: 0.79975, Loss_G: 3.04858, Loss_KL: 0.06406\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[45/400] [0/349] Loss_D: 0.44262, Loss_G: 3.87644, Loss_KL: 0.04674\n",
      "[45/400] [100/349] Loss_D: 0.67928, Loss_G: 2.38435, Loss_KL: 0.05941\n",
      "[45/400] [200/349] Loss_D: 0.74681, Loss_G: 2.35868, Loss_KL: 0.06700\n",
      "[45/400] [300/349] Loss_D: 0.52021, Loss_G: 4.31011, Loss_KL: 0.07170\n",
      "[45/400] Loss_D: 0.78661, Loss_G: 3.08482, Loss_KL: 0.06077\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[46/400] [0/349] Loss_D: 0.86608, Loss_G: 3.62901, Loss_KL: 0.06352\n",
      "[46/400] [100/349] Loss_D: 0.75938, Loss_G: 2.68021, Loss_KL: 0.06810\n",
      "[46/400] [200/349] Loss_D: 0.75624, Loss_G: 4.96471, Loss_KL: 0.05922\n",
      "[46/400] [300/349] Loss_D: 0.77149, Loss_G: 5.38584, Loss_KL: 0.06700\n",
      "[46/400] Loss_D: 0.77418, Loss_G: 3.19816, Loss_KL: 0.06164\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[47/400] [0/349] Loss_D: 0.51422, Loss_G: 2.74995, Loss_KL: 0.05676\n",
      "[47/400] [100/349] Loss_D: 0.66123, Loss_G: 2.90721, Loss_KL: 0.06758\n",
      "[47/400] [200/349] Loss_D: 0.74642, Loss_G: 2.55814, Loss_KL: 0.05823\n",
      "[47/400] [300/349] Loss_D: 1.41854, Loss_G: 0.89603, Loss_KL: 0.06717\n",
      "[47/400] Loss_D: 0.76360, Loss_G: 3.21266, Loss_KL: 0.06222\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[48/400] [0/349] Loss_D: 0.84341, Loss_G: 3.21429, Loss_KL: 0.05760\n",
      "[48/400] [100/349] Loss_D: 0.80283, Loss_G: 2.39120, Loss_KL: 0.06479\n",
      "[48/400] [200/349] Loss_D: 0.32850, Loss_G: 3.17355, Loss_KL: 0.07536\n",
      "[48/400] [300/349] Loss_D: 1.68599, Loss_G: 4.25279, Loss_KL: 0.08217\n",
      "[48/400] Loss_D: 0.76981, Loss_G: 3.15693, Loss_KL: 0.06787\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[49/400] [0/349] Loss_D: 0.84361, Loss_G: 3.43082, Loss_KL: 0.06744\n",
      "[49/400] [100/349] Loss_D: 0.83315, Loss_G: 1.37266, Loss_KL: 0.06078\n",
      "[49/400] [200/349] Loss_D: 0.69938, Loss_G: 2.80441, Loss_KL: 0.04146\n",
      "[49/400] [300/349] Loss_D: 0.53717, Loss_G: 2.32590, Loss_KL: 0.05285\n",
      "[49/400] Loss_D: 0.76226, Loss_G: 3.20726, Loss_KL: 0.05971\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[50/400] [0/349] Loss_D: 0.96100, Loss_G: 3.30789, Loss_KL: 0.05503\n",
      "[50/400] [100/349] Loss_D: 0.35104, Loss_G: 3.66400, Loss_KL: 0.04417\n",
      "[50/400] [200/349] Loss_D: 0.97680, Loss_G: 4.01936, Loss_KL: 0.06847\n",
      "[50/400] [300/349] Loss_D: 0.76461, Loss_G: 1.96565, Loss_KL: 0.07575\n",
      "[50/400] Loss_D: 0.74981, Loss_G: 3.17080, Loss_KL: 0.06599\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[51/400] [0/349] Loss_D: 0.77638, Loss_G: 3.08892, Loss_KL: 0.06547\n",
      "[51/400] [100/349] Loss_D: 0.94997, Loss_G: 3.71805, Loss_KL: 0.07095\n",
      "[51/400] [200/349] Loss_D: 0.58781, Loss_G: 4.28556, Loss_KL: 0.07755\n",
      "[51/400] [300/349] Loss_D: 0.73126, Loss_G: 4.00021, Loss_KL: 0.07462\n",
      "[51/400] Loss_D: 0.72309, Loss_G: 3.47170, Loss_KL: 0.07057\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[52/400] [0/349] Loss_D: 0.84701, Loss_G: 2.93774, Loss_KL: 0.06169\n",
      "[52/400] [100/349] Loss_D: 0.62710, Loss_G: 2.81077, Loss_KL: 0.07381\n",
      "[52/400] [200/349] Loss_D: 0.70823, Loss_G: 2.60097, Loss_KL: 0.07490\n",
      "[52/400] [300/349] Loss_D: 0.86682, Loss_G: 2.65454, Loss_KL: 0.06800\n",
      "[52/400] Loss_D: 0.73781, Loss_G: 3.20356, Loss_KL: 0.06422\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[53/400] [0/349] Loss_D: 0.71769, Loss_G: 1.79769, Loss_KL: 0.05871\n",
      "[53/400] [100/349] Loss_D: 0.85222, Loss_G: 2.28021, Loss_KL: 0.04731\n",
      "[53/400] [200/349] Loss_D: 0.98978, Loss_G: 1.43777, Loss_KL: 0.07180\n",
      "[53/400] [300/349] Loss_D: 0.69942, Loss_G: 3.86247, Loss_KL: 0.07401\n",
      "[53/400] Loss_D: 0.73411, Loss_G: 3.35388, Loss_KL: 0.06443\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[54/400] [0/349] Loss_D: 1.14767, Loss_G: 3.36030, Loss_KL: 0.05522\n",
      "[54/400] [100/349] Loss_D: 0.91128, Loss_G: 3.37065, Loss_KL: 0.07432\n",
      "[54/400] [200/349] Loss_D: 0.74158, Loss_G: 3.75972, Loss_KL: 0.04163\n",
      "[54/400] [300/349] Loss_D: 0.72698, Loss_G: 4.74108, Loss_KL: 0.09849\n",
      "[54/400] Loss_D: 0.72699, Loss_G: 3.28960, Loss_KL: 0.06585\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[55/400] [0/349] Loss_D: 0.88102, Loss_G: 1.72223, Loss_KL: 0.05370\n",
      "[55/400] [100/349] Loss_D: 0.74443, Loss_G: 3.10472, Loss_KL: 0.06646\n",
      "[55/400] [200/349] Loss_D: 0.77800, Loss_G: 2.77584, Loss_KL: 0.09981\n",
      "[55/400] [300/349] Loss_D: 0.66005, Loss_G: 2.09933, Loss_KL: 0.07206\n",
      "[55/400] Loss_D: 0.74005, Loss_G: 3.19854, Loss_KL: 0.06752\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[56/400] [0/349] Loss_D: 0.56372, Loss_G: 4.32956, Loss_KL: 0.06398\n",
      "[56/400] [100/349] Loss_D: 1.10847, Loss_G: 4.37994, Loss_KL: 0.05343\n",
      "[56/400] [200/349] Loss_D: 0.87217, Loss_G: 3.89320, Loss_KL: 0.05884\n",
      "[56/400] [300/349] Loss_D: 0.43200, Loss_G: 3.49190, Loss_KL: 0.05425\n",
      "[56/400] Loss_D: 0.74530, Loss_G: 3.18631, Loss_KL: 0.06697\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[57/400] [0/349] Loss_D: 0.79523, Loss_G: 4.31543, Loss_KL: 0.05383\n",
      "[57/400] [100/349] Loss_D: 0.67854, Loss_G: 1.89780, Loss_KL: 0.08644\n",
      "[57/400] [200/349] Loss_D: 0.94295, Loss_G: 5.62166, Loss_KL: 0.07726\n",
      "[57/400] [300/349] Loss_D: 0.78194, Loss_G: 4.40751, Loss_KL: 0.07410\n",
      "[57/400] Loss_D: 0.74527, Loss_G: 3.20802, Loss_KL: 0.07252\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[58/400] [0/349] Loss_D: 1.27898, Loss_G: 4.63614, Loss_KL: 0.09221\n",
      "[58/400] [100/349] Loss_D: 0.74245, Loss_G: 4.93087, Loss_KL: 0.05603\n",
      "[58/400] [200/349] Loss_D: 0.90379, Loss_G: 2.59507, Loss_KL: 0.06271\n",
      "[58/400] [300/349] Loss_D: 0.56874, Loss_G: 3.36013, Loss_KL: 0.06000\n",
      "[58/400] Loss_D: 0.74596, Loss_G: 3.41079, Loss_KL: 0.07397\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[59/400] [0/349] Loss_D: 1.11022, Loss_G: 4.28991, Loss_KL: 0.06958\n",
      "[59/400] [100/349] Loss_D: 0.71939, Loss_G: 2.93004, Loss_KL: 0.06539\n",
      "[59/400] [200/349] Loss_D: 0.74408, Loss_G: 3.81912, Loss_KL: 0.07982\n",
      "[59/400] [300/349] Loss_D: 0.27527, Loss_G: 2.42007, Loss_KL: 0.06248\n",
      "[59/400] Loss_D: 0.71137, Loss_G: 3.39158, Loss_KL: 0.07634\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[60/400] [0/349] Loss_D: 0.47425, Loss_G: 2.99042, Loss_KL: 0.05843\n",
      "[60/400] [100/349] Loss_D: 1.25448, Loss_G: 0.97956, Loss_KL: 0.06206\n",
      "[60/400] [200/349] Loss_D: 0.72307, Loss_G: 2.72877, Loss_KL: 0.04514\n",
      "[60/400] [300/349] Loss_D: 1.11002, Loss_G: 3.21162, Loss_KL: 0.06574\n",
      "[60/400] Loss_D: 0.73789, Loss_G: 3.27812, Loss_KL: 0.06706\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[61/400] [0/349] Loss_D: 0.86803, Loss_G: 2.52254, Loss_KL: 0.08195\n",
      "[61/400] [100/349] Loss_D: 0.46953, Loss_G: 4.26192, Loss_KL: 0.08402\n",
      "[61/400] [200/349] Loss_D: 0.75051, Loss_G: 2.23814, Loss_KL: 0.08902\n",
      "[61/400] [300/349] Loss_D: 0.99520, Loss_G: 3.32153, Loss_KL: 0.10017\n",
      "[61/400] Loss_D: 0.72474, Loss_G: 3.23750, Loss_KL: 0.07589\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[62/400] [0/349] Loss_D: 0.96828, Loss_G: 5.43658, Loss_KL: 0.07241\n",
      "[62/400] [100/349] Loss_D: 0.68478, Loss_G: 4.24493, Loss_KL: 0.06800\n",
      "[62/400] [200/349] Loss_D: 0.81340, Loss_G: 3.20117, Loss_KL: 0.08824\n",
      "[62/400] [300/349] Loss_D: 0.78446, Loss_G: 3.78748, Loss_KL: 0.10133\n",
      "[62/400] Loss_D: 0.72207, Loss_G: 3.26540, Loss_KL: 0.07676\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[63/400] [0/349] Loss_D: 0.80702, Loss_G: 4.03964, Loss_KL: 0.07625\n",
      "[63/400] [100/349] Loss_D: 0.69810, Loss_G: 3.31029, Loss_KL: 0.06768\n",
      "[63/400] [200/349] Loss_D: 0.78427, Loss_G: 2.43542, Loss_KL: 0.10685\n",
      "[63/400] [300/349] Loss_D: 0.80349, Loss_G: 2.43872, Loss_KL: 0.06816\n",
      "[63/400] Loss_D: 0.71974, Loss_G: 3.22938, Loss_KL: 0.08017\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[64/400] [0/349] Loss_D: 0.74931, Loss_G: 4.85446, Loss_KL: 0.08809\n",
      "[64/400] [100/349] Loss_D: 0.80544, Loss_G: 3.36088, Loss_KL: 0.10190\n",
      "[64/400] [200/349] Loss_D: 1.01951, Loss_G: 6.31612, Loss_KL: 0.06886\n",
      "[64/400] [300/349] Loss_D: 0.44666, Loss_G: 2.19006, Loss_KL: 0.05910\n",
      "[64/400] Loss_D: 0.73635, Loss_G: 3.29158, Loss_KL: 0.07774\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[65/400] [0/349] Loss_D: 0.29734, Loss_G: 4.82794, Loss_KL: 0.07796\n",
      "[65/400] [100/349] Loss_D: 0.74869, Loss_G: 4.88141, Loss_KL: 0.08753\n",
      "[65/400] [200/349] Loss_D: 0.69255, Loss_G: 2.55430, Loss_KL: 0.06146\n",
      "[65/400] [300/349] Loss_D: 0.76145, Loss_G: 2.42541, Loss_KL: 0.07880\n",
      "[65/400] Loss_D: 0.72068, Loss_G: 3.26654, Loss_KL: 0.07559\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[66/400] [0/349] Loss_D: 0.42585, Loss_G: 4.25022, Loss_KL: 0.09136\n",
      "[66/400] [100/349] Loss_D: 0.77655, Loss_G: 3.03182, Loss_KL: 0.06967\n",
      "[66/400] [200/349] Loss_D: 0.73864, Loss_G: 3.33001, Loss_KL: 0.07827\n",
      "[66/400] [300/349] Loss_D: 0.64260, Loss_G: 3.01319, Loss_KL: 0.06002\n",
      "[66/400] Loss_D: 0.72132, Loss_G: 3.30525, Loss_KL: 0.07471\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[67/400] [0/349] Loss_D: 0.47389, Loss_G: 3.54511, Loss_KL: 0.08800\n",
      "[67/400] [100/349] Loss_D: 0.75874, Loss_G: 3.65014, Loss_KL: 0.07311\n",
      "[67/400] [200/349] Loss_D: 0.90872, Loss_G: 1.94924, Loss_KL: 0.08286\n",
      "[67/400] [300/349] Loss_D: 0.39055, Loss_G: 3.64426, Loss_KL: 0.06625\n",
      "[67/400] Loss_D: 0.71808, Loss_G: 3.21776, Loss_KL: 0.07574\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[68/400] [0/349] Loss_D: 0.86795, Loss_G: 2.80008, Loss_KL: 0.08252\n",
      "[68/400] [100/349] Loss_D: 0.80087, Loss_G: 4.26919, Loss_KL: 0.08300\n",
      "[68/400] [200/349] Loss_D: 0.58162, Loss_G: 3.40046, Loss_KL: 0.05338\n",
      "[68/400] [300/349] Loss_D: 1.55137, Loss_G: 3.63877, Loss_KL: 0.06267\n",
      "[68/400] Loss_D: 0.75085, Loss_G: 3.23666, Loss_KL: 0.06978\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[69/400] [0/349] Loss_D: 1.03252, Loss_G: 4.24911, Loss_KL: 0.06396\n",
      "[69/400] [100/349] Loss_D: 0.31205, Loss_G: 4.30766, Loss_KL: 0.06523\n",
      "[69/400] [200/349] Loss_D: 0.86848, Loss_G: 3.07172, Loss_KL: 0.10751\n",
      "[69/400] [300/349] Loss_D: 0.54960, Loss_G: 4.01250, Loss_KL: 0.07969\n",
      "[69/400] Loss_D: 0.71387, Loss_G: 3.23183, Loss_KL: 0.08177\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[70/400] [0/349] Loss_D: 0.82326, Loss_G: 2.80359, Loss_KL: 0.07844\n",
      "[70/400] [100/349] Loss_D: 0.81588, Loss_G: 4.10419, Loss_KL: 0.09176\n",
      "[70/400] [200/349] Loss_D: 0.80956, Loss_G: 2.62852, Loss_KL: 0.09498\n",
      "[70/400] [300/349] Loss_D: 0.75598, Loss_G: 3.47446, Loss_KL: 0.07125\n",
      "[70/400] Loss_D: 0.72605, Loss_G: 3.16530, Loss_KL: 0.08467\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[71/400] [0/349] Loss_D: 0.66354, Loss_G: 3.14740, Loss_KL: 0.07866\n",
      "[71/400] [100/349] Loss_D: 0.83020, Loss_G: 5.33983, Loss_KL: 0.06097\n",
      "[71/400] [200/349] Loss_D: 0.70267, Loss_G: 5.00755, Loss_KL: 0.05057\n",
      "[71/400] [300/349] Loss_D: 0.61273, Loss_G: 2.58208, Loss_KL: 0.07574\n",
      "[71/400] Loss_D: 0.72408, Loss_G: 3.17140, Loss_KL: 0.07400\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[72/400] [0/349] Loss_D: 0.74243, Loss_G: 2.68840, Loss_KL: 0.07517\n",
      "[72/400] [100/349] Loss_D: 0.90586, Loss_G: 2.45181, Loss_KL: 0.07848\n",
      "[72/400] [200/349] Loss_D: 0.78427, Loss_G: 4.20567, Loss_KL: 0.09926\n",
      "[72/400] [300/349] Loss_D: 0.72374, Loss_G: 2.09665, Loss_KL: 0.06461\n",
      "[72/400] Loss_D: 0.70183, Loss_G: 3.26416, Loss_KL: 0.07232\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[73/400] [0/349] Loss_D: 0.75820, Loss_G: 3.78367, Loss_KL: 0.08889\n",
      "[73/400] [100/349] Loss_D: 0.67473, Loss_G: 3.10583, Loss_KL: 0.08004\n",
      "[73/400] [200/349] Loss_D: 0.56328, Loss_G: 1.78548, Loss_KL: 0.07918\n",
      "[73/400] [300/349] Loss_D: 0.58285, Loss_G: 3.73333, Loss_KL: 0.07001\n",
      "[73/400] Loss_D: 0.70308, Loss_G: 3.13693, Loss_KL: 0.06971\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[74/400] [0/349] Loss_D: 1.37286, Loss_G: 6.00277, Loss_KL: 0.08438\n",
      "[74/400] [100/349] Loss_D: 1.59029, Loss_G: 4.14853, Loss_KL: 0.07004\n",
      "[74/400] [200/349] Loss_D: 0.79112, Loss_G: 2.54892, Loss_KL: 0.06396\n",
      "[74/400] [300/349] Loss_D: 0.55189, Loss_G: 3.06689, Loss_KL: 0.05905\n",
      "[74/400] Loss_D: 0.72016, Loss_G: 3.15473, Loss_KL: 0.07061\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[75/400] [0/349] Loss_D: 0.50567, Loss_G: 3.60167, Loss_KL: 0.07122\n",
      "[75/400] [100/349] Loss_D: 0.36840, Loss_G: 2.30133, Loss_KL: 0.05834\n",
      "[75/400] [200/349] Loss_D: 0.80266, Loss_G: 2.71225, Loss_KL: 0.04618\n",
      "[75/400] [300/349] Loss_D: 0.88410, Loss_G: 2.73939, Loss_KL: 0.06728\n",
      "[75/400] Loss_D: 0.68912, Loss_G: 3.18889, Loss_KL: 0.06037\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[76/400] [0/349] Loss_D: 0.52724, Loss_G: 1.72916, Loss_KL: 0.06392\n",
      "[76/400] [100/349] Loss_D: 1.06562, Loss_G: 3.95649, Loss_KL: 0.05908\n",
      "[76/400] [200/349] Loss_D: 1.12615, Loss_G: 4.83230, Loss_KL: 0.05754\n",
      "[76/400] [300/349] Loss_D: 0.90031, Loss_G: 2.25004, Loss_KL: 0.05722\n",
      "[76/400] Loss_D: 0.72625, Loss_G: 3.07915, Loss_KL: 0.06003\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[77/400] [0/349] Loss_D: 0.72643, Loss_G: 4.01436, Loss_KL: 0.04740\n",
      "[77/400] [100/349] Loss_D: 0.79250, Loss_G: 2.81570, Loss_KL: 0.06443\n",
      "[77/400] [200/349] Loss_D: 0.82913, Loss_G: 1.94451, Loss_KL: 0.07419\n",
      "[77/400] [300/349] Loss_D: 0.30324, Loss_G: 3.54764, Loss_KL: 0.05459\n",
      "[77/400] Loss_D: 0.70482, Loss_G: 3.16648, Loss_KL: 0.06688\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[78/400] [0/349] Loss_D: 0.74040, Loss_G: 3.04815, Loss_KL: 0.05006\n",
      "[78/400] [100/349] Loss_D: 0.56133, Loss_G: 3.15913, Loss_KL: 0.06188\n",
      "[78/400] [200/349] Loss_D: 0.38353, Loss_G: 3.00748, Loss_KL: 0.05889\n",
      "[78/400] [300/349] Loss_D: 0.70710, Loss_G: 2.61990, Loss_KL: 0.05632\n",
      "[78/400] Loss_D: 0.68524, Loss_G: 3.22461, Loss_KL: 0.06239\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[79/400] [0/349] Loss_D: 0.73983, Loss_G: 2.80656, Loss_KL: 0.06952\n",
      "[79/400] [100/349] Loss_D: 0.52649, Loss_G: 2.35400, Loss_KL: 0.06567\n",
      "[79/400] [200/349] Loss_D: 0.69769, Loss_G: 2.44824, Loss_KL: 0.03406\n",
      "[79/400] [300/349] Loss_D: 0.79449, Loss_G: 2.47818, Loss_KL: 0.05038\n",
      "[79/400] Loss_D: 0.64856, Loss_G: 3.04004, Loss_KL: 0.05982\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[80/400] [0/349] Loss_D: 0.57718, Loss_G: 2.68232, Loss_KL: 0.07361\n",
      "[80/400] [100/349] Loss_D: 0.91843, Loss_G: 5.29535, Loss_KL: 0.09354\n",
      "[80/400] [200/349] Loss_D: 0.48281, Loss_G: 4.44271, Loss_KL: 0.09185\n",
      "[80/400] [300/349] Loss_D: 0.41816, Loss_G: 2.19436, Loss_KL: 0.06793\n",
      "[80/400] Loss_D: 0.71482, Loss_G: 3.34815, Loss_KL: 0.08426\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[81/400] [0/349] Loss_D: 0.69512, Loss_G: 2.84462, Loss_KL: 0.06424\n",
      "[81/400] [100/349] Loss_D: 0.69320, Loss_G: 2.45650, Loss_KL: 0.09361\n",
      "[81/400] [200/349] Loss_D: 0.95968, Loss_G: 3.08480, Loss_KL: 0.05722\n",
      "[81/400] [300/349] Loss_D: 0.75455, Loss_G: 2.67142, Loss_KL: 0.08576\n",
      "[81/400] Loss_D: 0.70127, Loss_G: 3.13514, Loss_KL: 0.06741\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[82/400] [0/349] Loss_D: 0.45226, Loss_G: 2.67524, Loss_KL: 0.07935\n",
      "[82/400] [100/349] Loss_D: 0.75318, Loss_G: 3.38955, Loss_KL: 0.06426\n",
      "[82/400] [200/349] Loss_D: 0.90852, Loss_G: 2.50757, Loss_KL: 0.08311\n",
      "[82/400] [300/349] Loss_D: 0.51932, Loss_G: 2.11109, Loss_KL: 0.06707\n",
      "[82/400] Loss_D: 0.71765, Loss_G: 3.07999, Loss_KL: 0.07246\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[83/400] [0/349] Loss_D: 0.54444, Loss_G: 3.00554, Loss_KL: 0.07549\n",
      "[83/400] [100/349] Loss_D: 0.79891, Loss_G: 2.51332, Loss_KL: 0.06418\n",
      "[83/400] [200/349] Loss_D: 0.71521, Loss_G: 1.83068, Loss_KL: 0.04066\n",
      "[83/400] [300/349] Loss_D: 0.51405, Loss_G: 1.68404, Loss_KL: 0.05544\n",
      "[83/400] Loss_D: 0.71448, Loss_G: 3.11036, Loss_KL: 0.06378\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[84/400] [0/349] Loss_D: 0.51415, Loss_G: 2.92521, Loss_KL: 0.08072\n",
      "[84/400] [100/349] Loss_D: 0.88186, Loss_G: 3.47660, Loss_KL: 0.07462\n",
      "[84/400] [200/349] Loss_D: 1.00162, Loss_G: 1.09916, Loss_KL: 0.06594\n",
      "[84/400] [300/349] Loss_D: 0.87506, Loss_G: 3.80845, Loss_KL: 0.05486\n",
      "[84/400] Loss_D: 0.69910, Loss_G: 3.11155, Loss_KL: 0.06871\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[85/400] [0/349] Loss_D: 0.86572, Loss_G: 1.00461, Loss_KL: 0.06466\n",
      "[85/400] [100/349] Loss_D: 0.60104, Loss_G: 2.07139, Loss_KL: 0.04694\n",
      "[85/400] [200/349] Loss_D: 0.57695, Loss_G: 1.51589, Loss_KL: 0.08129\n",
      "[85/400] [300/349] Loss_D: 0.75397, Loss_G: 3.38738, Loss_KL: 0.07370\n",
      "[85/400] Loss_D: 0.69210, Loss_G: 3.18364, Loss_KL: 0.06425\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[86/400] [0/349] Loss_D: 0.65538, Loss_G: 2.84615, Loss_KL: 0.06577\n",
      "[86/400] [100/349] Loss_D: 0.77226, Loss_G: 2.52011, Loss_KL: 0.08344\n",
      "[86/400] [200/349] Loss_D: 0.95401, Loss_G: 4.91740, Loss_KL: 0.06918\n",
      "[86/400] [300/349] Loss_D: 0.82307, Loss_G: 3.73376, Loss_KL: 0.08294\n",
      "[86/400] Loss_D: 0.67813, Loss_G: 3.08174, Loss_KL: 0.06748\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[87/400] [0/349] Loss_D: 0.85615, Loss_G: 4.73865, Loss_KL: 0.06236\n",
      "[87/400] [100/349] Loss_D: 0.47737, Loss_G: 3.67097, Loss_KL: 0.07129\n",
      "[87/400] [200/349] Loss_D: 0.82625, Loss_G: 2.89454, Loss_KL: 0.08320\n",
      "[87/400] [300/349] Loss_D: 0.81334, Loss_G: 3.89928, Loss_KL: 0.08061\n",
      "[87/400] Loss_D: 0.66608, Loss_G: 3.20927, Loss_KL: 0.06897\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[88/400] [0/349] Loss_D: 0.35251, Loss_G: 4.61468, Loss_KL: 0.06752\n",
      "[88/400] [100/349] Loss_D: 0.40552, Loss_G: 3.48696, Loss_KL: 0.08966\n",
      "[88/400] [200/349] Loss_D: 0.70702, Loss_G: 3.05412, Loss_KL: 0.07295\n",
      "[88/400] [300/349] Loss_D: 0.74594, Loss_G: 3.45741, Loss_KL: 0.08628\n",
      "[88/400] Loss_D: 0.72337, Loss_G: 3.10205, Loss_KL: 0.07925\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[89/400] [0/349] Loss_D: 0.25753, Loss_G: 3.70130, Loss_KL: 0.08115\n",
      "[89/400] [100/349] Loss_D: 0.78451, Loss_G: 2.26021, Loss_KL: 0.05642\n",
      "[89/400] [200/349] Loss_D: 0.69333, Loss_G: 3.18767, Loss_KL: 0.07761\n",
      "[89/400] [300/349] Loss_D: 0.77363, Loss_G: 3.34035, Loss_KL: 0.05676\n",
      "[89/400] Loss_D: 0.69168, Loss_G: 3.13265, Loss_KL: 0.06609\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[90/400] [0/349] Loss_D: 1.06644, Loss_G: 3.46293, Loss_KL: 0.07544\n",
      "[90/400] [100/349] Loss_D: 0.86308, Loss_G: 2.37094, Loss_KL: 0.08051\n",
      "[90/400] [200/349] Loss_D: 0.70895, Loss_G: 3.02365, Loss_KL: 0.08027\n",
      "[90/400] [300/349] Loss_D: 0.60028, Loss_G: 2.10049, Loss_KL: 0.06141\n",
      "[90/400] Loss_D: 0.68996, Loss_G: 3.15982, Loss_KL: 0.07343\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[91/400] [0/349] Loss_D: 0.58144, Loss_G: 3.89878, Loss_KL: 0.08682\n",
      "[91/400] [100/349] Loss_D: 0.76101, Loss_G: 2.82206, Loss_KL: 0.06640\n",
      "[91/400] [200/349] Loss_D: 0.81802, Loss_G: 3.39797, Loss_KL: 0.07340\n",
      "[91/400] [300/349] Loss_D: 0.79328, Loss_G: 2.65435, Loss_KL: 0.05206\n",
      "[91/400] Loss_D: 0.67127, Loss_G: 3.11973, Loss_KL: 0.06988\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[92/400] [0/349] Loss_D: 0.60187, Loss_G: 3.99254, Loss_KL: 0.06370\n",
      "[92/400] [100/349] Loss_D: 0.40875, Loss_G: 3.56721, Loss_KL: 0.07393\n",
      "[92/400] [200/349] Loss_D: 0.67447, Loss_G: 2.62839, Loss_KL: 0.05899\n",
      "[92/400] [300/349] Loss_D: 0.60322, Loss_G: 4.21901, Loss_KL: 0.05959\n",
      "[92/400] Loss_D: 0.68032, Loss_G: 3.12358, Loss_KL: 0.06709\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[93/400] [0/349] Loss_D: 0.78870, Loss_G: 2.22318, Loss_KL: 0.06579\n",
      "[93/400] [100/349] Loss_D: 0.54168, Loss_G: 3.39747, Loss_KL: 0.04713\n",
      "[93/400] [200/349] Loss_D: 0.76551, Loss_G: 2.30211, Loss_KL: 0.06251\n",
      "[93/400] [300/349] Loss_D: 0.61312, Loss_G: 2.41477, Loss_KL: 0.05271\n",
      "[93/400] Loss_D: 0.65798, Loss_G: 3.10204, Loss_KL: 0.05676\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[94/400] [0/349] Loss_D: 0.55530, Loss_G: 4.34191, Loss_KL: 0.06179\n",
      "[94/400] [100/349] Loss_D: 0.80037, Loss_G: 3.49634, Loss_KL: 0.06005\n",
      "[94/400] [200/349] Loss_D: 0.83018, Loss_G: 2.90696, Loss_KL: 0.06485\n",
      "[94/400] [300/349] Loss_D: 0.46202, Loss_G: 3.91891, Loss_KL: 0.05780\n",
      "[94/400] Loss_D: 0.66017, Loss_G: 3.08528, Loss_KL: 0.06500\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[95/400] [0/349] Loss_D: 0.53188, Loss_G: 3.86781, Loss_KL: 0.07235\n",
      "[95/400] [100/349] Loss_D: 0.69062, Loss_G: 3.69396, Loss_KL: 0.06230\n",
      "[95/400] [200/349] Loss_D: 0.74099, Loss_G: 1.91104, Loss_KL: 0.06873\n",
      "[95/400] [300/349] Loss_D: 0.28099, Loss_G: 3.74561, Loss_KL: 0.06582\n",
      "[95/400] Loss_D: 0.68670, Loss_G: 3.06449, Loss_KL: 0.07130\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[96/400] [0/349] Loss_D: 1.00153, Loss_G: 3.50669, Loss_KL: 0.05994\n",
      "[96/400] [100/349] Loss_D: 1.18379, Loss_G: 2.73702, Loss_KL: 0.06177\n",
      "[96/400] [200/349] Loss_D: 0.50297, Loss_G: 4.11447, Loss_KL: 0.06368\n",
      "[96/400] [300/349] Loss_D: 0.86968, Loss_G: 2.78268, Loss_KL: 0.07046\n",
      "[96/400] Loss_D: 0.70933, Loss_G: 3.02774, Loss_KL: 0.06437\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[97/400] [0/349] Loss_D: 1.00380, Loss_G: 2.55247, Loss_KL: 0.08251\n",
      "[97/400] [100/349] Loss_D: 0.85811, Loss_G: 5.04359, Loss_KL: 0.05686\n",
      "[97/400] [200/349] Loss_D: 1.16758, Loss_G: 4.24787, Loss_KL: 0.07839\n",
      "[97/400] [300/349] Loss_D: 0.61321, Loss_G: 2.47309, Loss_KL: 0.06892\n",
      "[97/400] Loss_D: 0.67566, Loss_G: 3.13074, Loss_KL: 0.06796\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[98/400] [0/349] Loss_D: 0.73635, Loss_G: 2.46075, Loss_KL: 0.05452\n",
      "[98/400] [100/349] Loss_D: 0.80178, Loss_G: 3.16635, Loss_KL: 0.06427\n",
      "[98/400] [200/349] Loss_D: 0.76828, Loss_G: 3.08856, Loss_KL: 0.08232\n",
      "[98/400] [300/349] Loss_D: 0.71677, Loss_G: 2.62891, Loss_KL: 0.05537\n",
      "[98/400] Loss_D: 0.69613, Loss_G: 2.99522, Loss_KL: 0.06149\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[99/400] [0/349] Loss_D: 0.55965, Loss_G: 3.62974, Loss_KL: 0.06548\n",
      "[99/400] [100/349] Loss_D: 0.53821, Loss_G: 3.24975, Loss_KL: 0.06620\n",
      "[99/400] [200/349] Loss_D: 0.78245, Loss_G: 4.95912, Loss_KL: 0.07612\n",
      "[99/400] [300/349] Loss_D: 0.79393, Loss_G: 3.64391, Loss_KL: 0.06901\n",
      "[99/400] Loss_D: 0.67476, Loss_G: 3.18471, Loss_KL: 0.07342\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[100/400] [0/349] Loss_D: 0.45772, Loss_G: 3.71350, Loss_KL: 0.07145\n",
      "[100/400] [100/349] Loss_D: 0.81909, Loss_G: 2.63444, Loss_KL: 0.05396\n",
      "[100/400] [200/349] Loss_D: 0.71395, Loss_G: 2.73840, Loss_KL: 0.05069\n",
      "[100/400] [300/349] Loss_D: 0.78840, Loss_G: 2.35195, Loss_KL: 0.05328\n",
      "[100/400] Loss_D: 0.67155, Loss_G: 3.11050, Loss_KL: 0.06537\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[101/400] [0/349] Loss_D: 0.62671, Loss_G: 2.68010, Loss_KL: 0.04779\n",
      "[101/400] [100/349] Loss_D: 0.54556, Loss_G: 3.16181, Loss_KL: 0.05732\n",
      "[101/400] [200/349] Loss_D: 0.55121, Loss_G: 3.34755, Loss_KL: 0.08398\n",
      "[101/400] [300/349] Loss_D: 0.71622, Loss_G: 2.93924, Loss_KL: 0.06474\n",
      "[101/400] Loss_D: 0.65459, Loss_G: 2.80617, Loss_KL: 0.06326\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[102/400] [0/349] Loss_D: 0.66815, Loss_G: 3.68359, Loss_KL: 0.05095\n",
      "[102/400] [100/349] Loss_D: 0.60616, Loss_G: 2.75662, Loss_KL: 0.05318\n",
      "[102/400] [200/349] Loss_D: 0.58155, Loss_G: 3.08026, Loss_KL: 0.07436\n",
      "[102/400] [300/349] Loss_D: 0.62905, Loss_G: 2.61311, Loss_KL: 0.04869\n",
      "[102/400] Loss_D: 0.65819, Loss_G: 2.79568, Loss_KL: 0.06285\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[103/400] [0/349] Loss_D: 0.82661, Loss_G: 2.51840, Loss_KL: 0.06644\n",
      "[103/400] [100/349] Loss_D: 0.62330, Loss_G: 3.24315, Loss_KL: 0.05803\n",
      "[103/400] [200/349] Loss_D: 0.76919, Loss_G: 2.83408, Loss_KL: 0.04196\n",
      "[103/400] [300/349] Loss_D: 0.45169, Loss_G: 2.74181, Loss_KL: 0.07131\n",
      "[103/400] Loss_D: 0.63896, Loss_G: 2.83585, Loss_KL: 0.06582\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[104/400] [0/349] Loss_D: 0.69541, Loss_G: 3.19577, Loss_KL: 0.06379\n",
      "[104/400] [100/349] Loss_D: 0.66258, Loss_G: 3.15449, Loss_KL: 0.06516\n",
      "[104/400] [200/349] Loss_D: 1.15825, Loss_G: 2.34834, Loss_KL: 0.06429\n",
      "[104/400] [300/349] Loss_D: 0.44673, Loss_G: 2.90922, Loss_KL: 0.06287\n",
      "[104/400] Loss_D: 0.65437, Loss_G: 2.80555, Loss_KL: 0.07048\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[105/400] [0/349] Loss_D: 0.35731, Loss_G: 4.60911, Loss_KL: 0.06129\n",
      "[105/400] [100/349] Loss_D: 0.53874, Loss_G: 3.05720, Loss_KL: 0.06128\n",
      "[105/400] [200/349] Loss_D: 0.76862, Loss_G: 3.45052, Loss_KL: 0.05785\n",
      "[105/400] [300/349] Loss_D: 0.59713, Loss_G: 2.59584, Loss_KL: 0.07282\n",
      "[105/400] Loss_D: 0.62092, Loss_G: 2.89666, Loss_KL: 0.06631\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[106/400] [0/349] Loss_D: 0.61960, Loss_G: 3.72619, Loss_KL: 0.07075\n",
      "[106/400] [100/349] Loss_D: 0.63917, Loss_G: 2.66875, Loss_KL: 0.06333\n",
      "[106/400] [200/349] Loss_D: 0.80336, Loss_G: 2.86852, Loss_KL: 0.08209\n",
      "[106/400] [300/349] Loss_D: 0.71512, Loss_G: 3.03593, Loss_KL: 0.09136\n",
      "[106/400] Loss_D: 0.63852, Loss_G: 2.94015, Loss_KL: 0.07550\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[107/400] [0/349] Loss_D: 0.44746, Loss_G: 3.16458, Loss_KL: 0.09996\n",
      "[107/400] [100/349] Loss_D: 0.76324, Loss_G: 2.76975, Loss_KL: 0.06229\n",
      "[107/400] [200/349] Loss_D: 0.77659, Loss_G: 2.37612, Loss_KL: 0.09394\n",
      "[107/400] [300/349] Loss_D: 0.57177, Loss_G: 2.01884, Loss_KL: 0.08814\n",
      "[107/400] Loss_D: 0.63611, Loss_G: 2.87431, Loss_KL: 0.08070\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[108/400] [0/349] Loss_D: 0.56342, Loss_G: 2.77401, Loss_KL: 0.07285\n",
      "[108/400] [100/349] Loss_D: 0.35790, Loss_G: 3.00953, Loss_KL: 0.09664\n",
      "[108/400] [200/349] Loss_D: 0.30719, Loss_G: 2.57308, Loss_KL: 0.07321\n",
      "[108/400] [300/349] Loss_D: 0.36238, Loss_G: 2.18821, Loss_KL: 0.08297\n",
      "[108/400] Loss_D: 0.64651, Loss_G: 2.85142, Loss_KL: 0.07690\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[109/400] [0/349] Loss_D: 0.55938, Loss_G: 2.67378, Loss_KL: 0.08746\n",
      "[109/400] [100/349] Loss_D: 0.54320, Loss_G: 2.20558, Loss_KL: 0.06360\n",
      "[109/400] [200/349] Loss_D: 0.79748, Loss_G: 3.53318, Loss_KL: 0.06179\n",
      "[109/400] [300/349] Loss_D: 0.66325, Loss_G: 1.99889, Loss_KL: 0.05587\n",
      "[109/400] Loss_D: 0.64773, Loss_G: 2.76150, Loss_KL: 0.07252\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[110/400] [0/349] Loss_D: 0.58274, Loss_G: 2.42343, Loss_KL: 0.08184\n",
      "[110/400] [100/349] Loss_D: 0.98426, Loss_G: 4.27444, Loss_KL: 0.06599\n",
      "[110/400] [200/349] Loss_D: 0.79876, Loss_G: 1.98631, Loss_KL: 0.07637\n",
      "[110/400] [300/349] Loss_D: 0.46470, Loss_G: 2.71903, Loss_KL: 0.07243\n",
      "[110/400] Loss_D: 0.64266, Loss_G: 2.90109, Loss_KL: 0.06320\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[111/400] [0/349] Loss_D: 0.56278, Loss_G: 2.65084, Loss_KL: 0.07213\n",
      "[111/400] [100/349] Loss_D: 0.39363, Loss_G: 2.72515, Loss_KL: 0.06771\n",
      "[111/400] [200/349] Loss_D: 0.32126, Loss_G: 3.08050, Loss_KL: 0.07425\n",
      "[111/400] [300/349] Loss_D: 0.30000, Loss_G: 2.64708, Loss_KL: 0.07954\n",
      "[111/400] Loss_D: 0.63162, Loss_G: 2.84568, Loss_KL: 0.06869\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[112/400] [0/349] Loss_D: 0.70386, Loss_G: 3.03306, Loss_KL: 0.08173\n",
      "[112/400] [100/349] Loss_D: 0.39097, Loss_G: 3.56253, Loss_KL: 0.05573\n",
      "[112/400] [200/349] Loss_D: 0.27891, Loss_G: 3.26363, Loss_KL: 0.07189\n",
      "[112/400] [300/349] Loss_D: 0.31643, Loss_G: 3.40623, Loss_KL: 0.05890\n",
      "[112/400] Loss_D: 0.61741, Loss_G: 2.94825, Loss_KL: 0.07233\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[113/400] [0/349] Loss_D: 0.83473, Loss_G: 1.57146, Loss_KL: 0.06809\n",
      "[113/400] [100/349] Loss_D: 0.60757, Loss_G: 2.99794, Loss_KL: 0.06658\n",
      "[113/400] [200/349] Loss_D: 0.67613, Loss_G: 2.83117, Loss_KL: 0.07055\n",
      "[113/400] [300/349] Loss_D: 0.72027, Loss_G: 2.48922, Loss_KL: 0.06432\n",
      "[113/400] Loss_D: 0.59599, Loss_G: 2.89076, Loss_KL: 0.07291\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[114/400] [0/349] Loss_D: 0.70799, Loss_G: 2.60090, Loss_KL: 0.06935\n",
      "[114/400] [100/349] Loss_D: 0.35298, Loss_G: 2.62794, Loss_KL: 0.07993\n",
      "[114/400] [200/349] Loss_D: 0.64555, Loss_G: 2.96329, Loss_KL: 0.06161\n",
      "[114/400] [300/349] Loss_D: 0.71820, Loss_G: 2.20880, Loss_KL: 0.04636\n",
      "[114/400] Loss_D: 0.59807, Loss_G: 2.54015, Loss_KL: 0.05947\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[115/400] [0/349] Loss_D: 0.16427, Loss_G: 2.23608, Loss_KL: 0.05018\n",
      "[115/400] [100/349] Loss_D: 0.44677, Loss_G: 2.91649, Loss_KL: 0.04320\n",
      "[115/400] [200/349] Loss_D: 0.32077, Loss_G: 3.05004, Loss_KL: 0.03841\n",
      "[115/400] [300/349] Loss_D: 0.61637, Loss_G: 2.46119, Loss_KL: 0.03291\n",
      "[115/400] Loss_D: 0.57377, Loss_G: 2.66601, Loss_KL: 0.04593\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[116/400] [0/349] Loss_D: 0.36142, Loss_G: 2.64973, Loss_KL: 0.05522\n",
      "[116/400] [100/349] Loss_D: 0.53073, Loss_G: 2.58606, Loss_KL: 0.04521\n",
      "[116/400] [200/349] Loss_D: 0.62726, Loss_G: 3.06785, Loss_KL: 0.04327\n",
      "[116/400] [300/349] Loss_D: 0.55293, Loss_G: 2.55648, Loss_KL: 0.05515\n",
      "[116/400] Loss_D: 0.57858, Loss_G: 2.74434, Loss_KL: 0.04002\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[117/400] [0/349] Loss_D: 0.40200, Loss_G: 2.68710, Loss_KL: 0.03884\n",
      "[117/400] [100/349] Loss_D: 0.37516, Loss_G: 2.82838, Loss_KL: 0.03334\n",
      "[117/400] [200/349] Loss_D: 0.74803, Loss_G: 3.65089, Loss_KL: 0.03734\n",
      "[117/400] [300/349] Loss_D: 0.47851, Loss_G: 3.09502, Loss_KL: 0.06512\n",
      "[117/400] Loss_D: 0.57129, Loss_G: 2.68880, Loss_KL: 0.03633\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[118/400] [0/349] Loss_D: 0.55530, Loss_G: 2.42288, Loss_KL: 0.03206\n",
      "[118/400] [100/349] Loss_D: 0.75167, Loss_G: 2.85959, Loss_KL: 0.02149\n",
      "[118/400] [200/349] Loss_D: 0.63629, Loss_G: 2.59022, Loss_KL: 0.02011\n",
      "[118/400] [300/349] Loss_D: 0.61218, Loss_G: 2.76232, Loss_KL: 0.03264\n",
      "[118/400] Loss_D: 0.58986, Loss_G: 2.55839, Loss_KL: 0.02950\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[119/400] [0/349] Loss_D: 0.72791, Loss_G: 2.55272, Loss_KL: 0.02333\n",
      "[119/400] [100/349] Loss_D: 0.50655, Loss_G: 2.25385, Loss_KL: 0.02013\n",
      "[119/400] [200/349] Loss_D: 0.73695, Loss_G: 2.64412, Loss_KL: 0.01804\n",
      "[119/400] [300/349] Loss_D: 0.30112, Loss_G: 2.79211, Loss_KL: 0.03596\n",
      "[119/400] Loss_D: 0.56428, Loss_G: 2.61253, Loss_KL: 0.02710\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[120/400] [0/349] Loss_D: 0.32071, Loss_G: 2.66705, Loss_KL: 0.02063\n",
      "[120/400] [100/349] Loss_D: 0.66944, Loss_G: 2.98761, Loss_KL: 0.02801\n",
      "[120/400] [200/349] Loss_D: 0.85008, Loss_G: 2.43402, Loss_KL: 0.02440\n",
      "[120/400] [300/349] Loss_D: 0.70042, Loss_G: 2.32342, Loss_KL: 0.01589\n",
      "[120/400] Loss_D: 0.57024, Loss_G: 2.67514, Loss_KL: 0.02531\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[121/400] [0/349] Loss_D: 0.77544, Loss_G: 2.60153, Loss_KL: 0.03383\n",
      "[121/400] [100/349] Loss_D: 0.79428, Loss_G: 2.56624, Loss_KL: 0.01901\n",
      "[121/400] [200/349] Loss_D: 0.52261, Loss_G: 2.99985, Loss_KL: 0.02459\n",
      "[121/400] [300/349] Loss_D: 0.56767, Loss_G: 2.34391, Loss_KL: 0.03941\n",
      "[121/400] Loss_D: 0.58275, Loss_G: 2.69113, Loss_KL: 0.02869\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[122/400] [0/349] Loss_D: 0.40721, Loss_G: 2.89917, Loss_KL: 0.04429\n",
      "[122/400] [100/349] Loss_D: 0.64202, Loss_G: 2.47198, Loss_KL: 0.01895\n",
      "[122/400] [200/349] Loss_D: 0.22632, Loss_G: 2.71948, Loss_KL: 0.04555\n",
      "[122/400] [300/349] Loss_D: 0.66932, Loss_G: 2.40973, Loss_KL: 0.02404\n",
      "[122/400] Loss_D: 0.58362, Loss_G: 2.66713, Loss_KL: 0.02915\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[123/400] [0/349] Loss_D: 0.72136, Loss_G: 2.17649, Loss_KL: 0.02505\n",
      "[123/400] [100/349] Loss_D: 0.56805, Loss_G: 2.47698, Loss_KL: 0.03475\n",
      "[123/400] [200/349] Loss_D: 0.61021, Loss_G: 2.59320, Loss_KL: 0.02911\n",
      "[123/400] [300/349] Loss_D: 0.36892, Loss_G: 3.06184, Loss_KL: 0.04438\n",
      "[123/400] Loss_D: 0.57610, Loss_G: 2.64524, Loss_KL: 0.03040\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[124/400] [0/349] Loss_D: 0.73639, Loss_G: 2.32244, Loss_KL: 0.02057\n",
      "[124/400] [100/349] Loss_D: 0.71574, Loss_G: 2.52841, Loss_KL: 0.01579\n",
      "[124/400] [200/349] Loss_D: 0.66800, Loss_G: 2.78511, Loss_KL: 0.01630\n",
      "[124/400] [300/349] Loss_D: 0.57316, Loss_G: 2.64639, Loss_KL: 0.02623\n",
      "[124/400] Loss_D: 0.56687, Loss_G: 2.71266, Loss_KL: 0.02738\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[125/400] [0/349] Loss_D: 0.58413, Loss_G: 2.33907, Loss_KL: 0.02674\n",
      "[125/400] [100/349] Loss_D: 0.63709, Loss_G: 2.46766, Loss_KL: 0.01505\n",
      "[125/400] [200/349] Loss_D: 0.87456, Loss_G: 2.51941, Loss_KL: 0.03900\n",
      "[125/400] [300/349] Loss_D: 0.72013, Loss_G: 2.87647, Loss_KL: 0.03023\n",
      "[125/400] Loss_D: 0.56578, Loss_G: 2.68094, Loss_KL: 0.02683\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[126/400] [0/349] Loss_D: 0.68298, Loss_G: 2.93712, Loss_KL: 0.04496\n",
      "[126/400] [100/349] Loss_D: 0.60437, Loss_G: 3.57397, Loss_KL: 0.02744\n",
      "[126/400] [200/349] Loss_D: 0.31758, Loss_G: 2.44539, Loss_KL: 0.03292\n",
      "[126/400] [300/349] Loss_D: 0.87826, Loss_G: 2.51296, Loss_KL: 0.02061\n",
      "[126/400] Loss_D: 0.55506, Loss_G: 2.73282, Loss_KL: 0.02768\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[127/400] [0/349] Loss_D: 0.45047, Loss_G: 2.74120, Loss_KL: 0.03086\n",
      "[127/400] [100/349] Loss_D: 0.27482, Loss_G: 3.10185, Loss_KL: 0.03432\n",
      "[127/400] [200/349] Loss_D: 0.42455, Loss_G: 2.62013, Loss_KL: 0.03521\n",
      "[127/400] [300/349] Loss_D: 0.74811, Loss_G: 2.53413, Loss_KL: 0.02412\n",
      "[127/400] Loss_D: 0.56702, Loss_G: 2.67955, Loss_KL: 0.02780\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[128/400] [0/349] Loss_D: 0.42558, Loss_G: 2.54289, Loss_KL: 0.02380\n",
      "[128/400] [100/349] Loss_D: 0.43142, Loss_G: 2.90049, Loss_KL: 0.01647\n",
      "[128/400] [200/349] Loss_D: 0.63991, Loss_G: 2.45325, Loss_KL: 0.02509\n",
      "[128/400] [300/349] Loss_D: 0.77336, Loss_G: 2.34715, Loss_KL: 0.02699\n",
      "[128/400] Loss_D: 0.56277, Loss_G: 2.67272, Loss_KL: 0.02546\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[129/400] [0/349] Loss_D: 0.44370, Loss_G: 2.59078, Loss_KL: 0.02173\n",
      "[129/400] [100/349] Loss_D: 0.59677, Loss_G: 2.45826, Loss_KL: 0.02077\n",
      "[129/400] [200/349] Loss_D: 0.40348, Loss_G: 3.12150, Loss_KL: 0.01678\n",
      "[129/400] [300/349] Loss_D: 0.54366, Loss_G: 2.63458, Loss_KL: 0.01024\n",
      "[129/400] Loss_D: 0.55673, Loss_G: 2.68509, Loss_KL: 0.02090\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[130/400] [0/349] Loss_D: 0.29597, Loss_G: 2.79295, Loss_KL: 0.02230\n",
      "[130/400] [100/349] Loss_D: 0.54385, Loss_G: 2.66277, Loss_KL: 0.02407\n",
      "[130/400] [200/349] Loss_D: 0.71208, Loss_G: 2.26157, Loss_KL: 0.02871\n",
      "[130/400] [300/349] Loss_D: 0.73577, Loss_G: 2.46109, Loss_KL: 0.01709\n",
      "[130/400] Loss_D: 0.58843, Loss_G: 2.56593, Loss_KL: 0.01851\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[131/400] [0/349] Loss_D: 0.40500, Loss_G: 2.50629, Loss_KL: 0.03331\n",
      "[131/400] [100/349] Loss_D: 0.57073, Loss_G: 2.61887, Loss_KL: 0.02396\n",
      "[131/400] [200/349] Loss_D: 0.75892, Loss_G: 2.47154, Loss_KL: 0.01425\n",
      "[131/400] [300/349] Loss_D: 0.39920, Loss_G: 2.40285, Loss_KL: 0.01609\n",
      "[131/400] Loss_D: 0.57817, Loss_G: 2.60232, Loss_KL: 0.01621\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[132/400] [0/349] Loss_D: 0.55232, Loss_G: 2.50381, Loss_KL: 0.00588\n",
      "[132/400] [100/349] Loss_D: 0.57845, Loss_G: 2.46479, Loss_KL: 0.01662\n",
      "[132/400] [200/349] Loss_D: 0.73412, Loss_G: 2.87824, Loss_KL: 0.01477\n",
      "[132/400] [300/349] Loss_D: 0.73995, Loss_G: 2.58285, Loss_KL: 0.01353\n",
      "[132/400] Loss_D: 0.56505, Loss_G: 2.64372, Loss_KL: 0.01498\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[133/400] [0/349] Loss_D: 0.55333, Loss_G: 2.56019, Loss_KL: 0.01956\n",
      "[133/400] [100/349] Loss_D: 0.26888, Loss_G: 2.50856, Loss_KL: 0.01436\n",
      "[133/400] [200/349] Loss_D: 0.32491, Loss_G: 2.53520, Loss_KL: 0.01791\n",
      "[133/400] [300/349] Loss_D: 0.68602, Loss_G: 2.53346, Loss_KL: 0.01550\n",
      "[133/400] Loss_D: 0.57449, Loss_G: 2.59477, Loss_KL: 0.01453\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[134/400] [0/349] Loss_D: 0.61119, Loss_G: 2.92984, Loss_KL: 0.00986\n",
      "[134/400] [100/349] Loss_D: 0.69204, Loss_G: 2.43174, Loss_KL: 0.01301\n",
      "[134/400] [200/349] Loss_D: 0.65656, Loss_G: 2.65334, Loss_KL: 0.02703\n",
      "[134/400] [300/349] Loss_D: 0.67275, Loss_G: 2.57991, Loss_KL: 0.00961\n",
      "[134/400] Loss_D: 0.57013, Loss_G: 2.62917, Loss_KL: 0.01351\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[135/400] [0/349] Loss_D: 0.66077, Loss_G: 2.60716, Loss_KL: 0.01427\n",
      "[135/400] [100/349] Loss_D: 0.65955, Loss_G: 4.46337, Loss_KL: 0.02582\n",
      "[135/400] [200/349] Loss_D: 0.61876, Loss_G: 2.91402, Loss_KL: 0.02157\n",
      "[135/400] [300/349] Loss_D: 0.70721, Loss_G: 3.19384, Loss_KL: 0.06028\n",
      "[135/400] Loss_D: 0.61143, Loss_G: 3.04414, Loss_KL: 0.02729\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[136/400] [0/349] Loss_D: 0.63914, Loss_G: 3.55282, Loss_KL: 0.06467\n",
      "[136/400] [100/349] Loss_D: 0.41334, Loss_G: 3.77967, Loss_KL: 0.04956\n",
      "[136/400] [200/349] Loss_D: 0.31213, Loss_G: 2.98088, Loss_KL: 0.05183\n",
      "[136/400] [300/349] Loss_D: 0.82665, Loss_G: 2.67727, Loss_KL: 0.05177\n",
      "[136/400] Loss_D: 0.60996, Loss_G: 2.93139, Loss_KL: 0.05501\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[137/400] [0/349] Loss_D: 0.58095, Loss_G: 4.38284, Loss_KL: 0.07905\n",
      "[137/400] [100/349] Loss_D: 0.64922, Loss_G: 3.33864, Loss_KL: 0.06375\n",
      "[137/400] [200/349] Loss_D: 1.39501, Loss_G: 1.15604, Loss_KL: 0.06314\n",
      "[137/400] [300/349] Loss_D: 0.48920, Loss_G: 2.38184, Loss_KL: 0.04891\n",
      "[137/400] Loss_D: 0.61200, Loss_G: 2.92057, Loss_KL: 0.06261\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[138/400] [0/349] Loss_D: 0.85208, Loss_G: 2.13224, Loss_KL: 0.06462\n",
      "[138/400] [100/349] Loss_D: 0.64794, Loss_G: 3.81191, Loss_KL: 0.05915\n",
      "[138/400] [200/349] Loss_D: 0.64199, Loss_G: 3.26802, Loss_KL: 0.06409\n",
      "[138/400] [300/349] Loss_D: 0.70461, Loss_G: 2.59983, Loss_KL: 0.05199\n",
      "[138/400] Loss_D: 0.62869, Loss_G: 2.93125, Loss_KL: 0.05950\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[139/400] [0/349] Loss_D: 0.59742, Loss_G: 1.43678, Loss_KL: 0.06387\n",
      "[139/400] [100/349] Loss_D: 0.47289, Loss_G: 2.65793, Loss_KL: 0.06032\n",
      "[139/400] [200/349] Loss_D: 0.74252, Loss_G: 2.94044, Loss_KL: 0.05661\n",
      "[139/400] [300/349] Loss_D: 0.68891, Loss_G: 2.39010, Loss_KL: 0.05787\n",
      "[139/400] Loss_D: 0.61053, Loss_G: 2.69590, Loss_KL: 0.06253\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[140/400] [0/349] Loss_D: 0.71068, Loss_G: 3.05494, Loss_KL: 0.05859\n",
      "[140/400] [100/349] Loss_D: 0.80340, Loss_G: 2.72085, Loss_KL: 0.04913\n",
      "[140/400] [200/349] Loss_D: 0.60621, Loss_G: 3.84152, Loss_KL: 0.07179\n",
      "[140/400] [300/349] Loss_D: 0.30710, Loss_G: 2.98988, Loss_KL: 0.04822\n",
      "[140/400] Loss_D: 0.61323, Loss_G: 2.94657, Loss_KL: 0.05882\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[141/400] [0/349] Loss_D: 0.50441, Loss_G: 2.77172, Loss_KL: 0.06592\n",
      "[141/400] [100/349] Loss_D: 0.59212, Loss_G: 2.31909, Loss_KL: 0.06295\n",
      "[141/400] [200/349] Loss_D: 0.62923, Loss_G: 2.93041, Loss_KL: 0.08509\n",
      "[141/400] [300/349] Loss_D: 0.35251, Loss_G: 2.40454, Loss_KL: 0.05121\n",
      "[141/400] Loss_D: 0.60981, Loss_G: 2.95455, Loss_KL: 0.06217\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[142/400] [0/349] Loss_D: 0.48735, Loss_G: 2.77223, Loss_KL: 0.06670\n",
      "[142/400] [100/349] Loss_D: 0.62207, Loss_G: 4.12449, Loss_KL: 0.05650\n",
      "[142/400] [200/349] Loss_D: 0.71529, Loss_G: 2.83252, Loss_KL: 0.07093\n",
      "[142/400] [300/349] Loss_D: 0.51252, Loss_G: 2.74920, Loss_KL: 0.06539\n",
      "[142/400] Loss_D: 0.61610, Loss_G: 2.88886, Loss_KL: 0.06257\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[143/400] [0/349] Loss_D: 0.30449, Loss_G: 3.08191, Loss_KL: 0.04682\n",
      "[143/400] [100/349] Loss_D: 0.77649, Loss_G: 3.11500, Loss_KL: 0.05076\n",
      "[143/400] [200/349] Loss_D: 0.67308, Loss_G: 2.55783, Loss_KL: 0.06374\n",
      "[143/400] [300/349] Loss_D: 0.12255, Loss_G: 2.74511, Loss_KL: 0.04870\n",
      "[143/400] Loss_D: 0.59058, Loss_G: 2.67493, Loss_KL: 0.04938\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[144/400] [0/349] Loss_D: 0.60363, Loss_G: 4.09994, Loss_KL: 0.03683\n",
      "[144/400] [100/349] Loss_D: 0.68041, Loss_G: 2.85841, Loss_KL: 0.03508\n",
      "[144/400] [200/349] Loss_D: 0.71718, Loss_G: 2.43373, Loss_KL: 0.03900\n",
      "[144/400] [300/349] Loss_D: 0.71421, Loss_G: 3.16450, Loss_KL: 0.05954\n",
      "[144/400] Loss_D: 0.59867, Loss_G: 2.90636, Loss_KL: 0.05032\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[145/400] [0/349] Loss_D: 0.60608, Loss_G: 3.18538, Loss_KL: 0.05714\n",
      "[145/400] [100/349] Loss_D: 0.78663, Loss_G: 2.68729, Loss_KL: 0.07936\n",
      "[145/400] [200/349] Loss_D: 0.55737, Loss_G: 3.31537, Loss_KL: 0.07299\n",
      "[145/400] [300/349] Loss_D: 0.67858, Loss_G: 3.11126, Loss_KL: 0.06963\n",
      "[145/400] Loss_D: 0.62099, Loss_G: 2.84145, Loss_KL: 0.05750\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[146/400] [0/349] Loss_D: 0.23827, Loss_G: 3.89754, Loss_KL: 0.06755\n",
      "[146/400] [100/349] Loss_D: 0.59384, Loss_G: 2.53142, Loss_KL: 0.05618\n",
      "[146/400] [200/349] Loss_D: 0.70321, Loss_G: 2.12227, Loss_KL: 0.04170\n",
      "[146/400] [300/349] Loss_D: 0.68756, Loss_G: 2.38040, Loss_KL: 0.04781\n",
      "[146/400] Loss_D: 0.61368, Loss_G: 2.89731, Loss_KL: 0.05728\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[147/400] [0/349] Loss_D: 0.84343, Loss_G: 3.69837, Loss_KL: 0.06707\n",
      "[147/400] [100/349] Loss_D: 0.39562, Loss_G: 2.29214, Loss_KL: 0.05204\n",
      "[147/400] [200/349] Loss_D: 0.38959, Loss_G: 2.58035, Loss_KL: 0.05668\n",
      "[147/400] [300/349] Loss_D: 0.68038, Loss_G: 2.55387, Loss_KL: 0.04059\n",
      "[147/400] Loss_D: 0.58389, Loss_G: 2.79704, Loss_KL: 0.05534\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[148/400] [0/349] Loss_D: 0.68643, Loss_G: 2.61870, Loss_KL: 0.03459\n",
      "[148/400] [100/349] Loss_D: 0.62379, Loss_G: 2.58788, Loss_KL: 0.03713\n",
      "[148/400] [200/349] Loss_D: 0.42339, Loss_G: 2.66978, Loss_KL: 0.03636\n",
      "[148/400] [300/349] Loss_D: 0.74428, Loss_G: 2.73605, Loss_KL: 0.03847\n",
      "[148/400] Loss_D: 0.56482, Loss_G: 2.75884, Loss_KL: 0.04189\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[149/400] [0/349] Loss_D: 0.21608, Loss_G: 3.12692, Loss_KL: 0.04899\n",
      "[149/400] [100/349] Loss_D: 0.68505, Loss_G: 2.60498, Loss_KL: 0.06257\n",
      "[149/400] [200/349] Loss_D: 0.72002, Loss_G: 2.38127, Loss_KL: 0.03993\n",
      "[149/400] [300/349] Loss_D: 0.14496, Loss_G: 2.69880, Loss_KL: 0.03200\n",
      "[149/400] Loss_D: 0.57448, Loss_G: 2.66567, Loss_KL: 0.03239\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[150/400] [0/349] Loss_D: 0.73713, Loss_G: 2.94655, Loss_KL: 0.02151\n",
      "[150/400] [100/349] Loss_D: 0.69602, Loss_G: 2.82967, Loss_KL: 0.02688\n",
      "[150/400] [200/349] Loss_D: 0.64283, Loss_G: 2.52908, Loss_KL: 0.02061\n",
      "[150/400] [300/349] Loss_D: 0.51629, Loss_G: 2.26274, Loss_KL: 0.02217\n",
      "[150/400] Loss_D: 0.56386, Loss_G: 2.60048, Loss_KL: 0.02264\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[151/400] [0/349] Loss_D: 0.49024, Loss_G: 2.45329, Loss_KL: 0.01639\n",
      "[151/400] [100/349] Loss_D: 0.37443, Loss_G: 2.70902, Loss_KL: 0.01707\n",
      "[151/400] [200/349] Loss_D: 0.41497, Loss_G: 2.56405, Loss_KL: 0.02162\n",
      "[151/400] [300/349] Loss_D: 0.34570, Loss_G: 2.60232, Loss_KL: 0.01114\n",
      "[151/400] Loss_D: 0.55395, Loss_G: 2.62847, Loss_KL: 0.01724\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[152/400] [0/349] Loss_D: 0.55622, Loss_G: 2.51356, Loss_KL: 0.01438\n",
      "[152/400] [100/349] Loss_D: 0.30989, Loss_G: 2.53248, Loss_KL: 0.01716\n",
      "[152/400] [200/349] Loss_D: 0.38204, Loss_G: 3.02065, Loss_KL: 0.01179\n",
      "[152/400] [300/349] Loss_D: 0.42347, Loss_G: 2.52820, Loss_KL: 0.01893\n",
      "[152/400] Loss_D: 0.56034, Loss_G: 2.64327, Loss_KL: 0.01455\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[153/400] [0/349] Loss_D: 0.45660, Loss_G: 2.60376, Loss_KL: 0.01827\n",
      "[153/400] [100/349] Loss_D: 0.24254, Loss_G: 2.74291, Loss_KL: 0.02342\n",
      "[153/400] [200/349] Loss_D: 0.40422, Loss_G: 2.71060, Loss_KL: 0.01370\n",
      "[153/400] [300/349] Loss_D: 0.80613, Loss_G: 2.70256, Loss_KL: 0.01451\n",
      "[153/400] Loss_D: 0.56939, Loss_G: 2.62362, Loss_KL: 0.01645\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[154/400] [0/349] Loss_D: 0.58053, Loss_G: 2.48145, Loss_KL: 0.01393\n",
      "[154/400] [100/349] Loss_D: 0.58012, Loss_G: 2.73153, Loss_KL: 0.01731\n",
      "[154/400] [200/349] Loss_D: 0.50402, Loss_G: 2.47420, Loss_KL: 0.00956\n",
      "[154/400] [300/349] Loss_D: 0.47220, Loss_G: 2.78844, Loss_KL: 0.00734\n",
      "[154/400] Loss_D: 0.57532, Loss_G: 2.59708, Loss_KL: 0.01286\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[155/400] [0/349] Loss_D: 0.42613, Loss_G: 2.50800, Loss_KL: 0.01493\n",
      "[155/400] [100/349] Loss_D: 0.34319, Loss_G: 2.60568, Loss_KL: 0.00875\n",
      "[155/400] [200/349] Loss_D: 0.55120, Loss_G: 2.51780, Loss_KL: 0.00770\n",
      "[155/400] [300/349] Loss_D: 0.68294, Loss_G: 2.74509, Loss_KL: 0.00945\n",
      "[155/400] Loss_D: 0.55641, Loss_G: 2.62772, Loss_KL: 0.01112\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[156/400] [0/349] Loss_D: 0.73720, Loss_G: 2.55713, Loss_KL: 0.01014\n",
      "[156/400] [100/349] Loss_D: 0.38299, Loss_G: 2.47850, Loss_KL: 0.00717\n",
      "[156/400] [200/349] Loss_D: 0.45870, Loss_G: 2.76612, Loss_KL: 0.00679\n",
      "[156/400] [300/349] Loss_D: 0.71411, Loss_G: 2.56433, Loss_KL: 0.00753\n",
      "[156/400] Loss_D: 0.55662, Loss_G: 2.62151, Loss_KL: 0.00979\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[157/400] [0/349] Loss_D: 0.62716, Loss_G: 2.68581, Loss_KL: 0.02158\n",
      "[157/400] [100/349] Loss_D: 0.37201, Loss_G: 2.58249, Loss_KL: 0.00778\n",
      "[157/400] [200/349] Loss_D: 0.25313, Loss_G: 2.64387, Loss_KL: 0.01012\n",
      "[157/400] [300/349] Loss_D: 0.70585, Loss_G: 2.55099, Loss_KL: 0.01036\n",
      "[157/400] Loss_D: 0.57029, Loss_G: 2.64447, Loss_KL: 0.01008\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[158/400] [0/349] Loss_D: 0.54444, Loss_G: 2.70403, Loss_KL: 0.01040\n",
      "[158/400] [100/349] Loss_D: 0.41886, Loss_G: 2.63296, Loss_KL: 0.00966\n",
      "[158/400] [200/349] Loss_D: 0.67341, Loss_G: 2.56643, Loss_KL: 0.00946\n",
      "[158/400] [300/349] Loss_D: 0.54927, Loss_G: 2.51964, Loss_KL: 0.00779\n",
      "[158/400] Loss_D: 0.57846, Loss_G: 2.53867, Loss_KL: 0.01010\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[159/400] [0/349] Loss_D: 0.76050, Loss_G: 2.53522, Loss_KL: 0.00742\n",
      "[159/400] [100/349] Loss_D: 0.31039, Loss_G: 2.79895, Loss_KL: 0.01230\n",
      "[159/400] [200/349] Loss_D: 0.30908, Loss_G: 2.63356, Loss_KL: 0.01140\n",
      "[159/400] [300/349] Loss_D: 0.73377, Loss_G: 2.49125, Loss_KL: 0.00513\n",
      "[159/400] Loss_D: 0.56880, Loss_G: 2.59874, Loss_KL: 0.00971\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[160/400] [0/349] Loss_D: 0.60792, Loss_G: 2.54657, Loss_KL: 0.02036\n",
      "[160/400] [100/349] Loss_D: 0.74534, Loss_G: 2.50937, Loss_KL: 0.00708\n",
      "[160/400] [200/349] Loss_D: 0.70760, Loss_G: 2.59442, Loss_KL: 0.01032\n",
      "[160/400] [300/349] Loss_D: 0.41083, Loss_G: 2.39984, Loss_KL: 0.01203\n",
      "[160/400] Loss_D: 0.57359, Loss_G: 2.60351, Loss_KL: 0.00866\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[161/400] [0/349] Loss_D: 0.73733, Loss_G: 2.42168, Loss_KL: 0.00617\n",
      "[161/400] [100/349] Loss_D: 0.51276, Loss_G: 2.67529, Loss_KL: 0.00690\n",
      "[161/400] [200/349] Loss_D: 0.53572, Loss_G: 3.14155, Loss_KL: 0.01096\n",
      "[161/400] [300/349] Loss_D: 0.68243, Loss_G: 2.43809, Loss_KL: 0.00511\n",
      "[161/400] Loss_D: 0.57842, Loss_G: 2.68419, Loss_KL: 0.00839\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[162/400] [0/349] Loss_D: 0.73731, Loss_G: 2.61444, Loss_KL: 0.00350\n",
      "[162/400] [100/349] Loss_D: 0.47554, Loss_G: 3.23159, Loss_KL: 0.01057\n",
      "[162/400] [200/349] Loss_D: 0.55036, Loss_G: 2.81983, Loss_KL: 0.01413\n",
      "[162/400] [300/349] Loss_D: 0.73262, Loss_G: 2.38495, Loss_KL: 0.00560\n",
      "[162/400] Loss_D: 0.56442, Loss_G: 2.69443, Loss_KL: 0.00997\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[163/400] [0/349] Loss_D: 0.68344, Loss_G: 2.46404, Loss_KL: 0.01342\n",
      "[163/400] [100/349] Loss_D: 0.47328, Loss_G: 2.61847, Loss_KL: 0.01041\n",
      "[163/400] [200/349] Loss_D: 0.42570, Loss_G: 2.73957, Loss_KL: 0.01303\n",
      "[163/400] [300/349] Loss_D: 0.25408, Loss_G: 3.15882, Loss_KL: 0.00993\n",
      "[163/400] Loss_D: 0.54841, Loss_G: 2.74015, Loss_KL: 0.01130\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[164/400] [0/349] Loss_D: 0.79873, Loss_G: 2.81286, Loss_KL: 0.00999\n",
      "[164/400] [100/349] Loss_D: 0.26910, Loss_G: 2.80264, Loss_KL: 0.00642\n",
      "[164/400] [200/349] Loss_D: 0.63350, Loss_G: 2.56322, Loss_KL: 0.00977\n",
      "[164/400] [300/349] Loss_D: 0.21859, Loss_G: 2.73383, Loss_KL: 0.01669\n",
      "[164/400] Loss_D: 0.57628, Loss_G: 2.64679, Loss_KL: 0.01220\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[165/400] [0/349] Loss_D: 0.78162, Loss_G: 2.68063, Loss_KL: 0.00768\n",
      "[165/400] [100/349] Loss_D: 0.55495, Loss_G: 2.66529, Loss_KL: 0.02017\n",
      "[165/400] [200/349] Loss_D: 0.46441, Loss_G: 2.58922, Loss_KL: 0.02824\n",
      "[165/400] [300/349] Loss_D: 0.30426, Loss_G: 2.73338, Loss_KL: 0.01319\n",
      "[165/400] Loss_D: 0.56607, Loss_G: 2.63733, Loss_KL: 0.01349\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[166/400] [0/349] Loss_D: 0.18788, Loss_G: 2.86228, Loss_KL: 0.01110\n",
      "[166/400] [100/349] Loss_D: 0.23096, Loss_G: 2.83825, Loss_KL: 0.00940\n",
      "[166/400] [200/349] Loss_D: 0.78885, Loss_G: 2.53815, Loss_KL: 0.01449\n",
      "[166/400] [300/349] Loss_D: 0.38225, Loss_G: 2.63494, Loss_KL: 0.00961\n",
      "[166/400] Loss_D: 0.55389, Loss_G: 2.65550, Loss_KL: 0.01469\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[167/400] [0/349] Loss_D: 0.26095, Loss_G: 2.66402, Loss_KL: 0.00820\n",
      "[167/400] [100/349] Loss_D: 0.38286, Loss_G: 2.58459, Loss_KL: 0.01865\n",
      "[167/400] [200/349] Loss_D: 0.65600, Loss_G: 2.54470, Loss_KL: 0.01579\n",
      "[167/400] [300/349] Loss_D: 0.58191, Loss_G: 2.54141, Loss_KL: 0.01250\n",
      "[167/400] Loss_D: 0.56526, Loss_G: 2.63179, Loss_KL: 0.01595\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[168/400] [0/349] Loss_D: 0.75749, Loss_G: 2.46288, Loss_KL: 0.01320\n",
      "[168/400] [100/349] Loss_D: 0.68141, Loss_G: 2.65859, Loss_KL: 0.01629\n",
      "[168/400] [200/349] Loss_D: 0.67774, Loss_G: 2.68774, Loss_KL: 0.03204\n",
      "[168/400] [300/349] Loss_D: 0.70136, Loss_G: 2.81497, Loss_KL: 0.01424\n",
      "[168/400] Loss_D: 0.56025, Loss_G: 2.64368, Loss_KL: 0.01889\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[169/400] [0/349] Loss_D: 0.67471, Loss_G: 2.67941, Loss_KL: 0.01855\n",
      "[169/400] [100/349] Loss_D: 0.46289, Loss_G: 2.62856, Loss_KL: 0.01198\n",
      "[169/400] [200/349] Loss_D: 0.58829, Loss_G: 2.53329, Loss_KL: 0.01356\n",
      "[169/400] [300/349] Loss_D: 0.61931, Loss_G: 2.64174, Loss_KL: 0.02231\n",
      "[169/400] Loss_D: 0.57340, Loss_G: 2.60495, Loss_KL: 0.01638\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[170/400] [0/349] Loss_D: 0.66057, Loss_G: 2.47322, Loss_KL: 0.01461\n",
      "[170/400] [100/349] Loss_D: 0.69300, Loss_G: 2.49621, Loss_KL: 0.01544\n",
      "[170/400] [200/349] Loss_D: 0.35681, Loss_G: 2.69757, Loss_KL: 0.01357\n",
      "[170/400] [300/349] Loss_D: 0.47922, Loss_G: 2.63907, Loss_KL: 0.01823\n",
      "[170/400] Loss_D: 0.56639, Loss_G: 2.62998, Loss_KL: 0.01681\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[171/400] [0/349] Loss_D: 0.43513, Loss_G: 2.52716, Loss_KL: 0.01645\n",
      "[171/400] [100/349] Loss_D: 0.21217, Loss_G: 2.63024, Loss_KL: 0.02276\n",
      "[171/400] [200/349] Loss_D: 0.25181, Loss_G: 2.74926, Loss_KL: 0.01065\n",
      "[171/400] [300/349] Loss_D: 0.43177, Loss_G: 2.63521, Loss_KL: 0.01436\n",
      "[171/400] Loss_D: 0.55335, Loss_G: 2.65144, Loss_KL: 0.01571\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[172/400] [0/349] Loss_D: 0.67280, Loss_G: 2.42670, Loss_KL: 0.00799\n",
      "[172/400] [100/349] Loss_D: 0.66601, Loss_G: 2.56451, Loss_KL: 0.01661\n",
      "[172/400] [200/349] Loss_D: 0.30350, Loss_G: 2.65859, Loss_KL: 0.00620\n",
      "[172/400] [300/349] Loss_D: 0.40936, Loss_G: 2.76261, Loss_KL: 0.01015\n",
      "[172/400] Loss_D: 0.56814, Loss_G: 2.61158, Loss_KL: 0.01562\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[173/400] [0/349] Loss_D: 0.40333, Loss_G: 2.71918, Loss_KL: 0.00758\n",
      "[173/400] [100/349] Loss_D: 0.64127, Loss_G: 2.50401, Loss_KL: 0.01123\n",
      "[173/400] [200/349] Loss_D: 0.26147, Loss_G: 2.66655, Loss_KL: 0.01661\n",
      "[173/400] [300/349] Loss_D: 0.68939, Loss_G: 2.65567, Loss_KL: 0.00981\n",
      "[173/400] Loss_D: 0.57137, Loss_G: 2.60803, Loss_KL: 0.01612\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[174/400] [0/349] Loss_D: 0.59409, Loss_G: 2.79059, Loss_KL: 0.02674\n",
      "[174/400] [100/349] Loss_D: 0.69977, Loss_G: 2.43898, Loss_KL: 0.02618\n",
      "[174/400] [200/349] Loss_D: 0.71200, Loss_G: 2.62964, Loss_KL: 0.01423\n",
      "[174/400] [300/349] Loss_D: 0.47279, Loss_G: 2.55218, Loss_KL: 0.00740\n",
      "[174/400] Loss_D: 0.58902, Loss_G: 2.58083, Loss_KL: 0.01673\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[175/400] [0/349] Loss_D: 0.76238, Loss_G: 2.51311, Loss_KL: 0.01523\n",
      "[175/400] [100/349] Loss_D: 0.41209, Loss_G: 2.63129, Loss_KL: 0.03004\n",
      "[175/400] [200/349] Loss_D: 0.52090, Loss_G: 2.50569, Loss_KL: 0.01515\n",
      "[175/400] [300/349] Loss_D: 0.24804, Loss_G: 2.58043, Loss_KL: 0.01132\n",
      "[175/400] Loss_D: 0.57193, Loss_G: 2.61171, Loss_KL: 0.01473\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[176/400] [0/349] Loss_D: 0.70998, Loss_G: 2.55715, Loss_KL: 0.03084\n",
      "[176/400] [100/349] Loss_D: 0.73552, Loss_G: 2.81025, Loss_KL: 0.01137\n",
      "[176/400] [200/349] Loss_D: 0.23501, Loss_G: 2.76999, Loss_KL: 0.01511\n",
      "[176/400] [300/349] Loss_D: 0.25726, Loss_G: 2.86201, Loss_KL: 0.01190\n",
      "[176/400] Loss_D: 0.55095, Loss_G: 2.67105, Loss_KL: 0.01621\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[177/400] [0/349] Loss_D: 0.49887, Loss_G: 2.64183, Loss_KL: 0.01346\n",
      "[177/400] [100/349] Loss_D: 0.45029, Loss_G: 2.65800, Loss_KL: 0.00798\n",
      "[177/400] [200/349] Loss_D: 0.27863, Loss_G: 2.73937, Loss_KL: 0.02099\n",
      "[177/400] [300/349] Loss_D: 0.38538, Loss_G: 2.54767, Loss_KL: 0.01684\n",
      "[177/400] Loss_D: 0.55973, Loss_G: 2.63988, Loss_KL: 0.01480\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[178/400] [0/349] Loss_D: 0.67599, Loss_G: 2.64301, Loss_KL: 0.00939\n",
      "[178/400] [100/349] Loss_D: 0.17237, Loss_G: 2.66280, Loss_KL: 0.02066\n",
      "[178/400] [200/349] Loss_D: 0.62666, Loss_G: 2.50630, Loss_KL: 0.01102\n",
      "[178/400] [300/349] Loss_D: 0.44745, Loss_G: 2.87995, Loss_KL: 0.00646\n",
      "[178/400] Loss_D: 0.56232, Loss_G: 2.61764, Loss_KL: 0.01459\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[179/400] [0/349] Loss_D: 0.60129, Loss_G: 2.55544, Loss_KL: 0.00922\n",
      "[179/400] [100/349] Loss_D: 0.55942, Loss_G: 2.56201, Loss_KL: 0.01542\n",
      "[179/400] [200/349] Loss_D: 0.72025, Loss_G: 2.68273, Loss_KL: 0.01157\n",
      "[179/400] [300/349] Loss_D: 0.80534, Loss_G: 2.54302, Loss_KL: 0.00475\n",
      "[179/400] Loss_D: 0.56495, Loss_G: 2.60628, Loss_KL: 0.01119\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[180/400] [0/349] Loss_D: 0.72536, Loss_G: 2.69009, Loss_KL: 0.00367\n",
      "[180/400] [100/349] Loss_D: 0.59266, Loss_G: 2.57283, Loss_KL: 0.02135\n",
      "[180/400] [200/349] Loss_D: 0.48174, Loss_G: 2.58792, Loss_KL: 0.00821\n",
      "[180/400] [300/349] Loss_D: 0.47716, Loss_G: 2.43861, Loss_KL: 0.01564\n",
      "[180/400] Loss_D: 0.57141, Loss_G: 2.58002, Loss_KL: 0.00997\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[181/400] [0/349] Loss_D: 0.60487, Loss_G: 2.48033, Loss_KL: 0.00504\n",
      "[181/400] [100/349] Loss_D: 0.79717, Loss_G: 2.46852, Loss_KL: 0.01221\n",
      "[181/400] [200/349] Loss_D: 0.22365, Loss_G: 2.59055, Loss_KL: 0.00320\n",
      "[181/400] [300/349] Loss_D: 0.49790, Loss_G: 2.53211, Loss_KL: 0.00805\n",
      "[181/400] Loss_D: 0.56659, Loss_G: 2.60015, Loss_KL: 0.00965\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[182/400] [0/349] Loss_D: 0.83058, Loss_G: 2.55164, Loss_KL: 0.01496\n",
      "[182/400] [100/349] Loss_D: 0.71770, Loss_G: 2.71518, Loss_KL: 0.01287\n",
      "[182/400] [200/349] Loss_D: 0.22544, Loss_G: 2.72170, Loss_KL: 0.01274\n",
      "[182/400] [300/349] Loss_D: 0.65556, Loss_G: 2.62327, Loss_KL: 0.01996\n",
      "[182/400] Loss_D: 0.55853, Loss_G: 2.63688, Loss_KL: 0.01123\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[183/400] [0/349] Loss_D: 0.68513, Loss_G: 2.56826, Loss_KL: 0.00579\n",
      "[183/400] [100/349] Loss_D: 0.58783, Loss_G: 2.30153, Loss_KL: 0.00964\n",
      "[183/400] [200/349] Loss_D: 0.72970, Loss_G: 2.48912, Loss_KL: 0.00999\n",
      "[183/400] [300/349] Loss_D: 0.65584, Loss_G: 2.56317, Loss_KL: 0.00664\n",
      "[183/400] Loss_D: 0.57920, Loss_G: 2.66700, Loss_KL: 0.01062\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[184/400] [0/349] Loss_D: 0.82189, Loss_G: 3.18434, Loss_KL: 0.00921\n",
      "[184/400] [100/349] Loss_D: 0.76582, Loss_G: 2.70314, Loss_KL: 0.00459\n",
      "[184/400] [200/349] Loss_D: 0.65208, Loss_G: 2.70091, Loss_KL: 0.00486\n",
      "[184/400] [300/349] Loss_D: 0.72186, Loss_G: 2.45473, Loss_KL: 0.00912\n",
      "[184/400] Loss_D: 0.56922, Loss_G: 2.67569, Loss_KL: 0.01153\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[185/400] [0/349] Loss_D: 0.78371, Loss_G: 2.71631, Loss_KL: 0.00650\n",
      "[185/400] [100/349] Loss_D: 0.67001, Loss_G: 2.56549, Loss_KL: 0.00999\n",
      "[185/400] [200/349] Loss_D: 0.57983, Loss_G: 2.67102, Loss_KL: 0.01152\n",
      "[185/400] [300/349] Loss_D: 0.62649, Loss_G: 2.34461, Loss_KL: 0.00686\n",
      "[185/400] Loss_D: 0.56766, Loss_G: 2.62209, Loss_KL: 0.01178\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[186/400] [0/349] Loss_D: 0.49687, Loss_G: 2.75419, Loss_KL: 0.02077\n",
      "[186/400] [100/349] Loss_D: 0.66045, Loss_G: 2.52968, Loss_KL: 0.00628\n",
      "[186/400] [200/349] Loss_D: 0.56329, Loss_G: 2.46316, Loss_KL: 0.00542\n",
      "[186/400] [300/349] Loss_D: 0.48949, Loss_G: 2.59298, Loss_KL: 0.01648\n",
      "[186/400] Loss_D: 0.56055, Loss_G: 2.62335, Loss_KL: 0.01211\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[187/400] [0/349] Loss_D: 0.58660, Loss_G: 2.59475, Loss_KL: 0.00762\n",
      "[187/400] [100/349] Loss_D: 0.58795, Loss_G: 2.70829, Loss_KL: 0.00979\n",
      "[187/400] [200/349] Loss_D: 0.62024, Loss_G: 2.34906, Loss_KL: 0.02372\n",
      "[187/400] [300/349] Loss_D: 0.69197, Loss_G: 2.61788, Loss_KL: 0.01726\n",
      "[187/400] Loss_D: 0.58245, Loss_G: 2.58483, Loss_KL: 0.01097\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[188/400] [0/349] Loss_D: 0.30758, Loss_G: 2.37328, Loss_KL: 0.01348\n",
      "[188/400] [100/349] Loss_D: 0.74092, Loss_G: 2.54232, Loss_KL: 0.00938\n",
      "[188/400] [200/349] Loss_D: 0.53173, Loss_G: 2.75340, Loss_KL: 0.01120\n",
      "[188/400] [300/349] Loss_D: 0.63222, Loss_G: 2.75396, Loss_KL: 0.01252\n",
      "[188/400] Loss_D: 0.57986, Loss_G: 2.62567, Loss_KL: 0.00997\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[189/400] [0/349] Loss_D: 0.68782, Loss_G: 2.50967, Loss_KL: 0.01442\n",
      "[189/400] [100/349] Loss_D: 0.12138, Loss_G: 2.62709, Loss_KL: 0.00802\n",
      "[189/400] [200/349] Loss_D: 0.49090, Loss_G: 2.72299, Loss_KL: 0.00617\n",
      "[189/400] [300/349] Loss_D: 0.27417, Loss_G: 2.64724, Loss_KL: 0.02083\n",
      "[189/400] Loss_D: 0.55654, Loss_G: 2.59201, Loss_KL: 0.00941\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[190/400] [0/349] Loss_D: 0.58877, Loss_G: 2.49968, Loss_KL: 0.00729\n",
      "[190/400] [100/349] Loss_D: 0.47369, Loss_G: 2.60700, Loss_KL: 0.00425\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bf4fd3192232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4b1c2923d72f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(stage, batch_size, trainloader)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0ms1_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mlow_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage1_G\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                 \u001b[0;31m# pass stage 1 output to generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0ms2_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mused_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0m_limbo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_started\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba9bc282ea7dd8acf6b93a88ab047ea17bba2d98cff2c21ca6cffa26ac4d8f39"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
