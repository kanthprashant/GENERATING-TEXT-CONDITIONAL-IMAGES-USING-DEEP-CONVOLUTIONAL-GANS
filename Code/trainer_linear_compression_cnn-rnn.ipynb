{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/common/users/ppk31/CS543_DL_Proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from configs import config3 as config\n",
    "from pytorch_model_summary import summary\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from utils import (weights_init, make_train_test_split, load_data, compute_discriminator_loss, \n",
    "                   compute_generator_loss, KL_loss, L1_loss, save_img_results, save_model, load_from_checkpoint)\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from dataset import Text2ImgDataset_cnnrnn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import traceback\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "DATASET = '/freespace/local/ppk31_cs543/Project/Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.text_encoder == \"distilbert-base-uncased\":\n",
    "    from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n",
    "elif config.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "    from transformers import CLIPTokenizer, CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPU: True\n",
      "GPU ids: [0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"using GPU: {torch.cuda.is_available()}\")\n",
    "gpus = list(range(torch.cuda.device_count()))\n",
    "print(f\"GPU ids: {gpus}\")\n",
    "\n",
    "torch.random.seed()\n",
    "torch.manual_seed(0)\n",
    "\n",
    "torch.cuda.set_device(gpus[0])\n",
    "cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(text_encoder):\n",
    "    print(f\"using {text_encoder} as text encoder\")\n",
    "    if text_encoder == \"distilbert-base-uncased\":\n",
    "        return DistilBertTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "        return CLIPTokenizer.from_pretrained(text_encoder)\n",
    "    elif text_encoder == \"text-cnn-rnn\":\n",
    "        with open(config.trainEmbeddingsListPath, \"rb\") as f:\n",
    "            tokenizer = pickle.load(f)\n",
    "        return tokenizer\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, text_encoder, pretrained=True):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        if text_encoder == \"distilbert-base-uncased\":\n",
    "            self.encoder = DistilBertModel.from_pretrained(text_encoder)\n",
    "        elif text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            self.encoder = CLIPModel.from_pretrained(text_encoder)\n",
    "        # self.text_embedding = 768\n",
    "        # self.projection = ProjectionHead('text_projector', self.text_embedding, project_dim)\n",
    "        self.retrieve_token_index = 0\n",
    "    \n",
    "    def forward(self, input_tokens, attention_mask):\n",
    "        if self.text_encoder == \"distilbert-base-uncased\":\n",
    "            out = self.encoder(input_ids = input_tokens, attention_mask = attention_mask)\n",
    "            last_hidden_states = out.last_hidden_state\n",
    "            embeddings = last_hidden_states[:, self.retrieve_token_index, :]    # output_dimensions = 768\n",
    "        elif self.text_encoder == \"openai/clip-vit-base-patch32\":\n",
    "            embeddings = self.encoder.get_text_features(input_ids = input_tokens, attention_mask = attention_mask) # output_dimensions = 512\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augmented_Projection(nn.Module):\n",
    "    def __init__(self, stage, gen_channels, gen_dim):\n",
    "        super(Augmented_Projection, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.t_dim = config.text_dim\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.z_dim = config.z_dim\n",
    "        self.gen_in = gen_channels #config.generator_dim * gen_dim\n",
    "        self.fc = nn.Linear(self.t_dim, self.c_dim * 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        if stage == 1:\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(self.c_dim + self.z_dim, self.gen_in * gen_dim * gen_dim, bias=False), # bias=False, # 768 -> 192*8*8*8\n",
    "                nn.BatchNorm1d(self.gen_in * gen_dim * gen_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "\n",
    "    def augment(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp()\n",
    "        # if config.cuda_is_available:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_().cuda()\n",
    "        # else:\n",
    "        #     eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(torch.randn(std.size()).float().cuda())\n",
    "        # eps = Variable(eps)\n",
    "        # eps.mul(std).add(mu)\n",
    "        return mu + (std * eps)\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        if noise is None and self.stage==1:\n",
    "            noise = torch.randn((text_embedding.shape[0], self.z_dim)).float().cuda()\n",
    "        x = self.relu(self.fc(text_embedding))\n",
    "        mu = x[:, :self.c_dim]\n",
    "        logvar = x[:, self.c_dim:]\n",
    "        c_code = self.augment(mu, logvar)\n",
    "        \n",
    "        if self.stage == 1:\n",
    "            c_code = torch.cat((c_code, noise), dim=1)\n",
    "            c_code = self.project(c_code)\n",
    "        \n",
    "        return c_code, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, kernel_size=4, stride=2, padding=1, batch_norm=True, activation=True, use_conv=True, bias=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv2d(self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool2d(kernel_size=stride, stride=stride)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = self.op(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, out_channels=None, stride=1, padding=1, batch_norm=True, activation=True, bias=False, use_deconv=False, dropout=False):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.batch_norm = batch_norm\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.use_deconv = use_deconv\n",
    "\n",
    "        if use_deconv:\n",
    "            self.deconv = nn.ConvTranspose2d(self.channels, self.out_channels, kernel_size=4, stride=2, padding=padding, bias=bias) # use when not using interpolate\n",
    "        else:\n",
    "            self.conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, stride=stride, padding=padding, bias = bias)\n",
    "        if batch_norm:\n",
    "            self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        if activation:\n",
    "            self.activtn = nn.ReLU()\n",
    "        if self.dropout:\n",
    "            self.drop = nn.Dropout2d(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        if self.use_deconv:\n",
    "            x = self.deconv(x)\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            x = self.conv(x)\n",
    "        if self.batch_norm:\n",
    "            x = self.batchnorm(x)\n",
    "        if self.activation:\n",
    "            x = self.activtn(x)\n",
    "        if self.dropout:\n",
    "            x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block that can optionally change the number of channels.\n",
    "\n",
    "    :param in_channels: the number of input channels.\n",
    "    :param out_channels: if specified, the number of out channels.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=None,\n",
    "        stride = 1,\n",
    "        padding = 1\n",
    "    ):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=padding)\n",
    "        if in_channels == out_channels:\n",
    "                self.x_residual = nn.Identity()\n",
    "        else:\n",
    "            self.x_residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        g = self.bn2(self.conv2(self.relu(self.bn1(self.conv1(x)))))\n",
    "        x = self.x_residual(x)\n",
    "        h = x + g\n",
    "        return self.relu(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    :param channels: is the number of channels in the feature map\n",
    "    :param n_heads: is the number of attention heads\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, n_heads=1, cond_channels=None):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "        assert (\n",
    "            channels % n_heads == 0\n",
    "        ), f\"q,k,v channels {channels//n_heads} cannot be constructed for {n_heads} heads, input channels: {channels}\"\n",
    "        self.n_heads = n_heads\n",
    "        # self.norm1 = nn.GroupNorm(num_groups=16, num_channels=channels, eps=1e-6, affine=True) # num_groups=32\n",
    "        # self.norm1 = nn.BatchNorm2d(channels)\n",
    "        self.qkv = nn.Conv2d(channels, channels*3, kernel_size=1)\n",
    "        self.attention = QKVAttention(self.n_heads)\n",
    "        if cond_channels is not None:\n",
    "            # self.norm2 = nn.GroupNorm(num_groups=16, num_channels=cond_channels, eps=1e-6, affine=True) # num_groups=32\n",
    "            # self.norm2 = nn.BatchNorm2d(cond_channels)\n",
    "            self.cond_kv = nn.Conv2d(cond_channels, channels*2, kernel_size=1)\n",
    "        # self.proj_out = nn.Conv1d(channels, channels, kernel_size=1)\n",
    "    def forward(self, x, cond_out = None):\n",
    "        b, c, *spatial = x.shape\n",
    "        h, w = spatial\n",
    "        # qkv = self.qkv(self.norm1(x).view(b, c, -1)) # b, c*3, h*w\n",
    "        qkv = self.qkv(x).view(b, -1, h*w) # b, c*3, h*w\n",
    "        # qkv = self.qkv(x.view(b, c, -1)) # b, c*3, h*w\n",
    "        if cond_out is not None:\n",
    "            _, cc, *hw = cond_out.shape\n",
    "            hh, ww = hw\n",
    "            # cond_out = self.cond_kv(self.norm2(cond_out).view(b, cc, -1))\n",
    "            cond_out = self.cond_kv(cond_out).view(b, -1, hh*ww)\n",
    "            h = self.attention(qkv, cond_out)\n",
    "        else:\n",
    "            h = self.attention(qkv)\n",
    "        # h = self.proj_out(h)\n",
    "        return x + h.reshape(b, c, *spatial)\n",
    "\n",
    "class QKVAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "    def forward(self, qkv, cond_kv=None):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads) # no. of channels for q,k,v for each head\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(ch, dim=1)\n",
    "        if cond_kv is not None:\n",
    "            assert cond_kv.shape[1] == self.n_heads * ch * 2\n",
    "            ek, ev = cond_kv.reshape(bs * self.n_heads, ch * 2, -1).split(ch, dim=1)\n",
    "            k = torch.cat([ek, k], dim=-1)\n",
    "            v = torch.cat([ev, v], dim=-1)\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = torch.einsum(\"bct,bcs->bts\", q * scale, k * scale)\n",
    "        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = torch.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator1(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator1, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims # 4\n",
    "        self.in_channels = config.generator_dim * 8 # 192*8\n",
    "        self.channel_mul = config.channel_mul\n",
    "        self.num_resblocks = config.n_resblocks\n",
    "        self.use_deconv=config.use_deconv \n",
    "        self.dropout=config.dropout\n",
    "        ch = self.in_channels\n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, text_embedding, noise=None):\n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding, noise)\n",
    "        x = proj_x.view(-1, self.in_channels, self.in_dims, self.in_dims)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator2(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Generator2, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.in_dims = config.in_dims * config.in_dims # 16\n",
    "        self.in_channels = config.generator_dim # 192\n",
    "        self.channel_mul = config.channel_mul_stage2\n",
    "        self.num_resblocks = config.n_resblocks_stage2\n",
    "        self.use_deconv=config.use_deconv2 \n",
    "        self.dropout=config.dropout2\n",
    "        ch = self.in_channels * 4 \n",
    "        \n",
    "        self.c_dim = config.condition_dim\n",
    "        n_heads =  config.attention_heads\n",
    "        attention_resolutions = config.attention_resolutions\n",
    "        dims = self.in_dims\n",
    "\n",
    "        self.aug_project = Augmented_Projection(self.stage, self.in_channels, self.in_dims)\n",
    "        \n",
    "        self.downblocks= nn.Sequential(\n",
    "            Downsample(3, self.in_channels, kernel_size=3, stride=1, padding=1, batch_norm=False),\n",
    "            Downsample(self.in_channels, self.in_channels*2),\n",
    "            Downsample(self.in_channels*2, self.in_channels*4)\n",
    "        )\n",
    "        self.combined = nn.Sequential(\n",
    "            Downsample(self.in_channels*4 + self.c_dim, self.in_channels*4, kernel_size=3, stride=1, padding=1) # 768 x 16 x 16\n",
    "        )\n",
    "            \n",
    "        self.blocks = nn.ModuleList()\n",
    "        for layer, cmul in enumerate(self.channel_mul):\n",
    "\n",
    "            for _ in range(self.num_resblocks[layer]): # n_resblocks in stage2 = 2\n",
    "                self.blocks.append(ResBlock(ch//cmul, ch//cmul, stride=1, padding=1))\n",
    "            \n",
    "            if dims in attention_resolutions:\n",
    "                self.blocks.append(AttentionBlock(ch//cmul, n_heads=n_heads))\n",
    "            \n",
    "            if layer < len(self.channel_mul)-1:\n",
    "                self.blocks.append(Upsample(ch//cmul, ch//self.channel_mul[layer+1], use_deconv=self.use_deconv, dropout=self.dropout if layer<2 else False))\n",
    "            \n",
    "            dims *= 2\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch//self.channel_mul[-1], 3, kernel_size=3, padding=1, bias=True),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        print(\"Initialized stage2 Generator\")\n",
    "        \n",
    "    def forward(self, text_embedding, stage1_out):\n",
    "        enc_img = self.downblocks(stage1_out)\n",
    "        \n",
    "        proj_x, mu, logvar = self.aug_project(text_embedding)\n",
    "        x = proj_x.view(-1, self.c_dim, 1, 1)\n",
    "        x = x.repeat(1, 1, self.in_dims, self.in_dims)\n",
    "        x = torch.cat([enc_img, x], dim=1)\n",
    "        x = self.combined(x)\n",
    "\n",
    "        for up in self.blocks:\n",
    "            x = up(x)\n",
    "        img_out = self.out(x)\n",
    "        return img_out, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D_Logits(nn.Module):\n",
    "    def __init__(self, d_ch, c_dim, txt_dim, condition):\n",
    "        super(D_Logits, self).__init__()\n",
    "        self.condition = condition\n",
    "        self.d_ch = d_ch\n",
    "        self.c_dim = c_dim\n",
    "        self.txt_dim = txt_dim\n",
    "\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8+self.c_dim, n_heads=2, cond_channels=self.c_dim)\n",
    "        # self.conv1 = Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # self.attention = AttentionBlock(channels=self.d_ch*8, n_heads=1)\n",
    "\n",
    "        self.compress = nn.Sequential(\n",
    "            nn.Linear(self.txt_dim, self.c_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        if condition:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                # Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                Downsample(self.d_ch*8 + self.c_dim, self.d_ch*8, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "        else:\n",
    "            self.outlogits = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, 1, stride=4, padding=0, batch_norm=False, activation=False, bias=True),\n",
    "                nn.Sigmoid(),\n",
    "            )\n",
    "    \n",
    "    def forward(self, feat, cond_out=None):\n",
    "        if self.condition:\n",
    "            ### compress text_embeddings using a linear layer\n",
    "            cond_out = self.compress(cond_out)\n",
    "            ### reshape\n",
    "            cond_out = cond_out.view(-1, self.c_dim, 1, 1)\n",
    "            cond_out = cond_out.repeat(1, 1, 4, 4) # (1, 1, 8, 8) (1,1,config.in_dims,config.in_dims)\n",
    "            x = torch.cat((feat, cond_out), 1)\n",
    "        else:\n",
    "            x = feat\n",
    "        # x = self.conv1(x)\n",
    "        # x = self.attention(feat, cond_out)\n",
    "        # x = self.attention(x)\n",
    "        out = self.outlogits(x)\n",
    "        return out.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, stage):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.stage = stage\n",
    "        self.d_ch = config.discriminator_channel\n",
    "        self.c_dim = config.condition_dim\n",
    "        self.txt_dim = config.text_dim\n",
    "        # self.att_logits = config.att_logits\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            Downsample(3, self.d_ch, batch_norm=False),\n",
    "            Downsample(self.d_ch, self.d_ch*2),\n",
    "            Downsample(self.d_ch*2, self.d_ch*4),\n",
    "            Downsample(self.d_ch*4, self.d_ch*8),\n",
    "        )\n",
    "\n",
    "        if stage == 2:\n",
    "            self.encode_further = nn.Sequential(\n",
    "                Downsample(self.d_ch*8, self.d_ch*16),\n",
    "                Downsample(self.d_ch*16, self.d_ch*32),\n",
    "                Downsample(self.d_ch*32, self.d_ch*16, kernel_size=3, stride=1, padding=1),\n",
    "                Downsample(self.d_ch*16, self.d_ch*8, kernel_size=3, stride=1, padding=1),\n",
    "            )\n",
    "\n",
    "        self.cond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, self.txt_dim, condition=True)\n",
    "        self.uncond_discriminator_logits = None\n",
    "        # if self.stage == 2:\n",
    "        #     self.uncond_discriminator_logits = D_Logits(self.d_ch, self.c_dim, condition=False)\n",
    "        print(\"Initialized, stage {} discriminator\".format(stage))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        if self.stage == 2:\n",
    "            x = self.encode_further(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(stage, batch_size, random_captions=True):\n",
    "        imageSize = None\n",
    "        if stage == 1:\n",
    "                imageSize = config.imageSize # 64\n",
    "        else:\n",
    "                imageSize = config.imageSize * 4  # 64*4 = 256\n",
    "\n",
    "        print(f\"Genearting Dataset with image size: {imageSize}\")\n",
    "\n",
    "        tokenizer = get_tokenizer(config.text_encoder)\n",
    "\n",
    "        imageFolder = os.path.join(DATASET, config.dataset, config.imageFolder) # check these params in config before running\n",
    "\n",
    "        if random_captions:\n",
    "                train_images, train_captions = load_data(imageListPath=config.trainImageListPath, captionsListPath=config.trainCaptionsListPath)\n",
    "                train_dataset = Text2ImgDataset_cnnrnn(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage=False)\n",
    "        else:\n",
    "                imageListPath = os.path.join(DATASET, config.dataset, config.imageListPath) # check these params in config before running\n",
    "                captionsListPath = os.path.join(DATASET, config.dataset, config.captionsListPath) # check these params in config before running\n",
    "                train_images, train_captions, test_images, test_captions = make_train_test_split(imageListPath, captionsListPath, config.test_size)\n",
    "                train_dataset = Text2ImgDataset_cnnrnn(imageFolder, tokenizer, config.text_encoder, train_images, train_captions, imageSize, augmentImage = False) # change based on stage -imagesize\n",
    "        \n",
    "        print(\"Dataset created:\\n\\\n",
    "                length of train dataset: {}\\n\".format(len(train_dataset)))\n",
    "\n",
    "        trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "        return trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f48005437f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(config.tb_dir, exist_ok=True)       # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.model_out, exist_ok=True)    # change these in config when training stage1 and stage2 accordingly\n",
    "os.makedirs(config.out_img, exist_ok=True)      # change these in config when training stage1 and stage2 accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_controllers(netD, netG):\n",
    "    d_lr = config.d_lr\n",
    "    g_lr = config.g_lr\n",
    "    # We create the optimizer object of the discriminator\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr = d_lr, betas = (0.5, 0.999))\n",
    "    scheduler_D = MultiStepLR(optimizerD, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True) \n",
    "    # We create the optimizer object of the generator.\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr = g_lr, betas = (0.5, 0.999)) \n",
    "    scheduler_G = MultiStepLR(optimizerG, milestones=config.lr_decay_epoch, gamma=config.lr_gamma, verbose=True)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    L1Loss = nn.L1Loss()\n",
    "\n",
    "    return optimizerD, scheduler_D, optimizerG, scheduler_G, criterion, L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "def train(stage, batch_size, trainloader):\n",
    "\n",
    "    noise_dim = config.z_dim\n",
    "    noise = Variable(torch.FloatTensor(batch_size, noise_dim).float().cuda())\n",
    "    real_labels = Variable(torch.ones(batch_size).float().cuda())\n",
    "    fake_labels = Variable(torch.zeros(batch_size).float().cuda())\n",
    "\n",
    "    assert batch_size == real_labels.shape[0], \"batch_size and target size do not match in real_labels\"\n",
    "    assert batch_size == fake_labels.shape[0], \"batch_size and target size do not match in fake_labels\"\n",
    "\n",
    "    if config.text_encoder != \"text-cnn-rnn\":\n",
    "        text_encoder = TextEncoder(config.text_encoder, pretrained=True)\n",
    "        text_encoder.eval()\n",
    "\n",
    "    # stage 1 training, only stage 1 g and stage1 d\n",
    "    if stage == 1:\n",
    "        netG = Generator1(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    # stage 2 training, stage1 g output is fed to stage2 g, stage2 d\n",
    "    else:\n",
    "        stage1_G = Generator1(1)\n",
    "        stage1_G = load_from_checkpoint(stage1_G, config.gen1_ckpt)\n",
    "        stage1_G.float().cuda()\n",
    "        # fix parameters of stageI GAN\n",
    "        for param in stage1_G.parameters():\n",
    "            param.requires_grad = False\n",
    "        stage1_G.eval()\n",
    "        netG = Generator2(stage=stage)\n",
    "        netD = Discriminator(stage=stage)\n",
    "\n",
    "    recovered_epoch = 0\n",
    "    if config.load_checkpoint:\n",
    "        if stage == 1:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen1_ckpt, netD, config.d1_ckpt)\n",
    "        else:\n",
    "            recovered_epoch, netG, netD = load_from_checkpoint(netG, config.gen2_ckpt, netD, config.d2_ckpt)\n",
    "    else:\n",
    "        netG.apply(weights_init)\n",
    "        netD.apply(weights_init)\n",
    "    netG.float().cuda()\n",
    "    netD.float().cuda()\n",
    "\n",
    "    optimizerD, schedulerD, optimizerG, schedulerG, criterion, L1Loss = get_controllers(netD, netG)\n",
    "\n",
    "    tb = 'stage' + str(stage) + '_b' + str(batch_size) + '_d' + (str(config.imageSize) if stage==1 else str(config.imageSize*4)) + '_' + str(recovered_epoch)\n",
    "    summary = SummaryWriter(os.path.join(config.tb_dir, tb))\n",
    "   \n",
    "    running_count = 0\n",
    "    # KL_coeff = torch.linspace(0., config.KL_COEFF, 30)\n",
    "    # alpha_l1, _ = torch.linspace(0., config.alpha_L1, 30).sort(descending=True)\n",
    "    KL_coeff = config.KL_COEFF\n",
    "    alpha_L1 = config.alpha_L1\n",
    "\n",
    "    print(f\"Traininig Stage: {stage}, outputs at: {config.tb_dir}, {config.out_img}, {config.model_out}\")\n",
    "\n",
    "    for epoch in range(recovered_epoch+1, config.max_epoch+1):\n",
    "        D_loss = 0\n",
    "        D_real_loss = 0\n",
    "        D_fake_loss = 0\n",
    "        D_wrong_loss = 0\n",
    "        G_loss = 0\n",
    "        KL_l = 0\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            if config.text_encoder != \"text-cnn-rnn\":\n",
    "                with torch.no_grad():\n",
    "                    text_embeddings = text_encoder(batch['input_ids'], batch['attention_mask'])\n",
    "                text_embeddings = text_embeddings.float().cuda()\n",
    "            else:\n",
    "                text_embeddings = batch['embedding']\n",
    "            real_images = batch['image']\n",
    "            caption = None\n",
    "            if 'caption' in batch:\n",
    "                caption = batch['caption']\n",
    "\n",
    "            noise.data.normal_(0,1)\n",
    "            # noise = torch.randn((batch_size, noise_dim)).float().cuda()\n",
    "            \n",
    "            low_res = None\n",
    "            if stage == 1:\n",
    "                # Generate fake image\n",
    "                inputs = (text_embeddings, noise)\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 64, f\"Image size {fake_images.shape[-1]} differs from 64\"\n",
    "            \n",
    "            else:\n",
    "                # Generate fake image\n",
    "                s1_inputs = (text_embeddings, noise)\n",
    "                with torch.no_grad():\n",
    "                    low_res, _, _ = nn.parallel.data_parallel(stage1_G, s1_inputs, gpus)\n",
    "                # pass stage 1 output to generator\n",
    "                s2_inputs = (text_embeddings, low_res.detach())\n",
    "                fake_images, mu, logvar = nn.parallel.data_parallel(netG, s2_inputs, gpus)\n",
    "                assert fake_images.shape[-1] == 256, f\"Image size {fake_images.shape[-1]} differs from 256\"\n",
    "\n",
    "            rlabels = real_labels.clone()\n",
    "            flabels = fake_labels.clone()\n",
    "            # label smoothing and noisy labelling\n",
    "            if config.label_smoothening:\n",
    "                r = np.random.rand(1)[0]\n",
    "                if r <= 0.5:\n",
    "                    smoothening = np.random.choice(a=np.linspace(0., 0.20, num=5), replace=True, size=batch_size)\n",
    "                    smoothening = torch.tensor(smoothening).float().cuda()\n",
    "                    rlabels -= smoothening\n",
    "                    flabels += smoothening\n",
    "                # occasionally flip the labels\n",
    "                else:\n",
    "                    rlabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.05,0.95])\n",
    "                    flabels = np.random.choice(a=[0.,1.], replace=True, size=batch_size, p=[0.95,0.05])\n",
    "                    rlabels = torch.tensor(rlabels).float().cuda()\n",
    "                    flabels = torch.tensor(flabels).float().cuda()\n",
    "\n",
    "            # Update discriminator network\n",
    "            netD.zero_grad()\n",
    "            errD, errD_real, errD_wrong, errD_fake = compute_discriminator_loss(netD, criterion, real_images, fake_images,\n",
    "                                                                                rlabels, flabels, text_embeddings, gpus)\n",
    "            \n",
    "            # errD.backward()\n",
    "            errD_real.backward(retain_graph=True)\n",
    "            errD_fake_wrong = (errD_wrong + errD_fake) * 0.5\n",
    "            errD_fake_wrong.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netD.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerD.step()\n",
    "            D_loss += errD.item()\n",
    "            D_real_loss += errD_real.item()\n",
    "            D_fake_loss += errD_fake.item()\n",
    "            D_wrong_loss += errD_wrong.item()\n",
    "\n",
    "            # Update generator network\n",
    "            netG.zero_grad()\n",
    "            errG_fake = compute_generator_loss(netD, criterion, fake_images, real_images, \n",
    "                                               real_labels, text_embeddings, gpus)\n",
    "            kl_loss = KL_loss(mu, logvar)\n",
    "            errG_total = errG_fake +  (KL_coeff * kl_loss)\n",
    "            if alpha_L1 > 0:\n",
    "                errG_L1 = L1_loss(L1Loss, fake_images, real_images)\n",
    "                errG_total += (alpha_L1 * errG_L1)\n",
    "            \n",
    "            # annealing KL_coeff and L1_loss\n",
    "            # if epoch -1 < 30:\n",
    "            #     kld_coeff = KL_coeff[epoch-1].item()\n",
    "            #     l1_coeff = alpha_l1[epoch-1].item()\n",
    "            # else:\n",
    "            #     kld_coeff = KL_coeff[-1].item()\n",
    "            #     l1_coeff = alpha_l1[-1].item()\n",
    "\n",
    "            errG_total.backward()\n",
    "\n",
    "            # Gradient Norm Clipping\n",
    "            if config.clip_grad:\n",
    "                nn.utils.clip_grad_norm_(netG.parameters(), max_norm=2.0, norm_type=2)\n",
    "\n",
    "            optimizerG.step()\n",
    "            G_loss += errG_total.item()\n",
    "            KL_l += kl_loss.item()\n",
    "\n",
    "            running_count += 1\n",
    "            if i%100 == 0:\n",
    "                print('[%d/%d] [%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, i, len(trainloader), errD.item(), errG_total.item(), kl_loss.item()))\n",
    "                save_img_results(real_images, fake_images, low_res,  caption, epoch, config.out_img)\n",
    "\n",
    "        summary.add_scalars('Discriminator', {'DLoss':D_loss/len(trainloader), \n",
    "                                              'RealLoss':D_real_loss/len(trainloader), \n",
    "                                              'FakeLoss':D_fake_loss/len(trainloader), \n",
    "                                              'WrongLoss':D_wrong_loss/len(trainloader)}, epoch)\n",
    "        summary.add_scalars('Generator', {'GLoss':G_loss/len(trainloader), \n",
    "                                          'KL_Loss':KL_l/len(trainloader)}, epoch)\n",
    "        # summary.add_scalars('Grad_Norm', {'D':np.mean(d_grad_norm),\n",
    "        #                                   'G':np.mean(g_grad_norm)}, epoch) \n",
    "        print('[%d/%d] Loss_D: %.5f, Loss_G: %.5f, Loss_KL: %.5f' % (epoch, config.max_epoch, D_loss/len(trainloader), G_loss/len(trainloader), KL_l/len(trainloader)))\n",
    "        \n",
    "        schedulerD.step()\n",
    "        schedulerG.step()\n",
    "        \n",
    "        if epoch % config.save_snapshot == 0:\n",
    "            save_model(netG, netD, epoch, config.model_out, stage=stage) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# call model.eval() before feeding the data, as this will change the behavior of the BatchNorm layer \n",
    "# to use the running estimates instead of calculating them for the current batch\n",
    "# \"\"\"\n",
    "# netG1.eval()\n",
    "# netD1.eval()\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 64\n",
      "using text-cnn-rnn as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 10610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=1\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, stage 1 discriminator\n",
      "generator loaded from text-cnn-rnn/out_II/checkpoint_s1_ls_biasF/netG1_epoch_400.pth, starting at epoch: 400\n",
      "discriminator loaded from text-cnn-rnn/out_II/checkpoint_s1_ls_biasF/netD1_epoch_400.pth, starting at epoch: 400\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Traininig Stage: 1, outputs at: text-cnn-rnn/out_II/tensorboard_s1_ls_biasF, text-cnn-rnn/out_II/results_s1_ls_biasF, text-cnn-rnn/out_II/checkpoint_s1_ls_biasF\n",
      "[401/600] [0/331] Loss_D: 0.55524, Loss_G: 2.71915, Loss_KL: 0.32824\n",
      "[401/600] [100/331] Loss_D: 0.65250, Loss_G: 2.98563, Loss_KL: 0.30327\n",
      "[401/600] [200/331] Loss_D: 0.44875, Loss_G: 2.99873, Loss_KL: 0.40953\n",
      "[401/600] [300/331] Loss_D: 0.69367, Loss_G: 3.08154, Loss_KL: 0.34526\n",
      "[401/600] Loss_D: 0.59165, Loss_G: 2.82080, Loss_KL: 0.34587\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[402/600] [0/331] Loss_D: 0.74118, Loss_G: 2.22545, Loss_KL: 0.38751\n",
      "[402/600] [100/331] Loss_D: 0.57758, Loss_G: 2.02501, Loss_KL: 0.28075\n",
      "[402/600] [200/331] Loss_D: 0.24073, Loss_G: 3.01497, Loss_KL: 0.31310\n",
      "[402/600] [300/331] Loss_D: 0.24078, Loss_G: 3.26954, Loss_KL: 0.25444\n",
      "[402/600] Loss_D: 0.57928, Loss_G: 2.81378, Loss_KL: 0.34564\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[403/600] [0/331] Loss_D: 0.58401, Loss_G: 3.65261, Loss_KL: 0.35998\n",
      "[403/600] [100/331] Loss_D: 0.69064, Loss_G: 3.03546, Loss_KL: 0.47931\n",
      "[403/600] [200/331] Loss_D: 0.23404, Loss_G: 3.28920, Loss_KL: 0.28638\n",
      "[403/600] [300/331] Loss_D: 0.31497, Loss_G: 3.09751, Loss_KL: 0.47154\n",
      "[403/600] Loss_D: 0.59418, Loss_G: 2.80612, Loss_KL: 0.34294\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[404/600] [0/331] Loss_D: 0.74015, Loss_G: 2.28522, Loss_KL: 0.32934\n",
      "[404/600] [100/331] Loss_D: 0.66784, Loss_G: 2.31739, Loss_KL: 0.39406\n",
      "[404/600] [200/331] Loss_D: 0.30045, Loss_G: 2.78448, Loss_KL: 0.35414\n",
      "[404/600] [300/331] Loss_D: 0.54849, Loss_G: 2.43448, Loss_KL: 0.32377\n",
      "[404/600] Loss_D: 0.59448, Loss_G: 2.77760, Loss_KL: 0.34518\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[405/600] [0/331] Loss_D: 0.40534, Loss_G: 3.07981, Loss_KL: 0.39424\n",
      "[405/600] [100/331] Loss_D: 0.27954, Loss_G: 2.94566, Loss_KL: 0.33173\n",
      "[405/600] [200/331] Loss_D: 0.67191, Loss_G: 3.22386, Loss_KL: 0.28475\n",
      "[405/600] [300/331] Loss_D: 0.69753, Loss_G: 2.77268, Loss_KL: 0.33541\n",
      "[405/600] Loss_D: 0.59249, Loss_G: 2.77816, Loss_KL: 0.34399\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[406/600] [0/331] Loss_D: 0.40838, Loss_G: 2.79599, Loss_KL: 0.35189\n",
      "[406/600] [100/331] Loss_D: 0.36635, Loss_G: 3.88674, Loss_KL: 0.28254\n",
      "[406/600] [200/331] Loss_D: 0.74098, Loss_G: 1.98687, Loss_KL: 0.37322\n",
      "[406/600] [300/331] Loss_D: 0.62363, Loss_G: 2.42531, Loss_KL: 0.37856\n",
      "[406/600] Loss_D: 0.60388, Loss_G: 2.73916, Loss_KL: 0.34388\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[407/600] [0/331] Loss_D: 0.62922, Loss_G: 2.92048, Loss_KL: 0.37090\n",
      "[407/600] [100/331] Loss_D: 0.64721, Loss_G: 3.15178, Loss_KL: 0.29954\n",
      "[407/600] [200/331] Loss_D: 0.29411, Loss_G: 2.54808, Loss_KL: 0.37187\n",
      "[407/600] [300/331] Loss_D: 0.28206, Loss_G: 2.60874, Loss_KL: 0.34344\n",
      "[407/600] Loss_D: 0.60897, Loss_G: 2.73446, Loss_KL: 0.34969\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[408/600] [0/331] Loss_D: 0.72072, Loss_G: 3.12401, Loss_KL: 0.33487\n",
      "[408/600] [100/331] Loss_D: 0.73754, Loss_G: 3.04010, Loss_KL: 0.32341\n",
      "[408/600] [200/331] Loss_D: 0.67761, Loss_G: 2.89395, Loss_KL: 0.41370\n",
      "[408/600] [300/331] Loss_D: 0.70726, Loss_G: 2.47031, Loss_KL: 0.34834\n",
      "[408/600] Loss_D: 0.60348, Loss_G: 2.79069, Loss_KL: 0.34695\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[409/600] [0/331] Loss_D: 0.33688, Loss_G: 2.80997, Loss_KL: 0.37536\n",
      "[409/600] [100/331] Loss_D: 0.80800, Loss_G: 2.54341, Loss_KL: 0.31040\n",
      "[409/600] [200/331] Loss_D: 0.74533, Loss_G: 2.05895, Loss_KL: 0.42930\n",
      "[409/600] [300/331] Loss_D: 0.37682, Loss_G: 2.52754, Loss_KL: 0.30605\n",
      "[409/600] Loss_D: 0.59731, Loss_G: 2.75602, Loss_KL: 0.34807\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[410/600] [0/331] Loss_D: 0.71193, Loss_G: 3.29523, Loss_KL: 0.35782\n",
      "[410/600] [100/331] Loss_D: 0.69416, Loss_G: 2.45701, Loss_KL: 0.39556\n",
      "[410/600] [200/331] Loss_D: 0.78947, Loss_G: 2.69012, Loss_KL: 0.29109\n",
      "[410/600] [300/331] Loss_D: 0.49994, Loss_G: 4.25594, Loss_KL: 0.31612\n",
      "[410/600] Loss_D: 0.57860, Loss_G: 2.82387, Loss_KL: 0.34831\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[411/600] [0/331] Loss_D: 0.76791, Loss_G: 2.36702, Loss_KL: 0.36811\n",
      "[411/600] [100/331] Loss_D: 0.33554, Loss_G: 3.27590, Loss_KL: 0.43768\n",
      "[411/600] [200/331] Loss_D: 0.34953, Loss_G: 2.17741, Loss_KL: 0.39467\n",
      "[411/600] [300/331] Loss_D: 0.72963, Loss_G: 2.79216, Loss_KL: 0.30565\n",
      "[411/600] Loss_D: 0.61333, Loss_G: 2.78577, Loss_KL: 0.34761\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[412/600] [0/331] Loss_D: 0.77919, Loss_G: 2.78957, Loss_KL: 0.34953\n",
      "[412/600] [100/331] Loss_D: 0.85293, Loss_G: 2.90514, Loss_KL: 0.41050\n",
      "[412/600] [200/331] Loss_D: 0.77240, Loss_G: 3.45191, Loss_KL: 0.40490\n",
      "[412/600] [300/331] Loss_D: 0.78918, Loss_G: 3.06220, Loss_KL: 0.39042\n",
      "[412/600] Loss_D: 0.61003, Loss_G: 2.73205, Loss_KL: 0.35021\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[413/600] [0/331] Loss_D: 0.69194, Loss_G: 1.93091, Loss_KL: 0.32678\n",
      "[413/600] [100/331] Loss_D: 0.57729, Loss_G: 2.39855, Loss_KL: 0.41649\n",
      "[413/600] [200/331] Loss_D: 0.63323, Loss_G: 2.64610, Loss_KL: 0.35565\n",
      "[413/600] [300/331] Loss_D: 0.67653, Loss_G: 2.24651, Loss_KL: 0.31629\n",
      "[413/600] Loss_D: 0.59631, Loss_G: 2.75952, Loss_KL: 0.35056\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[414/600] [0/331] Loss_D: 0.52562, Loss_G: 2.44161, Loss_KL: 0.32375\n",
      "[414/600] [100/331] Loss_D: 0.30620, Loss_G: 2.61968, Loss_KL: 0.34681\n",
      "[414/600] [200/331] Loss_D: 0.68973, Loss_G: 2.31846, Loss_KL: 0.40869\n",
      "[414/600] [300/331] Loss_D: 0.70021, Loss_G: 2.54281, Loss_KL: 0.36203\n",
      "[414/600] Loss_D: 0.59212, Loss_G: 2.76080, Loss_KL: 0.34262\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[415/600] [0/331] Loss_D: 0.49887, Loss_G: 2.58316, Loss_KL: 0.30862\n",
      "[415/600] [100/331] Loss_D: 0.62078, Loss_G: 3.35923, Loss_KL: 0.30902\n",
      "[415/600] [200/331] Loss_D: 0.76520, Loss_G: 2.34551, Loss_KL: 0.40073\n",
      "[415/600] [300/331] Loss_D: 0.70095, Loss_G: 2.52371, Loss_KL: 0.36632\n",
      "[415/600] Loss_D: 0.59752, Loss_G: 2.80160, Loss_KL: 0.34346\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[416/600] [0/331] Loss_D: 0.67339, Loss_G: 2.57919, Loss_KL: 0.36440\n",
      "[416/600] [100/331] Loss_D: 0.24270, Loss_G: 3.56968, Loss_KL: 0.36045\n",
      "[416/600] [200/331] Loss_D: 0.63787, Loss_G: 2.26172, Loss_KL: 0.40700\n",
      "[416/600] [300/331] Loss_D: 0.67712, Loss_G: 2.79452, Loss_KL: 0.32746\n",
      "[416/600] Loss_D: 0.58928, Loss_G: 2.80649, Loss_KL: 0.34556\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[417/600] [0/331] Loss_D: 0.47553, Loss_G: 2.10725, Loss_KL: 0.40739\n",
      "[417/600] [100/331] Loss_D: 0.51756, Loss_G: 2.47203, Loss_KL: 0.37182\n",
      "[417/600] [200/331] Loss_D: 0.67026, Loss_G: 2.81243, Loss_KL: 0.36213\n",
      "[417/600] [300/331] Loss_D: 0.74284, Loss_G: 2.62762, Loss_KL: 0.34390\n",
      "[417/600] Loss_D: 0.59035, Loss_G: 2.84639, Loss_KL: 0.34503\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[418/600] [0/331] Loss_D: 0.72928, Loss_G: 2.45479, Loss_KL: 0.31020\n",
      "[418/600] [100/331] Loss_D: 0.65190, Loss_G: 2.51546, Loss_KL: 0.30391\n",
      "[418/600] [200/331] Loss_D: 0.66093, Loss_G: 3.16301, Loss_KL: 0.40854\n",
      "[418/600] [300/331] Loss_D: 0.76713, Loss_G: 2.73724, Loss_KL: 0.25737\n",
      "[418/600] Loss_D: 0.59065, Loss_G: 2.80233, Loss_KL: 0.34177\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[419/600] [0/331] Loss_D: 0.34513, Loss_G: 2.94041, Loss_KL: 0.37425\n",
      "[419/600] [100/331] Loss_D: 0.67495, Loss_G: 3.46737, Loss_KL: 0.40110\n",
      "[419/600] [200/331] Loss_D: 0.79123, Loss_G: 1.98502, Loss_KL: 0.33341\n",
      "[419/600] [300/331] Loss_D: 0.71570, Loss_G: 2.51840, Loss_KL: 0.35522\n",
      "[419/600] Loss_D: 0.59448, Loss_G: 2.81843, Loss_KL: 0.34475\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[420/600] [0/331] Loss_D: 0.75234, Loss_G: 3.19572, Loss_KL: 0.40363\n",
      "[420/600] [100/331] Loss_D: 0.40848, Loss_G: 2.95887, Loss_KL: 0.36764\n",
      "[420/600] [200/331] Loss_D: 0.73213, Loss_G: 3.14929, Loss_KL: 0.33725\n",
      "[420/600] [300/331] Loss_D: 0.58105, Loss_G: 2.95548, Loss_KL: 0.34826\n",
      "[420/600] Loss_D: 0.59217, Loss_G: 2.80407, Loss_KL: 0.34742\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Save G1/D1 models\n",
      "[421/600] [0/331] Loss_D: 0.42190, Loss_G: 2.72207, Loss_KL: 0.35232\n",
      "[421/600] [100/331] Loss_D: 0.29093, Loss_G: 2.10441, Loss_KL: 0.38627\n",
      "[421/600] [200/331] Loss_D: 0.70742, Loss_G: 2.76300, Loss_KL: 0.36059\n",
      "[421/600] [300/331] Loss_D: 0.74957, Loss_G: 2.82020, Loss_KL: 0.37198\n",
      "[421/600] Loss_D: 0.60288, Loss_G: 2.74620, Loss_KL: 0.34113\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[422/600] [0/331] Loss_D: 0.57667, Loss_G: 2.86132, Loss_KL: 0.33528\n",
      "[422/600] [100/331] Loss_D: 0.69962, Loss_G: 3.28591, Loss_KL: 0.30041\n",
      "[422/600] [200/331] Loss_D: 0.89553, Loss_G: 2.18176, Loss_KL: 0.34076\n",
      "[422/600] [300/331] Loss_D: 0.67610, Loss_G: 3.14753, Loss_KL: 0.43229\n",
      "[422/600] Loss_D: 0.61001, Loss_G: 2.82651, Loss_KL: 0.34132\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[423/600] [0/331] Loss_D: 0.67240, Loss_G: 2.42355, Loss_KL: 0.38143\n",
      "[423/600] [100/331] Loss_D: 0.53849, Loss_G: 3.01271, Loss_KL: 0.33773\n",
      "[423/600] [200/331] Loss_D: 0.68828, Loss_G: 2.69752, Loss_KL: 0.39644\n",
      "[423/600] [300/331] Loss_D: 0.37080, Loss_G: 2.71999, Loss_KL: 0.36921\n",
      "[423/600] Loss_D: 0.59997, Loss_G: 2.82697, Loss_KL: 0.34518\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[424/600] [0/331] Loss_D: 0.80636, Loss_G: 2.63728, Loss_KL: 0.32296\n",
      "[424/600] [100/331] Loss_D: 0.69514, Loss_G: 4.15919, Loss_KL: 0.32304\n",
      "[424/600] [200/331] Loss_D: 0.50347, Loss_G: 3.30967, Loss_KL: 0.26980\n",
      "[424/600] [300/331] Loss_D: 0.69531, Loss_G: 2.72551, Loss_KL: 0.36895\n",
      "[424/600] Loss_D: 0.59617, Loss_G: 2.82214, Loss_KL: 0.34080\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[425/600] [0/331] Loss_D: 0.71764, Loss_G: 2.25213, Loss_KL: 0.35337\n",
      "[425/600] [100/331] Loss_D: 0.78630, Loss_G: 2.62010, Loss_KL: 0.27297\n",
      "[425/600] [200/331] Loss_D: 0.80926, Loss_G: 2.87686, Loss_KL: 0.42616\n",
      "[425/600] [300/331] Loss_D: 0.26420, Loss_G: 2.92426, Loss_KL: 0.39973\n",
      "[425/600] Loss_D: 0.60017, Loss_G: 2.80454, Loss_KL: 0.34729\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[426/600] [0/331] Loss_D: 0.77300, Loss_G: 3.38608, Loss_KL: 0.33509\n",
      "[426/600] [100/331] Loss_D: 0.72655, Loss_G: 2.63044, Loss_KL: 0.36990\n",
      "[426/600] [200/331] Loss_D: 0.34517, Loss_G: 3.21754, Loss_KL: 0.37886\n",
      "[426/600] [300/331] Loss_D: 0.77352, Loss_G: 2.56004, Loss_KL: 0.33878\n",
      "[426/600] Loss_D: 0.57603, Loss_G: 2.83574, Loss_KL: 0.34726\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[427/600] [0/331] Loss_D: 0.40156, Loss_G: 3.19810, Loss_KL: 0.42119\n",
      "[427/600] [100/331] Loss_D: 0.29945, Loss_G: 2.52968, Loss_KL: 0.32694\n",
      "[427/600] [200/331] Loss_D: 0.83924, Loss_G: 2.58983, Loss_KL: 0.26413\n",
      "[427/600] [300/331] Loss_D: 0.44211, Loss_G: 2.61328, Loss_KL: 0.38780\n",
      "[427/600] Loss_D: 0.59338, Loss_G: 2.80151, Loss_KL: 0.34892\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[428/600] [0/331] Loss_D: 0.60812, Loss_G: 2.74101, Loss_KL: 0.35533\n",
      "[428/600] [100/331] Loss_D: 0.72687, Loss_G: 2.66850, Loss_KL: 0.31706\n",
      "[428/600] [200/331] Loss_D: 0.38827, Loss_G: 2.78686, Loss_KL: 0.36820\n",
      "[428/600] [300/331] Loss_D: 0.31712, Loss_G: 3.41301, Loss_KL: 0.41377\n",
      "[428/600] Loss_D: 0.58909, Loss_G: 2.82618, Loss_KL: 0.34809\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[429/600] [0/331] Loss_D: 0.49623, Loss_G: 2.60128, Loss_KL: 0.26821\n",
      "[429/600] [100/331] Loss_D: 0.52461, Loss_G: 2.85733, Loss_KL: 0.30664\n",
      "[429/600] [200/331] Loss_D: 0.64749, Loss_G: 2.69203, Loss_KL: 0.34105\n",
      "[429/600] [300/331] Loss_D: 0.25820, Loss_G: 3.13289, Loss_KL: 0.36029\n",
      "[429/600] Loss_D: 0.61051, Loss_G: 2.78926, Loss_KL: 0.34923\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[430/600] [0/331] Loss_D: 0.39394, Loss_G: 2.74995, Loss_KL: 0.37116\n",
      "[430/600] [100/331] Loss_D: 0.66010, Loss_G: 3.15785, Loss_KL: 0.44141\n",
      "[430/600] [200/331] Loss_D: 0.45168, Loss_G: 2.66157, Loss_KL: 0.30486\n",
      "[430/600] [300/331] Loss_D: 0.52976, Loss_G: 3.32986, Loss_KL: 0.27254\n",
      "[430/600] Loss_D: 0.61352, Loss_G: 2.74206, Loss_KL: 0.34956\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[431/600] [0/331] Loss_D: 0.76297, Loss_G: 2.43123, Loss_KL: 0.38151\n",
      "[431/600] [100/331] Loss_D: 0.69298, Loss_G: 2.77618, Loss_KL: 0.33846\n",
      "[431/600] [200/331] Loss_D: 0.31321, Loss_G: 3.04588, Loss_KL: 0.34122\n",
      "[431/600] [300/331] Loss_D: 0.57740, Loss_G: 3.04384, Loss_KL: 0.32631\n",
      "[431/600] Loss_D: 0.60158, Loss_G: 2.77019, Loss_KL: 0.34884\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[432/600] [0/331] Loss_D: 0.71967, Loss_G: 2.76605, Loss_KL: 0.33015\n",
      "[432/600] [100/331] Loss_D: 0.53219, Loss_G: 1.83459, Loss_KL: 0.31401\n",
      "[432/600] [200/331] Loss_D: 0.66342, Loss_G: 2.38305, Loss_KL: 0.30216\n",
      "[432/600] [300/331] Loss_D: 0.61966, Loss_G: 3.11608, Loss_KL: 0.33038\n",
      "[432/600] Loss_D: 0.58630, Loss_G: 2.75677, Loss_KL: 0.35000\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[433/600] [0/331] Loss_D: 0.75159, Loss_G: 2.97790, Loss_KL: 0.36374\n",
      "[433/600] [100/331] Loss_D: 0.37584, Loss_G: 3.10438, Loss_KL: 0.29747\n",
      "[433/600] [200/331] Loss_D: 0.45797, Loss_G: 3.79104, Loss_KL: 0.32520\n",
      "[433/600] [300/331] Loss_D: 0.66582, Loss_G: 2.33181, Loss_KL: 0.31174\n",
      "[433/600] Loss_D: 0.60844, Loss_G: 2.78030, Loss_KL: 0.34697\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[434/600] [0/331] Loss_D: 0.40140, Loss_G: 3.18016, Loss_KL: 0.43684\n",
      "[434/600] [100/331] Loss_D: 0.62252, Loss_G: 2.32241, Loss_KL: 0.32137\n",
      "[434/600] [200/331] Loss_D: 0.59577, Loss_G: 2.57243, Loss_KL: 0.29818\n",
      "[434/600] [300/331] Loss_D: 0.61381, Loss_G: 2.25925, Loss_KL: 0.30033\n",
      "[434/600] Loss_D: 0.58652, Loss_G: 2.79536, Loss_KL: 0.35055\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[435/600] [0/331] Loss_D: 0.62394, Loss_G: 2.60435, Loss_KL: 0.41885\n",
      "[435/600] [100/331] Loss_D: 0.82460, Loss_G: 3.71860, Loss_KL: 0.38331\n",
      "[435/600] [200/331] Loss_D: 0.39430, Loss_G: 2.71836, Loss_KL: 0.30884\n",
      "[435/600] [300/331] Loss_D: 0.21289, Loss_G: 2.75729, Loss_KL: 0.30097\n",
      "[435/600] Loss_D: 0.57940, Loss_G: 2.81035, Loss_KL: 0.34951\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[436/600] [0/331] Loss_D: 0.46227, Loss_G: 2.70240, Loss_KL: 0.34609\n",
      "[436/600] [100/331] Loss_D: 0.69402, Loss_G: 2.96828, Loss_KL: 0.34090\n",
      "[436/600] [200/331] Loss_D: 0.65632, Loss_G: 2.42795, Loss_KL: 0.32775\n",
      "[436/600] [300/331] Loss_D: 0.29856, Loss_G: 3.64500, Loss_KL: 0.33014\n",
      "[436/600] Loss_D: 0.58941, Loss_G: 2.82491, Loss_KL: 0.34491\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[437/600] [0/331] Loss_D: 0.26982, Loss_G: 2.57232, Loss_KL: 0.36036\n",
      "[437/600] [100/331] Loss_D: 0.53610, Loss_G: 2.90215, Loss_KL: 0.45906\n",
      "[437/600] [200/331] Loss_D: 0.49955, Loss_G: 4.19432, Loss_KL: 0.34984\n",
      "[437/600] [300/331] Loss_D: 0.73024, Loss_G: 3.25713, Loss_KL: 0.32904\n",
      "[437/600] Loss_D: 0.61262, Loss_G: 2.82349, Loss_KL: 0.35002\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[438/600] [0/331] Loss_D: 0.46417, Loss_G: 3.39345, Loss_KL: 0.32143\n",
      "[438/600] [100/331] Loss_D: 0.80550, Loss_G: 2.56311, Loss_KL: 0.29656\n",
      "[438/600] [200/331] Loss_D: 0.79405, Loss_G: 3.07286, Loss_KL: 0.26210\n",
      "[438/600] [300/331] Loss_D: 0.70970, Loss_G: 2.60212, Loss_KL: 0.31789\n",
      "[438/600] Loss_D: 0.60029, Loss_G: 2.79394, Loss_KL: 0.35400\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[439/600] [0/331] Loss_D: 0.58591, Loss_G: 2.44857, Loss_KL: 0.37313\n",
      "[439/600] [100/331] Loss_D: 0.72190, Loss_G: 2.99553, Loss_KL: 0.39828\n",
      "[439/600] [200/331] Loss_D: 0.42202, Loss_G: 2.15071, Loss_KL: 0.37168\n",
      "[439/600] [300/331] Loss_D: 0.64821, Loss_G: 2.80649, Loss_KL: 0.32409\n",
      "[439/600] Loss_D: 0.59817, Loss_G: 2.80662, Loss_KL: 0.35379\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[440/600] [0/331] Loss_D: 0.68302, Loss_G: 2.22799, Loss_KL: 0.44490\n",
      "[440/600] [100/331] Loss_D: 0.40483, Loss_G: 2.75455, Loss_KL: 0.34338\n",
      "[440/600] [200/331] Loss_D: 0.24126, Loss_G: 3.17703, Loss_KL: 0.33164\n",
      "[440/600] [300/331] Loss_D: 0.32019, Loss_G: 2.59971, Loss_KL: 0.45926\n",
      "[440/600] Loss_D: 0.57551, Loss_G: 2.91595, Loss_KL: 0.35729\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Save G1/D1 models\n",
      "[441/600] [0/331] Loss_D: 0.73626, Loss_G: 2.83275, Loss_KL: 0.35996\n",
      "[441/600] [100/331] Loss_D: 0.39615, Loss_G: 2.56177, Loss_KL: 0.43738\n",
      "[441/600] [200/331] Loss_D: 0.79712, Loss_G: 2.63067, Loss_KL: 0.29751\n",
      "[441/600] [300/331] Loss_D: 0.84191, Loss_G: 3.89334, Loss_KL: 0.31282\n",
      "[441/600] Loss_D: 0.59733, Loss_G: 2.83050, Loss_KL: 0.35250\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[442/600] [0/331] Loss_D: 0.31723, Loss_G: 2.85849, Loss_KL: 0.37704\n",
      "[442/600] [100/331] Loss_D: 0.59250, Loss_G: 2.35196, Loss_KL: 0.31698\n",
      "[442/600] [200/331] Loss_D: 0.46240, Loss_G: 3.03900, Loss_KL: 0.27721\n",
      "[442/600] [300/331] Loss_D: 0.59333, Loss_G: 2.69265, Loss_KL: 0.27409\n",
      "[442/600] Loss_D: 0.60760, Loss_G: 2.75794, Loss_KL: 0.35690\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[443/600] [0/331] Loss_D: 0.52232, Loss_G: 2.31260, Loss_KL: 0.39840\n",
      "[443/600] [100/331] Loss_D: 0.55005, Loss_G: 2.55287, Loss_KL: 0.35895\n",
      "[443/600] [200/331] Loss_D: 0.21045, Loss_G: 2.48316, Loss_KL: 0.37921\n",
      "[443/600] [300/331] Loss_D: 0.85021, Loss_G: 3.48533, Loss_KL: 0.45061\n",
      "[443/600] Loss_D: 0.61330, Loss_G: 2.83014, Loss_KL: 0.35674\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[444/600] [0/331] Loss_D: 0.72141, Loss_G: 2.64734, Loss_KL: 0.37960\n",
      "[444/600] [100/331] Loss_D: 0.58298, Loss_G: 2.77793, Loss_KL: 0.32447\n",
      "[444/600] [200/331] Loss_D: 0.68363, Loss_G: 2.97271, Loss_KL: 0.44303\n",
      "[444/600] [300/331] Loss_D: 0.52401, Loss_G: 2.69910, Loss_KL: 0.34028\n",
      "[444/600] Loss_D: 0.62378, Loss_G: 2.76393, Loss_KL: 0.35348\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[445/600] [0/331] Loss_D: 0.61210, Loss_G: 2.70535, Loss_KL: 0.42301\n",
      "[445/600] [100/331] Loss_D: 0.51766, Loss_G: 2.75825, Loss_KL: 0.34199\n",
      "[445/600] [200/331] Loss_D: 0.85376, Loss_G: 3.34324, Loss_KL: 0.37115\n",
      "[445/600] [300/331] Loss_D: 0.29513, Loss_G: 3.36820, Loss_KL: 0.39153\n",
      "[445/600] Loss_D: 0.60335, Loss_G: 2.77544, Loss_KL: 0.35947\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[446/600] [0/331] Loss_D: 0.64958, Loss_G: 3.50565, Loss_KL: 0.30583\n",
      "[446/600] [100/331] Loss_D: 0.71099, Loss_G: 3.21733, Loss_KL: 0.38840\n",
      "[446/600] [200/331] Loss_D: 0.68890, Loss_G: 2.86735, Loss_KL: 0.30563\n",
      "[446/600] [300/331] Loss_D: 0.46523, Loss_G: 2.49506, Loss_KL: 0.31861\n",
      "[446/600] Loss_D: 0.60095, Loss_G: 2.80100, Loss_KL: 0.35961\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[447/600] [0/331] Loss_D: 0.75320, Loss_G: 2.81108, Loss_KL: 0.33433\n",
      "[447/600] [100/331] Loss_D: 0.71243, Loss_G: 2.18524, Loss_KL: 0.33635\n",
      "[447/600] [200/331] Loss_D: 0.46264, Loss_G: 2.65775, Loss_KL: 0.36254\n",
      "[447/600] [300/331] Loss_D: 0.27168, Loss_G: 2.76085, Loss_KL: 0.34039\n",
      "[447/600] Loss_D: 0.60406, Loss_G: 2.80670, Loss_KL: 0.35803\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[448/600] [0/331] Loss_D: 0.42983, Loss_G: 3.37869, Loss_KL: 0.29335\n",
      "[448/600] [100/331] Loss_D: 0.39372, Loss_G: 2.67307, Loss_KL: 0.40771\n",
      "[448/600] [200/331] Loss_D: 0.74213, Loss_G: 2.38851, Loss_KL: 0.35798\n",
      "[448/600] [300/331] Loss_D: 0.59908, Loss_G: 2.78690, Loss_KL: 0.40294\n",
      "[448/600] Loss_D: 0.59094, Loss_G: 2.85839, Loss_KL: 0.35703\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[449/600] [0/331] Loss_D: 0.53690, Loss_G: 3.05226, Loss_KL: 0.41166\n",
      "[449/600] [100/331] Loss_D: 0.39698, Loss_G: 2.85398, Loss_KL: 0.34103\n",
      "[449/600] [200/331] Loss_D: 0.24000, Loss_G: 2.50609, Loss_KL: 0.35335\n",
      "[449/600] [300/331] Loss_D: 0.58499, Loss_G: 2.96282, Loss_KL: 0.37134\n",
      "[449/600] Loss_D: 0.60775, Loss_G: 2.82469, Loss_KL: 0.35805\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[450/600] [0/331] Loss_D: 0.73646, Loss_G: 2.28879, Loss_KL: 0.45244\n",
      "[450/600] [100/331] Loss_D: 0.85110, Loss_G: 2.92332, Loss_KL: 0.32004\n",
      "[450/600] [200/331] Loss_D: 0.71558, Loss_G: 2.05609, Loss_KL: 0.36064\n",
      "[450/600] [300/331] Loss_D: 0.26947, Loss_G: 2.82784, Loss_KL: 0.39840\n",
      "[450/600] Loss_D: 0.58891, Loss_G: 2.85473, Loss_KL: 0.35663\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[451/600] [0/331] Loss_D: 0.63891, Loss_G: 3.26356, Loss_KL: 0.35350\n",
      "[451/600] [100/331] Loss_D: 0.28481, Loss_G: 3.31536, Loss_KL: 0.36732\n",
      "[451/600] [200/331] Loss_D: 0.83045, Loss_G: 2.43351, Loss_KL: 0.36328\n",
      "[451/600] [300/331] Loss_D: 0.30317, Loss_G: 2.74146, Loss_KL: 0.40011\n",
      "[451/600] Loss_D: 0.60255, Loss_G: 2.77753, Loss_KL: 0.35657\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[452/600] [0/331] Loss_D: 0.43869, Loss_G: 3.23708, Loss_KL: 0.40578\n",
      "[452/600] [100/331] Loss_D: 0.73431, Loss_G: 2.02963, Loss_KL: 0.34681\n",
      "[452/600] [200/331] Loss_D: 0.67585, Loss_G: 2.36036, Loss_KL: 0.31944\n",
      "[452/600] [300/331] Loss_D: 0.46389, Loss_G: 2.66821, Loss_KL: 0.34505\n",
      "[452/600] Loss_D: 0.62525, Loss_G: 2.76117, Loss_KL: 0.35510\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[453/600] [0/331] Loss_D: 0.40731, Loss_G: 2.89949, Loss_KL: 0.36141\n",
      "[453/600] [100/331] Loss_D: 0.45965, Loss_G: 2.47535, Loss_KL: 0.41770\n",
      "[453/600] [200/331] Loss_D: 0.46831, Loss_G: 2.82624, Loss_KL: 0.33366\n",
      "[453/600] [300/331] Loss_D: 0.73673, Loss_G: 2.66649, Loss_KL: 0.34782\n",
      "[453/600] Loss_D: 0.59685, Loss_G: 2.78251, Loss_KL: 0.35492\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[454/600] [0/331] Loss_D: 0.70285, Loss_G: 2.39496, Loss_KL: 0.39648\n",
      "[454/600] [100/331] Loss_D: 0.62646, Loss_G: 2.67889, Loss_KL: 0.37745\n",
      "[454/600] [200/331] Loss_D: 0.71231, Loss_G: 3.10319, Loss_KL: 0.40491\n",
      "[454/600] [300/331] Loss_D: 0.35240, Loss_G: 2.63426, Loss_KL: 0.35117\n",
      "[454/600] Loss_D: 0.58507, Loss_G: 2.82579, Loss_KL: 0.35535\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[455/600] [0/331] Loss_D: 0.76153, Loss_G: 2.60885, Loss_KL: 0.40220\n",
      "[455/600] [100/331] Loss_D: 0.33401, Loss_G: 3.32973, Loss_KL: 0.32269\n",
      "[455/600] [200/331] Loss_D: 0.73933, Loss_G: 3.58284, Loss_KL: 0.33638\n",
      "[455/600] [300/331] Loss_D: 0.44970, Loss_G: 2.57227, Loss_KL: 0.37670\n",
      "[455/600] Loss_D: 0.58917, Loss_G: 2.88724, Loss_KL: 0.35189\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[456/600] [0/331] Loss_D: 0.82797, Loss_G: 2.93304, Loss_KL: 0.36245\n",
      "[456/600] [100/331] Loss_D: 0.53451, Loss_G: 3.08010, Loss_KL: 0.29736\n",
      "[456/600] [200/331] Loss_D: 0.32905, Loss_G: 3.18708, Loss_KL: 0.30711\n",
      "[456/600] [300/331] Loss_D: 0.75928, Loss_G: 2.78678, Loss_KL: 0.30947\n",
      "[456/600] Loss_D: 0.60419, Loss_G: 2.77823, Loss_KL: 0.35454\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[457/600] [0/331] Loss_D: 0.61967, Loss_G: 3.40455, Loss_KL: 0.35166\n",
      "[457/600] [100/331] Loss_D: 0.72443, Loss_G: 3.20133, Loss_KL: 0.39689\n",
      "[457/600] [200/331] Loss_D: 0.54945, Loss_G: 2.99512, Loss_KL: 0.34436\n",
      "[457/600] [300/331] Loss_D: 0.37074, Loss_G: 3.01947, Loss_KL: 0.30802\n",
      "[457/600] Loss_D: 0.60200, Loss_G: 2.81329, Loss_KL: 0.35708\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[458/600] [0/331] Loss_D: 0.35930, Loss_G: 2.82923, Loss_KL: 0.37376\n",
      "[458/600] [100/331] Loss_D: 0.77684, Loss_G: 2.21571, Loss_KL: 0.36264\n",
      "[458/600] [200/331] Loss_D: 0.74141, Loss_G: 3.36789, Loss_KL: 0.43725\n",
      "[458/600] [300/331] Loss_D: 0.78972, Loss_G: 3.22530, Loss_KL: 0.33589\n",
      "[458/600] Loss_D: 0.58653, Loss_G: 2.83516, Loss_KL: 0.35243\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[459/600] [0/331] Loss_D: 0.69313, Loss_G: 2.87986, Loss_KL: 0.30815\n",
      "[459/600] [100/331] Loss_D: 0.72221, Loss_G: 3.49763, Loss_KL: 0.36147\n",
      "[459/600] [200/331] Loss_D: 0.77037, Loss_G: 2.84227, Loss_KL: 0.33772\n",
      "[459/600] [300/331] Loss_D: 0.68308, Loss_G: 2.37266, Loss_KL: 0.30226\n",
      "[459/600] Loss_D: 0.58641, Loss_G: 2.83254, Loss_KL: 0.35132\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[460/600] [0/331] Loss_D: 0.41159, Loss_G: 3.13989, Loss_KL: 0.31316\n",
      "[460/600] [100/331] Loss_D: 0.69476, Loss_G: 2.88215, Loss_KL: 0.35707\n",
      "[460/600] [200/331] Loss_D: 0.74038, Loss_G: 2.29877, Loss_KL: 0.36302\n",
      "[460/600] [300/331] Loss_D: 0.39887, Loss_G: 3.12982, Loss_KL: 0.42120\n",
      "[460/600] Loss_D: 0.59007, Loss_G: 2.80825, Loss_KL: 0.35848\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Save G1/D1 models\n",
      "[461/600] [0/331] Loss_D: 0.46025, Loss_G: 3.10605, Loss_KL: 0.36571\n",
      "[461/600] [100/331] Loss_D: 0.62651, Loss_G: 2.85995, Loss_KL: 0.33161\n",
      "[461/600] [200/331] Loss_D: 0.78345, Loss_G: 2.54415, Loss_KL: 0.27447\n",
      "[461/600] [300/331] Loss_D: 0.32760, Loss_G: 2.85466, Loss_KL: 0.32761\n",
      "[461/600] Loss_D: 0.59246, Loss_G: 2.78917, Loss_KL: 0.35056\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[462/600] [0/331] Loss_D: 0.28182, Loss_G: 2.32998, Loss_KL: 0.39299\n",
      "[462/600] [100/331] Loss_D: 0.75705, Loss_G: 2.59877, Loss_KL: 0.32867\n",
      "[462/600] [200/331] Loss_D: 0.67511, Loss_G: 2.69881, Loss_KL: 0.30488\n",
      "[462/600] [300/331] Loss_D: 0.64549, Loss_G: 2.52919, Loss_KL: 0.34775\n",
      "[462/600] Loss_D: 0.59740, Loss_G: 2.76640, Loss_KL: 0.35444\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[463/600] [0/331] Loss_D: 0.40003, Loss_G: 3.39895, Loss_KL: 0.31968\n",
      "[463/600] [100/331] Loss_D: 0.59478, Loss_G: 2.52003, Loss_KL: 0.26723\n",
      "[463/600] [200/331] Loss_D: 0.58082, Loss_G: 2.54663, Loss_KL: 0.31201\n",
      "[463/600] [300/331] Loss_D: 0.55361, Loss_G: 3.15375, Loss_KL: 0.37116\n",
      "[463/600] Loss_D: 0.59235, Loss_G: 2.81265, Loss_KL: 0.35579\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[464/600] [0/331] Loss_D: 0.80462, Loss_G: 2.42169, Loss_KL: 0.39729\n",
      "[464/600] [100/331] Loss_D: 0.30810, Loss_G: 2.55234, Loss_KL: 0.32199\n",
      "[464/600] [200/331] Loss_D: 0.70565, Loss_G: 3.16142, Loss_KL: 0.53644\n",
      "[464/600] [300/331] Loss_D: 0.78590, Loss_G: 2.53793, Loss_KL: 0.40719\n",
      "[464/600] Loss_D: 0.60682, Loss_G: 2.79643, Loss_KL: 0.35587\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[465/600] [0/331] Loss_D: 0.71124, Loss_G: 2.32965, Loss_KL: 0.28476\n",
      "[465/600] [100/331] Loss_D: 0.32028, Loss_G: 3.34961, Loss_KL: 0.28366\n",
      "[465/600] [200/331] Loss_D: 0.56894, Loss_G: 2.73476, Loss_KL: 0.47232\n",
      "[465/600] [300/331] Loss_D: 0.51983, Loss_G: 2.75003, Loss_KL: 0.29281\n",
      "[465/600] Loss_D: 0.57807, Loss_G: 2.83485, Loss_KL: 0.35451\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[466/600] [0/331] Loss_D: 0.34890, Loss_G: 2.41222, Loss_KL: 0.38658\n",
      "[466/600] [100/331] Loss_D: 0.35589, Loss_G: 2.79176, Loss_KL: 0.28090\n",
      "[466/600] [200/331] Loss_D: 0.49581, Loss_G: 3.27123, Loss_KL: 0.44478\n",
      "[466/600] [300/331] Loss_D: 0.51452, Loss_G: 2.80892, Loss_KL: 0.38877\n",
      "[466/600] Loss_D: 0.59532, Loss_G: 2.82037, Loss_KL: 0.35472\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[467/600] [0/331] Loss_D: 0.58547, Loss_G: 2.56145, Loss_KL: 0.39937\n",
      "[467/600] [100/331] Loss_D: 0.64924, Loss_G: 2.85784, Loss_KL: 0.41679\n",
      "[467/600] [200/331] Loss_D: 0.70185, Loss_G: 3.04770, Loss_KL: 0.35305\n",
      "[467/600] [300/331] Loss_D: 0.60048, Loss_G: 3.66539, Loss_KL: 0.30885\n",
      "[467/600] Loss_D: 0.58682, Loss_G: 2.81386, Loss_KL: 0.35724\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[468/600] [0/331] Loss_D: 0.64563, Loss_G: 3.69969, Loss_KL: 0.27321\n",
      "[468/600] [100/331] Loss_D: 0.59949, Loss_G: 2.72202, Loss_KL: 0.27256\n",
      "[468/600] [200/331] Loss_D: 0.68960, Loss_G: 2.25284, Loss_KL: 0.32084\n",
      "[468/600] [300/331] Loss_D: 0.53941, Loss_G: 2.30813, Loss_KL: 0.33958\n",
      "[468/600] Loss_D: 0.59848, Loss_G: 2.82268, Loss_KL: 0.35444\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[469/600] [0/331] Loss_D: 0.66724, Loss_G: 2.86042, Loss_KL: 0.37645\n",
      "[469/600] [100/331] Loss_D: 0.70582, Loss_G: 3.14852, Loss_KL: 0.31458\n",
      "[469/600] [200/331] Loss_D: 0.73809, Loss_G: 2.53719, Loss_KL: 0.31781\n",
      "[469/600] [300/331] Loss_D: 0.29605, Loss_G: 2.05690, Loss_KL: 0.30218\n",
      "[469/600] Loss_D: 0.59486, Loss_G: 2.82300, Loss_KL: 0.35249\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[470/600] [0/331] Loss_D: 0.59145, Loss_G: 2.65152, Loss_KL: 0.30969\n",
      "[470/600] [100/331] Loss_D: 0.71027, Loss_G: 2.57690, Loss_KL: 0.36804\n",
      "[470/600] [200/331] Loss_D: 0.42228, Loss_G: 2.80423, Loss_KL: 0.35793\n",
      "[470/600] [300/331] Loss_D: 0.65704, Loss_G: 3.03147, Loss_KL: 0.33431\n",
      "[470/600] Loss_D: 0.59979, Loss_G: 2.78747, Loss_KL: 0.35357\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[471/600] [0/331] Loss_D: 0.82991, Loss_G: 2.71021, Loss_KL: 0.34613\n",
      "[471/600] [100/331] Loss_D: 0.75020, Loss_G: 2.42356, Loss_KL: 0.38759\n",
      "[471/600] [200/331] Loss_D: 0.33380, Loss_G: 3.03971, Loss_KL: 0.38338\n",
      "[471/600] [300/331] Loss_D: 0.68256, Loss_G: 2.36158, Loss_KL: 0.35402\n",
      "[471/600] Loss_D: 0.60143, Loss_G: 2.73921, Loss_KL: 0.35496\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[472/600] [0/331] Loss_D: 0.43863, Loss_G: 2.77327, Loss_KL: 0.35301\n",
      "[472/600] [100/331] Loss_D: 0.26150, Loss_G: 2.88359, Loss_KL: 0.39437\n",
      "[472/600] [200/331] Loss_D: 0.64105, Loss_G: 3.21049, Loss_KL: 0.34240\n",
      "[472/600] [300/331] Loss_D: 0.69928, Loss_G: 3.30668, Loss_KL: 0.38438\n",
      "[472/600] Loss_D: 0.58704, Loss_G: 2.78505, Loss_KL: 0.35529\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[473/600] [0/331] Loss_D: 0.74541, Loss_G: 1.97267, Loss_KL: 0.38735\n",
      "[473/600] [100/331] Loss_D: 0.69301, Loss_G: 2.29817, Loss_KL: 0.35905\n",
      "[473/600] [200/331] Loss_D: 0.65171, Loss_G: 2.68438, Loss_KL: 0.39489\n",
      "[473/600] [300/331] Loss_D: 0.73722, Loss_G: 2.72861, Loss_KL: 0.35172\n",
      "[473/600] Loss_D: 0.59846, Loss_G: 2.80035, Loss_KL: 0.35462\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[474/600] [0/331] Loss_D: 0.56186, Loss_G: 2.77407, Loss_KL: 0.31725\n",
      "[474/600] [100/331] Loss_D: 0.30775, Loss_G: 2.36929, Loss_KL: 0.34822\n",
      "[474/600] [200/331] Loss_D: 0.60813, Loss_G: 2.31376, Loss_KL: 0.39616\n",
      "[474/600] [300/331] Loss_D: 0.46156, Loss_G: 2.94117, Loss_KL: 0.33434\n",
      "[474/600] Loss_D: 0.60238, Loss_G: 2.80462, Loss_KL: 0.35642\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[475/600] [0/331] Loss_D: 0.67852, Loss_G: 3.02511, Loss_KL: 0.38069\n",
      "[475/600] [100/331] Loss_D: 0.60225, Loss_G: 3.26314, Loss_KL: 0.31685\n",
      "[475/600] [200/331] Loss_D: 0.88162, Loss_G: 1.90805, Loss_KL: 0.34124\n",
      "[475/600] [300/331] Loss_D: 0.72575, Loss_G: 3.03341, Loss_KL: 0.34841\n",
      "[475/600] Loss_D: 0.58574, Loss_G: 2.80402, Loss_KL: 0.35143\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[476/600] [0/331] Loss_D: 0.67401, Loss_G: 3.44539, Loss_KL: 0.34153\n",
      "[476/600] [100/331] Loss_D: 0.71189, Loss_G: 3.44574, Loss_KL: 0.29492\n",
      "[476/600] [200/331] Loss_D: 0.49524, Loss_G: 2.33384, Loss_KL: 0.36920\n",
      "[476/600] [300/331] Loss_D: 0.74362, Loss_G: 3.14022, Loss_KL: 0.39225\n",
      "[476/600] Loss_D: 0.58530, Loss_G: 2.84243, Loss_KL: 0.35069\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[477/600] [0/331] Loss_D: 0.79051, Loss_G: 3.49341, Loss_KL: 0.36312\n",
      "[477/600] [100/331] Loss_D: 0.75559, Loss_G: 3.11513, Loss_KL: 0.39920\n",
      "[477/600] [200/331] Loss_D: 0.70589, Loss_G: 2.73556, Loss_KL: 0.36104\n",
      "[477/600] [300/331] Loss_D: 0.50224, Loss_G: 2.00007, Loss_KL: 0.30356\n",
      "[477/600] Loss_D: 0.60122, Loss_G: 2.79678, Loss_KL: 0.35038\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[478/600] [0/331] Loss_D: 0.55426, Loss_G: 3.38873, Loss_KL: 0.46955\n",
      "[478/600] [100/331] Loss_D: 0.64838, Loss_G: 3.22070, Loss_KL: 0.41359\n",
      "[478/600] [200/331] Loss_D: 0.39598, Loss_G: 2.00687, Loss_KL: 0.31770\n",
      "[478/600] [300/331] Loss_D: 0.66461, Loss_G: 2.54852, Loss_KL: 0.29540\n",
      "[478/600] Loss_D: 0.58008, Loss_G: 2.83834, Loss_KL: 0.35212\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[479/600] [0/331] Loss_D: 0.43427, Loss_G: 2.49531, Loss_KL: 0.37787\n",
      "[479/600] [100/331] Loss_D: 0.86664, Loss_G: 2.51082, Loss_KL: 0.34590\n",
      "[479/600] [200/331] Loss_D: 0.25341, Loss_G: 2.61916, Loss_KL: 0.29764\n",
      "[479/600] [300/331] Loss_D: 0.65432, Loss_G: 2.44410, Loss_KL: 0.40842\n",
      "[479/600] Loss_D: 0.60842, Loss_G: 2.74242, Loss_KL: 0.34683\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[480/600] [0/331] Loss_D: 0.40701, Loss_G: 2.77484, Loss_KL: 0.29300\n",
      "[480/600] [100/331] Loss_D: 0.66946, Loss_G: 2.36950, Loss_KL: 0.37426\n",
      "[480/600] [200/331] Loss_D: 0.62346, Loss_G: 2.67626, Loss_KL: 0.32762\n",
      "[480/600] [300/331] Loss_D: 0.73947, Loss_G: 2.53310, Loss_KL: 0.31557\n",
      "[480/600] Loss_D: 0.58733, Loss_G: 2.81194, Loss_KL: 0.34881\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Save G1/D1 models\n",
      "[481/600] [0/331] Loss_D: 0.67147, Loss_G: 2.93937, Loss_KL: 0.34113\n",
      "[481/600] [100/331] Loss_D: 0.68392, Loss_G: 2.55333, Loss_KL: 0.32916\n",
      "[481/600] [200/331] Loss_D: 0.69658, Loss_G: 2.60050, Loss_KL: 0.41442\n",
      "[481/600] [300/331] Loss_D: 0.90273, Loss_G: 2.38164, Loss_KL: 0.43202\n",
      "[481/600] Loss_D: 0.59433, Loss_G: 2.77530, Loss_KL: 0.35381\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[482/600] [0/331] Loss_D: 0.54394, Loss_G: 2.72253, Loss_KL: 0.30752\n",
      "[482/600] [100/331] Loss_D: 0.49842, Loss_G: 2.23351, Loss_KL: 0.32728\n",
      "[482/600] [200/331] Loss_D: 0.57886, Loss_G: 3.29430, Loss_KL: 0.24538\n",
      "[482/600] [300/331] Loss_D: 0.63314, Loss_G: 2.66889, Loss_KL: 0.36941\n",
      "[482/600] Loss_D: 0.59816, Loss_G: 2.75176, Loss_KL: 0.34904\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[483/600] [0/331] Loss_D: 0.63621, Loss_G: 2.45547, Loss_KL: 0.29789\n",
      "[483/600] [100/331] Loss_D: 0.56059, Loss_G: 2.39089, Loss_KL: 0.29299\n",
      "[483/600] [200/331] Loss_D: 0.79469, Loss_G: 2.98775, Loss_KL: 0.37919\n",
      "[483/600] [300/331] Loss_D: 0.36968, Loss_G: 3.28809, Loss_KL: 0.41859\n",
      "[483/600] Loss_D: 0.61695, Loss_G: 2.75941, Loss_KL: 0.34944\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[484/600] [0/331] Loss_D: 0.52113, Loss_G: 2.60601, Loss_KL: 0.36972\n",
      "[484/600] [100/331] Loss_D: 0.31743, Loss_G: 3.46859, Loss_KL: 0.37652\n",
      "[484/600] [200/331] Loss_D: 0.71962, Loss_G: 3.16750, Loss_KL: 0.32330\n",
      "[484/600] [300/331] Loss_D: 0.71437, Loss_G: 2.86075, Loss_KL: 0.44950\n",
      "[484/600] Loss_D: 0.58639, Loss_G: 2.79255, Loss_KL: 0.35021\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[485/600] [0/331] Loss_D: 0.71290, Loss_G: 2.86678, Loss_KL: 0.31025\n",
      "[485/600] [100/331] Loss_D: 0.75118, Loss_G: 2.02754, Loss_KL: 0.36179\n",
      "[485/600] [200/331] Loss_D: 0.31577, Loss_G: 2.50286, Loss_KL: 0.39215\n",
      "[485/600] [300/331] Loss_D: 0.68892, Loss_G: 2.72816, Loss_KL: 0.37413\n",
      "[485/600] Loss_D: 0.59248, Loss_G: 2.79451, Loss_KL: 0.34886\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[486/600] [0/331] Loss_D: 0.42451, Loss_G: 3.11103, Loss_KL: 0.34508\n",
      "[486/600] [100/331] Loss_D: 0.81528, Loss_G: 2.90473, Loss_KL: 0.38561\n",
      "[486/600] [200/331] Loss_D: 0.48520, Loss_G: 2.88852, Loss_KL: 0.31081\n",
      "[486/600] [300/331] Loss_D: 0.62920, Loss_G: 2.83143, Loss_KL: 0.28451\n",
      "[486/600] Loss_D: 0.59828, Loss_G: 2.78996, Loss_KL: 0.34832\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[487/600] [0/331] Loss_D: 0.72014, Loss_G: 2.64850, Loss_KL: 0.43223\n",
      "[487/600] [100/331] Loss_D: 0.89448, Loss_G: 2.23206, Loss_KL: 0.30734\n",
      "[487/600] [200/331] Loss_D: 0.72553, Loss_G: 2.67262, Loss_KL: 0.35667\n",
      "[487/600] [300/331] Loss_D: 0.37785, Loss_G: 2.78643, Loss_KL: 0.37714\n",
      "[487/600] Loss_D: 0.60507, Loss_G: 2.79035, Loss_KL: 0.34612\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[488/600] [0/331] Loss_D: 0.71459, Loss_G: 3.80942, Loss_KL: 0.33879\n",
      "[488/600] [100/331] Loss_D: 0.77845, Loss_G: 2.38718, Loss_KL: 0.35957\n",
      "[488/600] [200/331] Loss_D: 0.59948, Loss_G: 2.79155, Loss_KL: 0.27219\n",
      "[488/600] [300/331] Loss_D: 0.71445, Loss_G: 2.68919, Loss_KL: 0.31677\n",
      "[488/600] Loss_D: 0.58325, Loss_G: 2.88449, Loss_KL: 0.35328\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[489/600] [0/331] Loss_D: 0.52203, Loss_G: 3.03539, Loss_KL: 0.45910\n",
      "[489/600] [100/331] Loss_D: 0.34285, Loss_G: 2.92960, Loss_KL: 0.30556\n",
      "[489/600] [200/331] Loss_D: 0.69439, Loss_G: 2.72509, Loss_KL: 0.41018\n",
      "[489/600] [300/331] Loss_D: 0.38672, Loss_G: 3.18117, Loss_KL: 0.43822\n",
      "[489/600] Loss_D: 0.58051, Loss_G: 2.84055, Loss_KL: 0.35132\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[490/600] [0/331] Loss_D: 0.56164, Loss_G: 3.28193, Loss_KL: 0.32652\n",
      "[490/600] [100/331] Loss_D: 0.47251, Loss_G: 3.74493, Loss_KL: 0.34380\n",
      "[490/600] [200/331] Loss_D: 0.71746, Loss_G: 3.70028, Loss_KL: 0.35456\n",
      "[490/600] [300/331] Loss_D: 0.40315, Loss_G: 2.58863, Loss_KL: 0.31998\n",
      "[490/600] Loss_D: 0.58543, Loss_G: 2.86396, Loss_KL: 0.35483\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[491/600] [0/331] Loss_D: 0.73804, Loss_G: 2.18833, Loss_KL: 0.41211\n",
      "[491/600] [100/331] Loss_D: 0.76696, Loss_G: 3.52169, Loss_KL: 0.25091\n",
      "[491/600] [200/331] Loss_D: 0.82028, Loss_G: 1.98817, Loss_KL: 0.31953\n",
      "[491/600] [300/331] Loss_D: 0.58924, Loss_G: 2.07415, Loss_KL: 0.35035\n",
      "[491/600] Loss_D: 0.59595, Loss_G: 2.79842, Loss_KL: 0.35382\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[492/600] [0/331] Loss_D: 0.65632, Loss_G: 2.97961, Loss_KL: 0.28651\n",
      "[492/600] [100/331] Loss_D: 0.45197, Loss_G: 2.50628, Loss_KL: 0.37970\n",
      "[492/600] [200/331] Loss_D: 0.64812, Loss_G: 3.03102, Loss_KL: 0.41549\n",
      "[492/600] [300/331] Loss_D: 0.61562, Loss_G: 2.17817, Loss_KL: 0.32647\n",
      "[492/600] Loss_D: 0.58875, Loss_G: 2.80870, Loss_KL: 0.35415\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[493/600] [0/331] Loss_D: 0.55060, Loss_G: 3.05136, Loss_KL: 0.32125\n",
      "[493/600] [100/331] Loss_D: 0.73530, Loss_G: 2.38915, Loss_KL: 0.36296\n",
      "[493/600] [200/331] Loss_D: 0.70510, Loss_G: 2.23407, Loss_KL: 0.36365\n",
      "[493/600] [300/331] Loss_D: 0.75225, Loss_G: 3.33258, Loss_KL: 0.37377\n",
      "[493/600] Loss_D: 0.59232, Loss_G: 2.78962, Loss_KL: 0.35693\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[494/600] [0/331] Loss_D: 0.70502, Loss_G: 2.64761, Loss_KL: 0.27934\n",
      "[494/600] [100/331] Loss_D: 0.79641, Loss_G: 2.54298, Loss_KL: 0.31061\n",
      "[494/600] [200/331] Loss_D: 0.77027, Loss_G: 2.49458, Loss_KL: 0.42945\n",
      "[494/600] [300/331] Loss_D: 0.11475, Loss_G: 2.61530, Loss_KL: 0.31306\n",
      "[494/600] Loss_D: 0.58493, Loss_G: 2.78747, Loss_KL: 0.35384\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[495/600] [0/331] Loss_D: 0.83225, Loss_G: 2.71156, Loss_KL: 0.46343\n",
      "[495/600] [100/331] Loss_D: 0.73102, Loss_G: 2.94125, Loss_KL: 0.39352\n",
      "[495/600] [200/331] Loss_D: 0.67099, Loss_G: 2.12870, Loss_KL: 0.30127\n",
      "[495/600] [300/331] Loss_D: 0.31725, Loss_G: 2.81926, Loss_KL: 0.30594\n",
      "[495/600] Loss_D: 0.60476, Loss_G: 2.78932, Loss_KL: 0.35968\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[496/600] [0/331] Loss_D: 0.43882, Loss_G: 3.04839, Loss_KL: 0.38089\n",
      "[496/600] [100/331] Loss_D: 0.70549, Loss_G: 3.03972, Loss_KL: 0.29377\n",
      "[496/600] [200/331] Loss_D: 0.43516, Loss_G: 2.75268, Loss_KL: 0.41353\n",
      "[496/600] [300/331] Loss_D: 0.67328, Loss_G: 2.35334, Loss_KL: 0.37318\n",
      "[496/600] Loss_D: 0.60702, Loss_G: 2.77222, Loss_KL: 0.35994\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[497/600] [0/331] Loss_D: 0.78821, Loss_G: 2.70195, Loss_KL: 0.36661\n",
      "[497/600] [100/331] Loss_D: 0.49017, Loss_G: 2.96400, Loss_KL: 0.29786\n",
      "[497/600] [200/331] Loss_D: 0.32399, Loss_G: 2.55740, Loss_KL: 0.35050\n",
      "[497/600] [300/331] Loss_D: 0.41935, Loss_G: 2.71630, Loss_KL: 0.39094\n",
      "[497/600] Loss_D: 0.60910, Loss_G: 2.79575, Loss_KL: 0.35590\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[498/600] [0/331] Loss_D: 0.72126, Loss_G: 2.32970, Loss_KL: 0.43361\n",
      "[498/600] [100/331] Loss_D: 0.31893, Loss_G: 3.31023, Loss_KL: 0.40906\n",
      "[498/600] [200/331] Loss_D: 0.56774, Loss_G: 3.11373, Loss_KL: 0.31658\n",
      "[498/600] [300/331] Loss_D: 0.78292, Loss_G: 2.53040, Loss_KL: 0.39288\n",
      "[498/600] Loss_D: 0.59397, Loss_G: 2.83576, Loss_KL: 0.35888\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[499/600] [0/331] Loss_D: 0.27928, Loss_G: 2.90316, Loss_KL: 0.35064\n",
      "[499/600] [100/331] Loss_D: 0.26587, Loss_G: 3.44069, Loss_KL: 0.33151\n",
      "[499/600] [200/331] Loss_D: 0.76778, Loss_G: 2.28135, Loss_KL: 0.36596\n",
      "[499/600] [300/331] Loss_D: 0.52412, Loss_G: 1.92345, Loss_KL: 0.26896\n",
      "[499/600] Loss_D: 0.59783, Loss_G: 2.81885, Loss_KL: 0.35997\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "Adjusting learning rate of group 0 to 1.2500e-05.\n",
      "[500/600] [0/331] Loss_D: 0.58146, Loss_G: 2.69688, Loss_KL: 0.41744\n",
      "[500/600] [100/331] Loss_D: 0.34398, Loss_G: 2.77183, Loss_KL: 0.40216\n",
      "[500/600] [200/331] Loss_D: 0.38223, Loss_G: 3.33899, Loss_KL: 0.37609\n",
      "[500/600] [300/331] Loss_D: 0.55582, Loss_G: 2.07886, Loss_KL: 0.32371\n",
      "[500/600] Loss_D: 0.59709, Loss_G: 2.81075, Loss_KL: 0.36031\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Save G1/D1 models\n",
      "[501/600] [0/331] Loss_D: 0.47140, Loss_G: 2.63914, Loss_KL: 0.36090\n",
      "[501/600] [100/331] Loss_D: 0.67922, Loss_G: 2.38351, Loss_KL: 0.40074\n",
      "[501/600] [200/331] Loss_D: 0.51668, Loss_G: 2.45155, Loss_KL: 0.33056\n",
      "[501/600] [300/331] Loss_D: 0.80863, Loss_G: 2.74887, Loss_KL: 0.43111\n",
      "[501/600] Loss_D: 0.62097, Loss_G: 2.79701, Loss_KL: 0.35863\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[502/600] [0/331] Loss_D: 0.76279, Loss_G: 2.49769, Loss_KL: 0.32169\n",
      "[502/600] [100/331] Loss_D: 0.79580, Loss_G: 3.25547, Loss_KL: 0.32471\n",
      "[502/600] [200/331] Loss_D: 0.57842, Loss_G: 3.43004, Loss_KL: 0.44678\n",
      "[502/600] [300/331] Loss_D: 0.76924, Loss_G: 2.24550, Loss_KL: 0.34331\n",
      "[502/600] Loss_D: 0.59900, Loss_G: 2.77618, Loss_KL: 0.36087\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[503/600] [0/331] Loss_D: 0.46798, Loss_G: 2.24620, Loss_KL: 0.38008\n",
      "[503/600] [100/331] Loss_D: 0.54432, Loss_G: 2.36908, Loss_KL: 0.36038\n",
      "[503/600] [200/331] Loss_D: 0.59411, Loss_G: 2.59279, Loss_KL: 0.44328\n",
      "[503/600] [300/331] Loss_D: 0.55216, Loss_G: 2.02869, Loss_KL: 0.34179\n",
      "[503/600] Loss_D: 0.60028, Loss_G: 2.80874, Loss_KL: 0.36448\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[504/600] [0/331] Loss_D: 0.51534, Loss_G: 2.56520, Loss_KL: 0.28154\n",
      "[504/600] [100/331] Loss_D: 0.46836, Loss_G: 3.05011, Loss_KL: 0.48517\n",
      "[504/600] [200/331] Loss_D: 0.50654, Loss_G: 2.75679, Loss_KL: 0.37925\n",
      "[504/600] [300/331] Loss_D: 0.35848, Loss_G: 2.63183, Loss_KL: 0.38337\n",
      "[504/600] Loss_D: 0.58171, Loss_G: 2.82537, Loss_KL: 0.36065\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[505/600] [0/331] Loss_D: 0.45022, Loss_G: 2.79284, Loss_KL: 0.44798\n",
      "[505/600] [100/331] Loss_D: 0.70506, Loss_G: 2.25991, Loss_KL: 0.47722\n",
      "[505/600] [200/331] Loss_D: 0.80733, Loss_G: 2.25673, Loss_KL: 0.37245\n",
      "[505/600] [300/331] Loss_D: 0.65838, Loss_G: 2.47872, Loss_KL: 0.33732\n",
      "[505/600] Loss_D: 0.59106, Loss_G: 2.79539, Loss_KL: 0.35668\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[506/600] [0/331] Loss_D: 0.70498, Loss_G: 2.76700, Loss_KL: 0.30795\n",
      "[506/600] [100/331] Loss_D: 0.69081, Loss_G: 3.50913, Loss_KL: 0.32275\n",
      "[506/600] [200/331] Loss_D: 0.41789, Loss_G: 2.79082, Loss_KL: 0.32789\n",
      "[506/600] [300/331] Loss_D: 0.37003, Loss_G: 2.72639, Loss_KL: 0.37025\n",
      "[506/600] Loss_D: 0.58120, Loss_G: 2.80298, Loss_KL: 0.36066\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[507/600] [0/331] Loss_D: 0.69290, Loss_G: 2.63123, Loss_KL: 0.29412\n",
      "[507/600] [100/331] Loss_D: 0.42949, Loss_G: 3.06016, Loss_KL: 0.32246\n",
      "[507/600] [200/331] Loss_D: 0.68054, Loss_G: 2.64675, Loss_KL: 0.39250\n",
      "[507/600] [300/331] Loss_D: 0.68174, Loss_G: 2.33848, Loss_KL: 0.41698\n",
      "[507/600] Loss_D: 0.59362, Loss_G: 2.77074, Loss_KL: 0.35894\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[508/600] [0/331] Loss_D: 0.49837, Loss_G: 2.28396, Loss_KL: 0.40441\n",
      "[508/600] [100/331] Loss_D: 0.69477, Loss_G: 3.23631, Loss_KL: 0.29248\n",
      "[508/600] [200/331] Loss_D: 0.65934, Loss_G: 2.97221, Loss_KL: 0.31283\n",
      "[508/600] [300/331] Loss_D: 0.78479, Loss_G: 2.69054, Loss_KL: 0.37375\n",
      "[508/600] Loss_D: 0.58057, Loss_G: 2.82565, Loss_KL: 0.35765\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[509/600] [0/331] Loss_D: 0.36541, Loss_G: 2.79079, Loss_KL: 0.34773\n",
      "[509/600] [100/331] Loss_D: 0.70269, Loss_G: 3.13248, Loss_KL: 0.33065\n",
      "[509/600] [200/331] Loss_D: 0.76546, Loss_G: 2.15400, Loss_KL: 0.47268\n",
      "[509/600] [300/331] Loss_D: 0.66898, Loss_G: 2.18524, Loss_KL: 0.37932\n",
      "[509/600] Loss_D: 0.59842, Loss_G: 2.78522, Loss_KL: 0.35603\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[510/600] [0/331] Loss_D: 0.70225, Loss_G: 3.15180, Loss_KL: 0.37897\n",
      "[510/600] [100/331] Loss_D: 0.58943, Loss_G: 2.74819, Loss_KL: 0.32414\n",
      "[510/600] [200/331] Loss_D: 0.81681, Loss_G: 3.22031, Loss_KL: 0.29200\n",
      "[510/600] [300/331] Loss_D: 0.63802, Loss_G: 3.34207, Loss_KL: 0.32473\n",
      "[510/600] Loss_D: 0.59833, Loss_G: 2.76038, Loss_KL: 0.35866\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[511/600] [0/331] Loss_D: 0.60858, Loss_G: 2.99641, Loss_KL: 0.41694\n",
      "[511/600] [100/331] Loss_D: 0.64753, Loss_G: 2.66556, Loss_KL: 0.37528\n",
      "[511/600] [200/331] Loss_D: 0.68088, Loss_G: 2.62030, Loss_KL: 0.34541\n",
      "[511/600] [300/331] Loss_D: 0.81198, Loss_G: 2.24629, Loss_KL: 0.41983\n",
      "[511/600] Loss_D: 0.60450, Loss_G: 2.72214, Loss_KL: 0.35721\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[512/600] [0/331] Loss_D: 0.67185, Loss_G: 2.57590, Loss_KL: 0.29804\n",
      "[512/600] [100/331] Loss_D: 0.60593, Loss_G: 2.46562, Loss_KL: 0.31656\n",
      "[512/600] [200/331] Loss_D: 0.79987, Loss_G: 2.77041, Loss_KL: 0.33154\n",
      "[512/600] [300/331] Loss_D: 0.70506, Loss_G: 3.10091, Loss_KL: 0.35510\n",
      "[512/600] Loss_D: 0.59175, Loss_G: 2.77114, Loss_KL: 0.35472\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[513/600] [0/331] Loss_D: 0.86663, Loss_G: 2.55162, Loss_KL: 0.31761\n",
      "[513/600] [100/331] Loss_D: 0.66414, Loss_G: 2.52542, Loss_KL: 0.30437\n",
      "[513/600] [200/331] Loss_D: 0.67011, Loss_G: 3.13819, Loss_KL: 0.30329\n",
      "[513/600] [300/331] Loss_D: 0.52295, Loss_G: 2.22925, Loss_KL: 0.42050\n",
      "[513/600] Loss_D: 0.60173, Loss_G: 2.74692, Loss_KL: 0.35647\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[514/600] [0/331] Loss_D: 0.69163, Loss_G: 2.37010, Loss_KL: 0.27988\n",
      "[514/600] [100/331] Loss_D: 0.75687, Loss_G: 2.84067, Loss_KL: 0.36809\n",
      "[514/600] [200/331] Loss_D: 0.76738, Loss_G: 3.45088, Loss_KL: 0.37727\n",
      "[514/600] [300/331] Loss_D: 0.74593, Loss_G: 2.07426, Loss_KL: 0.34527\n",
      "[514/600] Loss_D: 0.59732, Loss_G: 2.77387, Loss_KL: 0.35371\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[515/600] [0/331] Loss_D: 0.63414, Loss_G: 2.41623, Loss_KL: 0.30457\n",
      "[515/600] [100/331] Loss_D: 0.66828, Loss_G: 3.10457, Loss_KL: 0.36860\n",
      "[515/600] [200/331] Loss_D: 0.63069, Loss_G: 3.34451, Loss_KL: 0.40636\n",
      "[515/600] [300/331] Loss_D: 0.26934, Loss_G: 3.33129, Loss_KL: 0.40144\n",
      "[515/600] Loss_D: 0.59274, Loss_G: 2.83733, Loss_KL: 0.35627\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[516/600] [0/331] Loss_D: 0.32810, Loss_G: 2.84989, Loss_KL: 0.38445\n",
      "[516/600] [100/331] Loss_D: 0.65996, Loss_G: 2.12620, Loss_KL: 0.33905\n",
      "[516/600] [200/331] Loss_D: 0.49772, Loss_G: 2.00525, Loss_KL: 0.32535\n",
      "[516/600] [300/331] Loss_D: 0.28298, Loss_G: 3.51246, Loss_KL: 0.37061\n",
      "[516/600] Loss_D: 0.58096, Loss_G: 2.84095, Loss_KL: 0.35609\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[517/600] [0/331] Loss_D: 0.38424, Loss_G: 3.55994, Loss_KL: 0.30240\n",
      "[517/600] [100/331] Loss_D: 0.76973, Loss_G: 2.94356, Loss_KL: 0.49110\n",
      "[517/600] [200/331] Loss_D: 0.29494, Loss_G: 2.29190, Loss_KL: 0.34682\n",
      "[517/600] [300/331] Loss_D: 0.58981, Loss_G: 2.10486, Loss_KL: 0.33189\n",
      "[517/600] Loss_D: 0.58715, Loss_G: 2.81933, Loss_KL: 0.35891\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[518/600] [0/331] Loss_D: 0.58430, Loss_G: 2.18159, Loss_KL: 0.33713\n",
      "[518/600] [100/331] Loss_D: 0.76184, Loss_G: 3.20579, Loss_KL: 0.37577\n",
      "[518/600] [200/331] Loss_D: 0.69544, Loss_G: 2.73442, Loss_KL: 0.35353\n",
      "[518/600] [300/331] Loss_D: 0.70882, Loss_G: 3.27079, Loss_KL: 0.34527\n",
      "[518/600] Loss_D: 0.58775, Loss_G: 2.81101, Loss_KL: 0.35646\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[519/600] [0/331] Loss_D: 0.69683, Loss_G: 2.53421, Loss_KL: 0.37073\n",
      "[519/600] [100/331] Loss_D: 0.78636, Loss_G: 1.93009, Loss_KL: 0.29478\n",
      "[519/600] [200/331] Loss_D: 0.44678, Loss_G: 3.01887, Loss_KL: 0.40854\n",
      "[519/600] [300/331] Loss_D: 0.63649, Loss_G: 2.88752, Loss_KL: 0.35123\n",
      "[519/600] Loss_D: 0.58561, Loss_G: 2.79376, Loss_KL: 0.36103\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[520/600] [0/331] Loss_D: 0.64182, Loss_G: 2.61606, Loss_KL: 0.27956\n",
      "[520/600] [100/331] Loss_D: 0.42827, Loss_G: 3.17445, Loss_KL: 0.33660\n",
      "[520/600] [200/331] Loss_D: 0.67973, Loss_G: 3.04223, Loss_KL: 0.32925\n",
      "[520/600] [300/331] Loss_D: 0.79169, Loss_G: 3.59493, Loss_KL: 0.42806\n",
      "[520/600] Loss_D: 0.59545, Loss_G: 2.77598, Loss_KL: 0.36408\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Save G1/D1 models\n",
      "[521/600] [0/331] Loss_D: 0.34996, Loss_G: 2.68667, Loss_KL: 0.40869\n",
      "[521/600] [100/331] Loss_D: 0.38316, Loss_G: 2.63558, Loss_KL: 0.35649\n",
      "[521/600] [200/331] Loss_D: 0.43317, Loss_G: 2.64989, Loss_KL: 0.34322\n",
      "[521/600] [300/331] Loss_D: 0.62319, Loss_G: 2.96317, Loss_KL: 0.37947\n",
      "[521/600] Loss_D: 0.59441, Loss_G: 2.75754, Loss_KL: 0.35672\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[522/600] [0/331] Loss_D: 0.68815, Loss_G: 2.77534, Loss_KL: 0.32884\n",
      "[522/600] [100/331] Loss_D: 0.49227, Loss_G: 2.87100, Loss_KL: 0.36103\n",
      "[522/600] [200/331] Loss_D: 0.70487, Loss_G: 2.36529, Loss_KL: 0.30553\n",
      "[522/600] [300/331] Loss_D: 0.53646, Loss_G: 2.53236, Loss_KL: 0.35965\n",
      "[522/600] Loss_D: 0.59078, Loss_G: 2.81863, Loss_KL: 0.35676\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[523/600] [0/331] Loss_D: 0.74369, Loss_G: 2.43732, Loss_KL: 0.31022\n",
      "[523/600] [100/331] Loss_D: 0.67536, Loss_G: 2.66294, Loss_KL: 0.39917\n",
      "[523/600] [200/331] Loss_D: 0.51366, Loss_G: 2.84241, Loss_KL: 0.41031\n",
      "[523/600] [300/331] Loss_D: 0.66426, Loss_G: 1.95274, Loss_KL: 0.31815\n",
      "[523/600] Loss_D: 0.58879, Loss_G: 2.81978, Loss_KL: 0.35697\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[524/600] [0/331] Loss_D: 0.41334, Loss_G: 2.26021, Loss_KL: 0.31909\n",
      "[524/600] [100/331] Loss_D: 0.65421, Loss_G: 3.44834, Loss_KL: 0.32712\n",
      "[524/600] [200/331] Loss_D: 0.81167, Loss_G: 2.55382, Loss_KL: 0.31406\n",
      "[524/600] [300/331] Loss_D: 0.75141, Loss_G: 2.42438, Loss_KL: 0.34990\n",
      "[524/600] Loss_D: 0.60202, Loss_G: 2.79706, Loss_KL: 0.35606\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[525/600] [0/331] Loss_D: 0.54884, Loss_G: 3.51376, Loss_KL: 0.33143\n",
      "[525/600] [100/331] Loss_D: 0.85267, Loss_G: 2.39365, Loss_KL: 0.39800\n",
      "[525/600] [200/331] Loss_D: 0.80740, Loss_G: 2.37992, Loss_KL: 0.35476\n",
      "[525/600] [300/331] Loss_D: 0.83834, Loss_G: 2.01419, Loss_KL: 0.33390\n",
      "[525/600] Loss_D: 0.60922, Loss_G: 2.75181, Loss_KL: 0.35651\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[526/600] [0/331] Loss_D: 0.47470, Loss_G: 2.65248, Loss_KL: 0.27253\n",
      "[526/600] [100/331] Loss_D: 0.73181, Loss_G: 2.64340, Loss_KL: 0.35302\n",
      "[526/600] [200/331] Loss_D: 0.76479, Loss_G: 3.02443, Loss_KL: 0.37344\n",
      "[526/600] [300/331] Loss_D: 0.40224, Loss_G: 2.16214, Loss_KL: 0.30172\n",
      "[526/600] Loss_D: 0.60178, Loss_G: 2.73899, Loss_KL: 0.35817\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[527/600] [0/331] Loss_D: 0.53408, Loss_G: 2.73667, Loss_KL: 0.39922\n",
      "[527/600] [100/331] Loss_D: 0.75485, Loss_G: 3.43327, Loss_KL: 0.28551\n",
      "[527/600] [200/331] Loss_D: 0.32350, Loss_G: 2.54799, Loss_KL: 0.34284\n",
      "[527/600] [300/331] Loss_D: 0.34727, Loss_G: 2.56631, Loss_KL: 0.34890\n",
      "[527/600] Loss_D: 0.59200, Loss_G: 2.79420, Loss_KL: 0.35610\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[528/600] [0/331] Loss_D: 0.65295, Loss_G: 2.41109, Loss_KL: 0.28831\n",
      "[528/600] [100/331] Loss_D: 0.80311, Loss_G: 3.86076, Loss_KL: 0.39868\n",
      "[528/600] [200/331] Loss_D: 0.69940, Loss_G: 3.10553, Loss_KL: 0.40683\n",
      "[528/600] [300/331] Loss_D: 0.74880, Loss_G: 3.67401, Loss_KL: 0.38715\n",
      "[528/600] Loss_D: 0.59214, Loss_G: 2.80791, Loss_KL: 0.35743\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[529/600] [0/331] Loss_D: 0.38261, Loss_G: 3.44777, Loss_KL: 0.39413\n",
      "[529/600] [100/331] Loss_D: 0.67157, Loss_G: 3.20649, Loss_KL: 0.36111\n",
      "[529/600] [200/331] Loss_D: 0.36021, Loss_G: 3.11196, Loss_KL: 0.36430\n",
      "[529/600] [300/331] Loss_D: 0.25421, Loss_G: 2.65650, Loss_KL: 0.39613\n",
      "[529/600] Loss_D: 0.59578, Loss_G: 2.78859, Loss_KL: 0.35613\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[530/600] [0/331] Loss_D: 0.70192, Loss_G: 3.24789, Loss_KL: 0.35536\n",
      "[530/600] [100/331] Loss_D: 0.62119, Loss_G: 2.20083, Loss_KL: 0.31306\n",
      "[530/600] [200/331] Loss_D: 0.68587, Loss_G: 2.91321, Loss_KL: 0.38278\n",
      "[530/600] [300/331] Loss_D: 0.75706, Loss_G: 2.68067, Loss_KL: 0.31115\n",
      "[530/600] Loss_D: 0.58456, Loss_G: 2.78585, Loss_KL: 0.35661\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[531/600] [0/331] Loss_D: 0.69229, Loss_G: 2.44283, Loss_KL: 0.39680\n",
      "[531/600] [100/331] Loss_D: 0.48289, Loss_G: 3.15496, Loss_KL: 0.42029\n",
      "[531/600] [200/331] Loss_D: 0.41950, Loss_G: 2.95103, Loss_KL: 0.30547\n",
      "[531/600] [300/331] Loss_D: 0.40120, Loss_G: 2.88722, Loss_KL: 0.36707\n",
      "[531/600] Loss_D: 0.57705, Loss_G: 2.80062, Loss_KL: 0.35652\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[532/600] [0/331] Loss_D: 0.22895, Loss_G: 2.98819, Loss_KL: 0.34944\n",
      "[532/600] [100/331] Loss_D: 0.72756, Loss_G: 3.37107, Loss_KL: 0.41064\n",
      "[532/600] [200/331] Loss_D: 0.24980, Loss_G: 2.93243, Loss_KL: 0.35764\n",
      "[532/600] [300/331] Loss_D: 0.44815, Loss_G: 2.34024, Loss_KL: 0.34923\n",
      "[532/600] Loss_D: 0.59356, Loss_G: 2.82785, Loss_KL: 0.35849\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[533/600] [0/331] Loss_D: 0.52844, Loss_G: 2.44109, Loss_KL: 0.35859\n",
      "[533/600] [100/331] Loss_D: 0.73584, Loss_G: 2.14060, Loss_KL: 0.36260\n",
      "[533/600] [200/331] Loss_D: 0.33066, Loss_G: 3.94888, Loss_KL: 0.38796\n",
      "[533/600] [300/331] Loss_D: 0.66208, Loss_G: 2.80756, Loss_KL: 0.42574\n",
      "[533/600] Loss_D: 0.58687, Loss_G: 2.82113, Loss_KL: 0.35729\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[534/600] [0/331] Loss_D: 0.72962, Loss_G: 3.21551, Loss_KL: 0.43083\n",
      "[534/600] [100/331] Loss_D: 0.68735, Loss_G: 2.92122, Loss_KL: 0.33435\n",
      "[534/600] [200/331] Loss_D: 0.75275, Loss_G: 3.63850, Loss_KL: 0.33644\n",
      "[534/600] [300/331] Loss_D: 0.46759, Loss_G: 3.56645, Loss_KL: 0.38997\n",
      "[534/600] Loss_D: 0.56972, Loss_G: 2.87036, Loss_KL: 0.35574\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[535/600] [0/331] Loss_D: 0.59995, Loss_G: 2.77554, Loss_KL: 0.28549\n",
      "[535/600] [100/331] Loss_D: 0.29931, Loss_G: 2.92907, Loss_KL: 0.51120\n",
      "[535/600] [200/331] Loss_D: 0.30571, Loss_G: 2.95965, Loss_KL: 0.36189\n",
      "[535/600] [300/331] Loss_D: 0.74911, Loss_G: 2.21395, Loss_KL: 0.36619\n",
      "[535/600] Loss_D: 0.59033, Loss_G: 2.84589, Loss_KL: 0.35488\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[536/600] [0/331] Loss_D: 0.44469, Loss_G: 2.62364, Loss_KL: 0.33899\n",
      "[536/600] [100/331] Loss_D: 0.51062, Loss_G: 2.61252, Loss_KL: 0.34137\n",
      "[536/600] [200/331] Loss_D: 0.76725, Loss_G: 2.28242, Loss_KL: 0.34409\n",
      "[536/600] [300/331] Loss_D: 0.30185, Loss_G: 2.46708, Loss_KL: 0.35777\n",
      "[536/600] Loss_D: 0.58248, Loss_G: 2.86011, Loss_KL: 0.35657\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[537/600] [0/331] Loss_D: 0.85785, Loss_G: 2.38276, Loss_KL: 0.39054\n",
      "[537/600] [100/331] Loss_D: 0.28543, Loss_G: 2.95477, Loss_KL: 0.31986\n",
      "[537/600] [200/331] Loss_D: 0.65688, Loss_G: 2.39219, Loss_KL: 0.31690\n",
      "[537/600] [300/331] Loss_D: 0.57271, Loss_G: 2.39487, Loss_KL: 0.41157\n",
      "[537/600] Loss_D: 0.58034, Loss_G: 2.83586, Loss_KL: 0.35535\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[538/600] [0/331] Loss_D: 0.67034, Loss_G: 3.38430, Loss_KL: 0.32570\n",
      "[538/600] [100/331] Loss_D: 0.35015, Loss_G: 2.93762, Loss_KL: 0.35861\n",
      "[538/600] [200/331] Loss_D: 0.63291, Loss_G: 2.53133, Loss_KL: 0.38901\n",
      "[538/600] [300/331] Loss_D: 0.71600, Loss_G: 3.17596, Loss_KL: 0.33764\n",
      "[538/600] Loss_D: 0.59119, Loss_G: 2.80833, Loss_KL: 0.35380\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[539/600] [0/331] Loss_D: 0.32377, Loss_G: 3.33723, Loss_KL: 0.51268\n",
      "[539/600] [100/331] Loss_D: 0.37864, Loss_G: 2.65067, Loss_KL: 0.31378\n",
      "[539/600] [200/331] Loss_D: 0.51456, Loss_G: 3.12309, Loss_KL: 0.27349\n",
      "[539/600] [300/331] Loss_D: 0.65217, Loss_G: 2.78379, Loss_KL: 0.36064\n",
      "[539/600] Loss_D: 0.58057, Loss_G: 2.85851, Loss_KL: 0.35677\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[540/600] [0/331] Loss_D: 0.68118, Loss_G: 2.65153, Loss_KL: 0.36865\n",
      "[540/600] [100/331] Loss_D: 0.40989, Loss_G: 3.15112, Loss_KL: 0.39428\n",
      "[540/600] [200/331] Loss_D: 0.45319, Loss_G: 2.54101, Loss_KL: 0.27743\n",
      "[540/600] [300/331] Loss_D: 0.67719, Loss_G: 2.56312, Loss_KL: 0.40485\n",
      "[540/600] Loss_D: 0.59046, Loss_G: 2.79828, Loss_KL: 0.35680\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Save G1/D1 models\n",
      "[541/600] [0/331] Loss_D: 0.75853, Loss_G: 2.71212, Loss_KL: 0.35782\n",
      "[541/600] [100/331] Loss_D: 0.76713, Loss_G: 2.90550, Loss_KL: 0.39874\n",
      "[541/600] [200/331] Loss_D: 0.64073, Loss_G: 2.71400, Loss_KL: 0.46850\n",
      "[541/600] [300/331] Loss_D: 0.67750, Loss_G: 3.04752, Loss_KL: 0.35729\n",
      "[541/600] Loss_D: 0.60285, Loss_G: 2.83761, Loss_KL: 0.35907\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[542/600] [0/331] Loss_D: 0.79034, Loss_G: 2.59431, Loss_KL: 0.35643\n",
      "[542/600] [100/331] Loss_D: 0.72336, Loss_G: 3.25011, Loss_KL: 0.42120\n",
      "[542/600] [200/331] Loss_D: 0.72385, Loss_G: 2.90217, Loss_KL: 0.31008\n",
      "[542/600] [300/331] Loss_D: 0.64048, Loss_G: 2.51223, Loss_KL: 0.35018\n",
      "[542/600] Loss_D: 0.57926, Loss_G: 2.83317, Loss_KL: 0.35581\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[543/600] [0/331] Loss_D: 0.22591, Loss_G: 3.09174, Loss_KL: 0.33660\n",
      "[543/600] [100/331] Loss_D: 0.43438, Loss_G: 3.07284, Loss_KL: 0.43043\n",
      "[543/600] [200/331] Loss_D: 0.62803, Loss_G: 3.46929, Loss_KL: 0.29033\n",
      "[543/600] [300/331] Loss_D: 0.62153, Loss_G: 2.79323, Loss_KL: 0.35635\n",
      "[543/600] Loss_D: 0.60246, Loss_G: 2.80074, Loss_KL: 0.35585\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[544/600] [0/331] Loss_D: 0.59377, Loss_G: 2.67043, Loss_KL: 0.35199\n",
      "[544/600] [100/331] Loss_D: 0.71506, Loss_G: 2.88742, Loss_KL: 0.28088\n",
      "[544/600] [200/331] Loss_D: 0.69848, Loss_G: 2.95215, Loss_KL: 0.33441\n",
      "[544/600] [300/331] Loss_D: 0.31473, Loss_G: 3.50299, Loss_KL: 0.35246\n",
      "[544/600] Loss_D: 0.58357, Loss_G: 2.78748, Loss_KL: 0.35742\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[545/600] [0/331] Loss_D: 0.68298, Loss_G: 2.94250, Loss_KL: 0.32919\n",
      "[545/600] [100/331] Loss_D: 0.41862, Loss_G: 2.31408, Loss_KL: 0.34474\n",
      "[545/600] [200/331] Loss_D: 0.38177, Loss_G: 2.90813, Loss_KL: 0.38369\n",
      "[545/600] [300/331] Loss_D: 0.43249, Loss_G: 3.25710, Loss_KL: 0.38874\n",
      "[545/600] Loss_D: 0.59020, Loss_G: 2.81191, Loss_KL: 0.35965\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[546/600] [0/331] Loss_D: 0.64295, Loss_G: 3.45653, Loss_KL: 0.32337\n",
      "[546/600] [100/331] Loss_D: 0.67849, Loss_G: 3.62694, Loss_KL: 0.34200\n",
      "[546/600] [200/331] Loss_D: 0.60499, Loss_G: 2.60702, Loss_KL: 0.32950\n",
      "[546/600] [300/331] Loss_D: 0.63480, Loss_G: 2.88279, Loss_KL: 0.34543\n",
      "[546/600] Loss_D: 0.58488, Loss_G: 2.83070, Loss_KL: 0.35526\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[547/600] [0/331] Loss_D: 0.81276, Loss_G: 3.11275, Loss_KL: 0.37254\n",
      "[547/600] [100/331] Loss_D: 0.59142, Loss_G: 3.07391, Loss_KL: 0.36761\n",
      "[547/600] [200/331] Loss_D: 0.39548, Loss_G: 3.15846, Loss_KL: 0.47063\n",
      "[547/600] [300/331] Loss_D: 0.73690, Loss_G: 2.98390, Loss_KL: 0.38257\n",
      "[547/600] Loss_D: 0.58251, Loss_G: 2.80826, Loss_KL: 0.35905\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[548/600] [0/331] Loss_D: 0.69615, Loss_G: 2.63160, Loss_KL: 0.29307\n",
      "[548/600] [100/331] Loss_D: 0.61472, Loss_G: 3.00355, Loss_KL: 0.35223\n",
      "[548/600] [200/331] Loss_D: 0.55719, Loss_G: 2.66851, Loss_KL: 0.28083\n",
      "[548/600] [300/331] Loss_D: 0.81133, Loss_G: 2.52396, Loss_KL: 0.36074\n",
      "[548/600] Loss_D: 0.58151, Loss_G: 2.78460, Loss_KL: 0.35958\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[549/600] [0/331] Loss_D: 0.75417, Loss_G: 2.13910, Loss_KL: 0.38418\n",
      "[549/600] [100/331] Loss_D: 0.67344, Loss_G: 2.09816, Loss_KL: 0.35255\n",
      "[549/600] [200/331] Loss_D: 0.82973, Loss_G: 2.57790, Loss_KL: 0.39748\n",
      "[549/600] [300/331] Loss_D: 0.66796, Loss_G: 3.13539, Loss_KL: 0.37956\n",
      "[549/600] Loss_D: 0.58798, Loss_G: 2.78453, Loss_KL: 0.36396\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[550/600] [0/331] Loss_D: 0.46297, Loss_G: 2.74711, Loss_KL: 0.32850\n",
      "[550/600] [100/331] Loss_D: 0.50440, Loss_G: 3.21698, Loss_KL: 0.29770\n",
      "[550/600] [200/331] Loss_D: 0.27129, Loss_G: 2.44067, Loss_KL: 0.31827\n",
      "[550/600] [300/331] Loss_D: 0.40594, Loss_G: 2.35692, Loss_KL: 0.33950\n",
      "[550/600] Loss_D: 0.59849, Loss_G: 2.82530, Loss_KL: 0.35885\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[551/600] [0/331] Loss_D: 0.48256, Loss_G: 3.56934, Loss_KL: 0.34561\n",
      "[551/600] [100/331] Loss_D: 0.43767, Loss_G: 3.58471, Loss_KL: 0.35105\n",
      "[551/600] [200/331] Loss_D: 0.80927, Loss_G: 2.14971, Loss_KL: 0.33796\n",
      "[551/600] [300/331] Loss_D: 0.51322, Loss_G: 2.56111, Loss_KL: 0.35229\n",
      "[551/600] Loss_D: 0.58423, Loss_G: 2.79464, Loss_KL: 0.36103\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[552/600] [0/331] Loss_D: 0.60985, Loss_G: 2.42737, Loss_KL: 0.37769\n",
      "[552/600] [100/331] Loss_D: 0.42771, Loss_G: 2.70087, Loss_KL: 0.40599\n",
      "[552/600] [200/331] Loss_D: 0.36746, Loss_G: 3.21975, Loss_KL: 0.31864\n",
      "[552/600] [300/331] Loss_D: 0.41041, Loss_G: 3.09362, Loss_KL: 0.44477\n",
      "[552/600] Loss_D: 0.59101, Loss_G: 2.82305, Loss_KL: 0.36290\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[553/600] [0/331] Loss_D: 0.66926, Loss_G: 3.86056, Loss_KL: 0.37074\n",
      "[553/600] [100/331] Loss_D: 0.51009, Loss_G: 3.06489, Loss_KL: 0.37441\n",
      "[553/600] [200/331] Loss_D: 0.77770, Loss_G: 3.14890, Loss_KL: 0.37951\n",
      "[553/600] [300/331] Loss_D: 0.43808, Loss_G: 2.71647, Loss_KL: 0.30933\n",
      "[553/600] Loss_D: 0.57189, Loss_G: 2.88304, Loss_KL: 0.35963\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[554/600] [0/331] Loss_D: 0.26467, Loss_G: 3.17768, Loss_KL: 0.45026\n",
      "[554/600] [100/331] Loss_D: 0.10211, Loss_G: 3.56854, Loss_KL: 0.36036\n",
      "[554/600] [200/331] Loss_D: 0.64617, Loss_G: 2.65457, Loss_KL: 0.32922\n",
      "[554/600] [300/331] Loss_D: 0.43977, Loss_G: 4.70555, Loss_KL: 0.27365\n",
      "[554/600] Loss_D: 0.57496, Loss_G: 2.87752, Loss_KL: 0.36029\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[555/600] [0/331] Loss_D: 0.65475, Loss_G: 2.75227, Loss_KL: 0.34706\n",
      "[555/600] [100/331] Loss_D: 0.70001, Loss_G: 3.10445, Loss_KL: 0.36819\n",
      "[555/600] [200/331] Loss_D: 0.59269, Loss_G: 2.47752, Loss_KL: 0.36091\n",
      "[555/600] [300/331] Loss_D: 0.23762, Loss_G: 2.40015, Loss_KL: 0.29948\n",
      "[555/600] Loss_D: 0.57815, Loss_G: 2.81393, Loss_KL: 0.36143\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[556/600] [0/331] Loss_D: 0.41443, Loss_G: 3.16663, Loss_KL: 0.39158\n",
      "[556/600] [100/331] Loss_D: 0.56707, Loss_G: 2.61519, Loss_KL: 0.31332\n",
      "[556/600] [200/331] Loss_D: 0.73300, Loss_G: 3.22842, Loss_KL: 0.41530\n",
      "[556/600] [300/331] Loss_D: 0.59313, Loss_G: 3.22975, Loss_KL: 0.45357\n",
      "[556/600] Loss_D: 0.58211, Loss_G: 2.82818, Loss_KL: 0.36193\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[557/600] [0/331] Loss_D: 0.76245, Loss_G: 2.70627, Loss_KL: 0.41140\n",
      "[557/600] [100/331] Loss_D: 0.78489, Loss_G: 3.16345, Loss_KL: 0.40556\n",
      "[557/600] [200/331] Loss_D: 0.95251, Loss_G: 2.73244, Loss_KL: 0.42015\n",
      "[557/600] [300/331] Loss_D: 0.59599, Loss_G: 2.51643, Loss_KL: 0.30689\n",
      "[557/600] Loss_D: 0.59415, Loss_G: 2.77850, Loss_KL: 0.36228\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[558/600] [0/331] Loss_D: 0.55256, Loss_G: 2.07973, Loss_KL: 0.31398\n",
      "[558/600] [100/331] Loss_D: 0.34023, Loss_G: 3.21269, Loss_KL: 0.38066\n",
      "[558/600] [200/331] Loss_D: 0.74972, Loss_G: 3.62109, Loss_KL: 0.34675\n",
      "[558/600] [300/331] Loss_D: 0.71431, Loss_G: 2.03298, Loss_KL: 0.37709\n",
      "[558/600] Loss_D: 0.59294, Loss_G: 2.80103, Loss_KL: 0.36244\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[559/600] [0/331] Loss_D: 0.68062, Loss_G: 3.05186, Loss_KL: 0.36664\n",
      "[559/600] [100/331] Loss_D: 0.29331, Loss_G: 2.47949, Loss_KL: 0.39170\n",
      "[559/600] [200/331] Loss_D: 0.58527, Loss_G: 2.72133, Loss_KL: 0.36839\n",
      "[559/600] [300/331] Loss_D: 0.44914, Loss_G: 2.19210, Loss_KL: 0.32816\n",
      "[559/600] Loss_D: 0.59717, Loss_G: 2.81782, Loss_KL: 0.36190\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[560/600] [0/331] Loss_D: 0.80341, Loss_G: 2.91260, Loss_KL: 0.34947\n",
      "[560/600] [100/331] Loss_D: 0.65797, Loss_G: 2.41114, Loss_KL: 0.41998\n",
      "[560/600] [200/331] Loss_D: 0.27991, Loss_G: 2.53029, Loss_KL: 0.33642\n",
      "[560/600] [300/331] Loss_D: 0.71174, Loss_G: 2.55105, Loss_KL: 0.39586\n",
      "[560/600] Loss_D: 0.57848, Loss_G: 2.78782, Loss_KL: 0.36216\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Save G1/D1 models\n",
      "[561/600] [0/331] Loss_D: 0.17189, Loss_G: 2.53934, Loss_KL: 0.40828\n",
      "[561/600] [100/331] Loss_D: 0.58670, Loss_G: 2.64817, Loss_KL: 0.41181\n",
      "[561/600] [200/331] Loss_D: 0.38687, Loss_G: 2.93563, Loss_KL: 0.39477\n",
      "[561/600] [300/331] Loss_D: 0.29884, Loss_G: 3.07301, Loss_KL: 0.33769\n",
      "[561/600] Loss_D: 0.58958, Loss_G: 2.78256, Loss_KL: 0.36314\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[562/600] [0/331] Loss_D: 0.66309, Loss_G: 2.31094, Loss_KL: 0.34617\n",
      "[562/600] [100/331] Loss_D: 0.24053, Loss_G: 2.77907, Loss_KL: 0.40819\n",
      "[562/600] [200/331] Loss_D: 0.43130, Loss_G: 2.55818, Loss_KL: 0.31159\n",
      "[562/600] [300/331] Loss_D: 0.68093, Loss_G: 2.63202, Loss_KL: 0.35659\n",
      "[562/600] Loss_D: 0.59382, Loss_G: 2.82311, Loss_KL: 0.35964\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[563/600] [0/331] Loss_D: 0.41412, Loss_G: 2.80230, Loss_KL: 0.33876\n",
      "[563/600] [100/331] Loss_D: 0.48892, Loss_G: 2.03984, Loss_KL: 0.31969\n",
      "[563/600] [200/331] Loss_D: 0.40093, Loss_G: 3.11274, Loss_KL: 0.42263\n",
      "[563/600] [300/331] Loss_D: 0.37176, Loss_G: 2.68342, Loss_KL: 0.36861\n",
      "[563/600] Loss_D: 0.59674, Loss_G: 2.83033, Loss_KL: 0.36536\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[564/600] [0/331] Loss_D: 0.86610, Loss_G: 2.91780, Loss_KL: 0.34440\n",
      "[564/600] [100/331] Loss_D: 0.80651, Loss_G: 2.79635, Loss_KL: 0.36189\n",
      "[564/600] [200/331] Loss_D: 0.72314, Loss_G: 2.27005, Loss_KL: 0.38229\n",
      "[564/600] [300/331] Loss_D: 0.61991, Loss_G: 3.15829, Loss_KL: 0.36122\n",
      "[564/600] Loss_D: 0.57898, Loss_G: 2.83005, Loss_KL: 0.36527\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[565/600] [0/331] Loss_D: 0.78809, Loss_G: 3.10734, Loss_KL: 0.31221\n",
      "[565/600] [100/331] Loss_D: 0.75127, Loss_G: 1.93306, Loss_KL: 0.31270\n",
      "[565/600] [200/331] Loss_D: 0.86883, Loss_G: 3.16580, Loss_KL: 0.36121\n",
      "[565/600] [300/331] Loss_D: 0.39318, Loss_G: 2.59512, Loss_KL: 0.34905\n",
      "[565/600] Loss_D: 0.59651, Loss_G: 2.78101, Loss_KL: 0.36107\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[566/600] [0/331] Loss_D: 0.70232, Loss_G: 2.82662, Loss_KL: 0.34059\n",
      "[566/600] [100/331] Loss_D: 0.16882, Loss_G: 3.15463, Loss_KL: 0.32158\n",
      "[566/600] [200/331] Loss_D: 0.32855, Loss_G: 2.11505, Loss_KL: 0.35088\n",
      "[566/600] [300/331] Loss_D: 0.63183, Loss_G: 2.83187, Loss_KL: 0.35472\n",
      "[566/600] Loss_D: 0.58619, Loss_G: 2.76697, Loss_KL: 0.35986\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[567/600] [0/331] Loss_D: 0.27851, Loss_G: 2.45668, Loss_KL: 0.27728\n",
      "[567/600] [100/331] Loss_D: 0.79970, Loss_G: 3.25083, Loss_KL: 0.30357\n",
      "[567/600] [200/331] Loss_D: 0.44145, Loss_G: 3.84533, Loss_KL: 0.35118\n",
      "[567/600] [300/331] Loss_D: 0.50343, Loss_G: 3.06951, Loss_KL: 0.40613\n",
      "[567/600] Loss_D: 0.59397, Loss_G: 2.80127, Loss_KL: 0.36132\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[568/600] [0/331] Loss_D: 0.76877, Loss_G: 2.51708, Loss_KL: 0.37044\n",
      "[568/600] [100/331] Loss_D: 0.70129, Loss_G: 1.94903, Loss_KL: 0.34114\n",
      "[568/600] [200/331] Loss_D: 0.52023, Loss_G: 2.57152, Loss_KL: 0.46355\n",
      "[568/600] [300/331] Loss_D: 0.58751, Loss_G: 1.93566, Loss_KL: 0.43773\n",
      "[568/600] Loss_D: 0.60088, Loss_G: 2.79422, Loss_KL: 0.35996\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[569/600] [0/331] Loss_D: 0.41106, Loss_G: 2.73636, Loss_KL: 0.40807\n",
      "[569/600] [100/331] Loss_D: 0.62359, Loss_G: 2.78025, Loss_KL: 0.42448\n",
      "[569/600] [200/331] Loss_D: 0.68800, Loss_G: 3.05183, Loss_KL: 0.34900\n",
      "[569/600] [300/331] Loss_D: 0.76352, Loss_G: 2.47807, Loss_KL: 0.36109\n",
      "[569/600] Loss_D: 0.60297, Loss_G: 2.70682, Loss_KL: 0.36282\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[570/600] [0/331] Loss_D: 0.71701, Loss_G: 2.88226, Loss_KL: 0.36396\n",
      "[570/600] [100/331] Loss_D: 0.60780, Loss_G: 3.13075, Loss_KL: 0.33729\n",
      "[570/600] [200/331] Loss_D: 0.28205, Loss_G: 3.17861, Loss_KL: 0.29344\n",
      "[570/600] [300/331] Loss_D: 0.73378, Loss_G: 2.69523, Loss_KL: 0.38339\n",
      "[570/600] Loss_D: 0.61037, Loss_G: 2.79669, Loss_KL: 0.36084\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[571/600] [0/331] Loss_D: 0.34787, Loss_G: 2.21822, Loss_KL: 0.37122\n",
      "[571/600] [100/331] Loss_D: 0.68282, Loss_G: 2.85997, Loss_KL: 0.27620\n",
      "[571/600] [200/331] Loss_D: 0.40251, Loss_G: 3.51452, Loss_KL: 0.42137\n",
      "[571/600] [300/331] Loss_D: 0.45472, Loss_G: 2.96008, Loss_KL: 0.42792\n",
      "[571/600] Loss_D: 0.58838, Loss_G: 2.76689, Loss_KL: 0.36059\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[572/600] [0/331] Loss_D: 0.62628, Loss_G: 2.20001, Loss_KL: 0.30572\n",
      "[572/600] [100/331] Loss_D: 0.44183, Loss_G: 2.95375, Loss_KL: 0.40729\n",
      "[572/600] [200/331] Loss_D: 0.40742, Loss_G: 2.75228, Loss_KL: 0.40716\n",
      "[572/600] [300/331] Loss_D: 0.38147, Loss_G: 3.72131, Loss_KL: 0.36327\n",
      "[572/600] Loss_D: 0.58458, Loss_G: 2.80806, Loss_KL: 0.35985\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[573/600] [0/331] Loss_D: 0.72932, Loss_G: 2.55218, Loss_KL: 0.32622\n",
      "[573/600] [100/331] Loss_D: 0.53786, Loss_G: 2.63078, Loss_KL: 0.32869\n",
      "[573/600] [200/331] Loss_D: 0.69357, Loss_G: 2.57481, Loss_KL: 0.35335\n",
      "[573/600] [300/331] Loss_D: 0.77368, Loss_G: 2.74565, Loss_KL: 0.32982\n",
      "[573/600] Loss_D: 0.58426, Loss_G: 2.77394, Loss_KL: 0.35491\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[574/600] [0/331] Loss_D: 0.86189, Loss_G: 2.81974, Loss_KL: 0.35263\n",
      "[574/600] [100/331] Loss_D: 0.41897, Loss_G: 2.54819, Loss_KL: 0.30579\n",
      "[574/600] [200/331] Loss_D: 0.71432, Loss_G: 2.31935, Loss_KL: 0.41890\n",
      "[574/600] [300/331] Loss_D: 0.76699, Loss_G: 3.81701, Loss_KL: 0.27420\n",
      "[574/600] Loss_D: 0.60335, Loss_G: 2.74296, Loss_KL: 0.36045\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[575/600] [0/331] Loss_D: 0.53886, Loss_G: 3.30725, Loss_KL: 0.39271\n",
      "[575/600] [100/331] Loss_D: 0.46882, Loss_G: 2.58031, Loss_KL: 0.35912\n",
      "[575/600] [200/331] Loss_D: 0.18154, Loss_G: 2.95825, Loss_KL: 0.37140\n",
      "[575/600] [300/331] Loss_D: 0.52254, Loss_G: 2.76267, Loss_KL: 0.36010\n",
      "[575/600] Loss_D: 0.60597, Loss_G: 2.76441, Loss_KL: 0.36128\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[576/600] [0/331] Loss_D: 0.22515, Loss_G: 3.02033, Loss_KL: 0.30286\n",
      "[576/600] [100/331] Loss_D: 0.90317, Loss_G: 2.59675, Loss_KL: 0.33990\n",
      "[576/600] [200/331] Loss_D: 0.70813, Loss_G: 2.92712, Loss_KL: 0.33885\n",
      "[576/600] [300/331] Loss_D: 0.78634, Loss_G: 2.86661, Loss_KL: 0.31851\n",
      "[576/600] Loss_D: 0.59233, Loss_G: 2.80474, Loss_KL: 0.35739\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[577/600] [0/331] Loss_D: 0.42617, Loss_G: 3.00337, Loss_KL: 0.30194\n",
      "[577/600] [100/331] Loss_D: 0.76996, Loss_G: 2.78007, Loss_KL: 0.38527\n",
      "[577/600] [200/331] Loss_D: 0.59339, Loss_G: 2.75473, Loss_KL: 0.36653\n",
      "[577/600] [300/331] Loss_D: 0.54073, Loss_G: 3.01498, Loss_KL: 0.34194\n",
      "[577/600] Loss_D: 0.58520, Loss_G: 2.84374, Loss_KL: 0.36275\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[578/600] [0/331] Loss_D: 0.70726, Loss_G: 2.91563, Loss_KL: 0.31673\n",
      "[578/600] [100/331] Loss_D: 0.32530, Loss_G: 2.27462, Loss_KL: 0.46208\n",
      "[578/600] [200/331] Loss_D: 0.26524, Loss_G: 2.85063, Loss_KL: 0.42723\n",
      "[578/600] [300/331] Loss_D: 0.76833, Loss_G: 2.67005, Loss_KL: 0.39742\n",
      "[578/600] Loss_D: 0.58439, Loss_G: 2.77071, Loss_KL: 0.35946\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[579/600] [0/331] Loss_D: 0.59494, Loss_G: 3.14924, Loss_KL: 0.44479\n",
      "[579/600] [100/331] Loss_D: 0.40455, Loss_G: 3.00766, Loss_KL: 0.47909\n",
      "[579/600] [200/331] Loss_D: 0.69014, Loss_G: 3.01299, Loss_KL: 0.35349\n",
      "[579/600] [300/331] Loss_D: 0.74174, Loss_G: 2.29949, Loss_KL: 0.31744\n",
      "[579/600] Loss_D: 0.58974, Loss_G: 2.78891, Loss_KL: 0.36316\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[580/600] [0/331] Loss_D: 0.48473, Loss_G: 2.57956, Loss_KL: 0.36754\n",
      "[580/600] [100/331] Loss_D: 0.62632, Loss_G: 2.44229, Loss_KL: 0.37041\n",
      "[580/600] [200/331] Loss_D: 0.67275, Loss_G: 2.29398, Loss_KL: 0.31142\n",
      "[580/600] [300/331] Loss_D: 0.64874, Loss_G: 2.82829, Loss_KL: 0.32285\n",
      "[580/600] Loss_D: 0.59050, Loss_G: 2.81409, Loss_KL: 0.35578\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Save G1/D1 models\n",
      "[581/600] [0/331] Loss_D: 0.47891, Loss_G: 2.73673, Loss_KL: 0.34790\n",
      "[581/600] [100/331] Loss_D: 0.70623, Loss_G: 1.90464, Loss_KL: 0.31270\n",
      "[581/600] [200/331] Loss_D: 0.74227, Loss_G: 2.85833, Loss_KL: 0.36630\n",
      "[581/600] [300/331] Loss_D: 0.71823, Loss_G: 2.29021, Loss_KL: 0.37489\n",
      "[581/600] Loss_D: 0.61250, Loss_G: 2.82674, Loss_KL: 0.35689\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[582/600] [0/331] Loss_D: 0.70433, Loss_G: 2.80636, Loss_KL: 0.38258\n",
      "[582/600] [100/331] Loss_D: 0.37517, Loss_G: 3.13132, Loss_KL: 0.42289\n",
      "[582/600] [200/331] Loss_D: 0.42319, Loss_G: 2.73602, Loss_KL: 0.43539\n",
      "[582/600] [300/331] Loss_D: 0.80004, Loss_G: 2.70807, Loss_KL: 0.37326\n",
      "[582/600] Loss_D: 0.60065, Loss_G: 2.83788, Loss_KL: 0.35930\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[583/600] [0/331] Loss_D: 0.65613, Loss_G: 2.48915, Loss_KL: 0.37280\n",
      "[583/600] [100/331] Loss_D: 0.84801, Loss_G: 3.39297, Loss_KL: 0.38958\n",
      "[583/600] [200/331] Loss_D: 0.42197, Loss_G: 3.01435, Loss_KL: 0.32890\n",
      "[583/600] [300/331] Loss_D: 0.66506, Loss_G: 2.96487, Loss_KL: 0.35717\n",
      "[583/600] Loss_D: 0.60725, Loss_G: 2.78846, Loss_KL: 0.35734\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[584/600] [0/331] Loss_D: 0.80584, Loss_G: 1.86279, Loss_KL: 0.36589\n",
      "[584/600] [100/331] Loss_D: 0.38197, Loss_G: 3.73551, Loss_KL: 0.36031\n",
      "[584/600] [200/331] Loss_D: 0.69566, Loss_G: 2.90080, Loss_KL: 0.32276\n",
      "[584/600] [300/331] Loss_D: 0.65471, Loss_G: 2.50103, Loss_KL: 0.39373\n",
      "[584/600] Loss_D: 0.58723, Loss_G: 2.77032, Loss_KL: 0.35818\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[585/600] [0/331] Loss_D: 0.49086, Loss_G: 3.07140, Loss_KL: 0.29860\n",
      "[585/600] [100/331] Loss_D: 0.76208, Loss_G: 3.23708, Loss_KL: 0.40082\n",
      "[585/600] [200/331] Loss_D: 0.22479, Loss_G: 2.97738, Loss_KL: 0.41027\n",
      "[585/600] [300/331] Loss_D: 0.47733, Loss_G: 2.88808, Loss_KL: 0.34284\n",
      "[585/600] Loss_D: 0.58048, Loss_G: 2.82152, Loss_KL: 0.35729\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[586/600] [0/331] Loss_D: 0.66132, Loss_G: 2.76513, Loss_KL: 0.36051\n",
      "[586/600] [100/331] Loss_D: 0.75092, Loss_G: 2.12229, Loss_KL: 0.39716\n",
      "[586/600] [200/331] Loss_D: 0.77941, Loss_G: 2.22478, Loss_KL: 0.32927\n",
      "[586/600] [300/331] Loss_D: 0.68457, Loss_G: 2.68166, Loss_KL: 0.36821\n",
      "[586/600] Loss_D: 0.58114, Loss_G: 2.79384, Loss_KL: 0.35712\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[587/600] [0/331] Loss_D: 0.50289, Loss_G: 2.64773, Loss_KL: 0.34950\n",
      "[587/600] [100/331] Loss_D: 0.75585, Loss_G: 3.56897, Loss_KL: 0.36405\n",
      "[587/600] [200/331] Loss_D: 0.17218, Loss_G: 2.48392, Loss_KL: 0.33591\n",
      "[587/600] [300/331] Loss_D: 0.30667, Loss_G: 3.42654, Loss_KL: 0.38952\n",
      "[587/600] Loss_D: 0.59329, Loss_G: 2.82426, Loss_KL: 0.36163\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[588/600] [0/331] Loss_D: 0.54717, Loss_G: 3.19322, Loss_KL: 0.36929\n",
      "[588/600] [100/331] Loss_D: 0.61730, Loss_G: 2.85092, Loss_KL: 0.39850\n",
      "[588/600] [200/331] Loss_D: 0.67392, Loss_G: 2.54288, Loss_KL: 0.34401\n",
      "[588/600] [300/331] Loss_D: 0.33756, Loss_G: 2.57624, Loss_KL: 0.31039\n",
      "[588/600] Loss_D: 0.58307, Loss_G: 2.82439, Loss_KL: 0.35729\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[589/600] [0/331] Loss_D: 0.66173, Loss_G: 3.03982, Loss_KL: 0.36886\n",
      "[589/600] [100/331] Loss_D: 0.38590, Loss_G: 3.11271, Loss_KL: 0.33331\n",
      "[589/600] [200/331] Loss_D: 0.73918, Loss_G: 2.60020, Loss_KL: 0.38781\n",
      "[589/600] [300/331] Loss_D: 0.70751, Loss_G: 2.76883, Loss_KL: 0.37941\n",
      "[589/600] Loss_D: 0.59436, Loss_G: 2.78700, Loss_KL: 0.36116\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[590/600] [0/331] Loss_D: 0.65156, Loss_G: 2.90611, Loss_KL: 0.37192\n",
      "[590/600] [100/331] Loss_D: 0.20955, Loss_G: 2.33584, Loss_KL: 0.39983\n",
      "[590/600] [200/331] Loss_D: 0.72992, Loss_G: 2.95636, Loss_KL: 0.33408\n",
      "[590/600] [300/331] Loss_D: 0.59653, Loss_G: 2.64045, Loss_KL: 0.39882\n",
      "[590/600] Loss_D: 0.59248, Loss_G: 2.72800, Loss_KL: 0.35821\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[591/600] [0/331] Loss_D: 0.73113, Loss_G: 2.17668, Loss_KL: 0.29551\n",
      "[591/600] [100/331] Loss_D: 0.50788, Loss_G: 2.67239, Loss_KL: 0.39232\n",
      "[591/600] [200/331] Loss_D: 0.70799, Loss_G: 3.59806, Loss_KL: 0.37428\n",
      "[591/600] [300/331] Loss_D: 0.69769, Loss_G: 2.52767, Loss_KL: 0.37678\n",
      "[591/600] Loss_D: 0.59723, Loss_G: 2.80397, Loss_KL: 0.35778\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[592/600] [0/331] Loss_D: 0.69715, Loss_G: 3.62171, Loss_KL: 0.38787\n",
      "[592/600] [100/331] Loss_D: 0.72692, Loss_G: 2.56901, Loss_KL: 0.32660\n",
      "[592/600] [200/331] Loss_D: 0.87899, Loss_G: 2.33729, Loss_KL: 0.32355\n",
      "[592/600] [300/331] Loss_D: 0.66984, Loss_G: 3.40056, Loss_KL: 0.35680\n",
      "[592/600] Loss_D: 0.59267, Loss_G: 2.80899, Loss_KL: 0.35944\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[593/600] [0/331] Loss_D: 0.60973, Loss_G: 2.38221, Loss_KL: 0.39553\n",
      "[593/600] [100/331] Loss_D: 0.79792, Loss_G: 3.15445, Loss_KL: 0.33396\n",
      "[593/600] [200/331] Loss_D: 0.66605, Loss_G: 2.93517, Loss_KL: 0.30809\n",
      "[593/600] [300/331] Loss_D: 0.48068, Loss_G: 2.88134, Loss_KL: 0.35040\n",
      "[593/600] Loss_D: 0.60034, Loss_G: 2.81230, Loss_KL: 0.35934\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[594/600] [0/331] Loss_D: 0.38504, Loss_G: 2.82724, Loss_KL: 0.37714\n",
      "[594/600] [100/331] Loss_D: 0.43865, Loss_G: 2.36189, Loss_KL: 0.33298\n",
      "[594/600] [200/331] Loss_D: 0.56090, Loss_G: 2.57381, Loss_KL: 0.47773\n",
      "[594/600] [300/331] Loss_D: 0.55494, Loss_G: 2.57110, Loss_KL: 0.32414\n",
      "[594/600] Loss_D: 0.58442, Loss_G: 2.85177, Loss_KL: 0.36251\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[595/600] [0/331] Loss_D: 0.54863, Loss_G: 2.18644, Loss_KL: 0.42323\n",
      "[595/600] [100/331] Loss_D: 0.55839, Loss_G: 3.08921, Loss_KL: 0.37669\n",
      "[595/600] [200/331] Loss_D: 0.45478, Loss_G: 2.57315, Loss_KL: 0.47144\n",
      "[595/600] [300/331] Loss_D: 0.23190, Loss_G: 3.34637, Loss_KL: 0.31601\n",
      "[595/600] Loss_D: 0.59564, Loss_G: 2.80041, Loss_KL: 0.36369\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[596/600] [0/331] Loss_D: 0.40894, Loss_G: 2.52066, Loss_KL: 0.33968\n",
      "[596/600] [100/331] Loss_D: 0.39815, Loss_G: 3.13350, Loss_KL: 0.39568\n",
      "[596/600] [200/331] Loss_D: 0.42920, Loss_G: 3.16477, Loss_KL: 0.27033\n",
      "[596/600] [300/331] Loss_D: 0.36336, Loss_G: 2.84042, Loss_KL: 0.26917\n",
      "[596/600] Loss_D: 0.60315, Loss_G: 2.86105, Loss_KL: 0.35846\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[597/600] [0/331] Loss_D: 0.62614, Loss_G: 1.58121, Loss_KL: 0.28634\n",
      "[597/600] [100/331] Loss_D: 0.75364, Loss_G: 2.29153, Loss_KL: 0.33331\n",
      "[597/600] [200/331] Loss_D: 0.59132, Loss_G: 2.49051, Loss_KL: 0.30509\n",
      "[597/600] [300/331] Loss_D: 0.82621, Loss_G: 2.04852, Loss_KL: 0.33029\n",
      "[597/600] Loss_D: 0.58514, Loss_G: 2.79937, Loss_KL: 0.35630\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[598/600] [0/331] Loss_D: 0.61659, Loss_G: 3.13249, Loss_KL: 0.45213\n",
      "[598/600] [100/331] Loss_D: 0.54730, Loss_G: 2.72464, Loss_KL: 0.56761\n",
      "[598/600] [200/331] Loss_D: 0.58728, Loss_G: 3.31950, Loss_KL: 0.37145\n",
      "[598/600] [300/331] Loss_D: 0.48296, Loss_G: 3.91081, Loss_KL: 0.39237\n",
      "[598/600] Loss_D: 0.60351, Loss_G: 2.79141, Loss_KL: 0.35783\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[599/600] [0/331] Loss_D: 0.72791, Loss_G: 2.94551, Loss_KL: 0.43812\n",
      "[599/600] [100/331] Loss_D: 0.50114, Loss_G: 2.62187, Loss_KL: 0.30920\n",
      "[599/600] [200/331] Loss_D: 0.38551, Loss_G: 3.43680, Loss_KL: 0.33940\n",
      "[599/600] [300/331] Loss_D: 0.73337, Loss_G: 2.46947, Loss_KL: 0.29827\n",
      "[599/600] Loss_D: 0.60960, Loss_G: 2.78938, Loss_KL: 0.35782\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "Adjusting learning rate of group 0 to 6.2500e-06.\n",
      "[600/600] [0/331] Loss_D: 0.50393, Loss_G: 3.39553, Loss_KL: 0.36907\n",
      "[600/600] [100/331] Loss_D: 0.76667, Loss_G: 2.21058, Loss_KL: 0.27264\n",
      "[600/600] [200/331] Loss_D: 0.51386, Loss_G: 3.32507, Loss_KL: 0.36631\n",
      "[600/600] [300/331] Loss_D: 0.30086, Loss_G: 2.90035, Loss_KL: 0.42637\n",
      "[600/600] Loss_D: 0.59380, Loss_G: 2.80831, Loss_KL: 0.35755\n",
      "Adjusting learning rate of group 0 to 3.1250e-06.\n",
      "Adjusting learning rate of group 0 to 3.1250e-06.\n",
      "Save G1/D1 models\n"
     ]
    }
   ],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genearting Dataset with image size: 256\n",
      "using text-cnn-rnn as text encoder\n",
      "Dataset created:\n",
      "                length of train dataset: 10610\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage=2\n",
    "batch_size = config.batch_size * len(gpus)\n",
    "trainloader = get_loader(stage, batch_size, random_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator loaded from text-cnn-rnn/out_II/checkpoint_s1_ls_biasF/netG1_epoch_600.pth, starting at epoch: 600\n",
      "Initialized stage2 Generator\n",
      "Initialized, stage 2 discriminator\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Traininig Stage: 2, outputs at: text-cnn-rnn/out_II/tensorboard_s2_ls_biasF, text-cnn-rnn/out_II/results_s2_ls_biasF, text-cnn-rnn/out_II/checkpoint_s2_ls_biasF\n",
      "[1/600] [0/331] Loss_D: 1.69299, Loss_G: 14.64559, Loss_KL: 0.00254\n",
      "[1/600] [100/331] Loss_D: 1.37969, Loss_G: 1.34476, Loss_KL: 0.00162\n",
      "[1/600] [200/331] Loss_D: 1.28780, Loss_G: 1.98312, Loss_KL: 0.00141\n",
      "[1/600] [300/331] Loss_D: 1.39675, Loss_G: 0.96004, Loss_KL: 0.00428\n",
      "[1/600] Loss_D: 1.50581, Loss_G: 1.65170, Loss_KL: 0.00235\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[2/600] [0/331] Loss_D: 1.30989, Loss_G: 1.07266, Loss_KL: 0.00815\n",
      "[2/600] [100/331] Loss_D: 1.24998, Loss_G: 2.17696, Loss_KL: 0.02177\n",
      "[2/600] [200/331] Loss_D: 1.32396, Loss_G: 0.81860, Loss_KL: 0.02766\n",
      "[2/600] [300/331] Loss_D: 1.31030, Loss_G: 0.99695, Loss_KL: 0.02442\n",
      "[2/600] Loss_D: 1.30622, Loss_G: 1.39053, Loss_KL: 0.01945\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[3/600] [0/331] Loss_D: 1.22465, Loss_G: 1.08591, Loss_KL: 0.02988\n",
      "[3/600] [100/331] Loss_D: 1.10509, Loss_G: 1.32627, Loss_KL: 0.03376\n",
      "[3/600] [200/331] Loss_D: 1.14965, Loss_G: 1.69048, Loss_KL: 0.03401\n",
      "[3/600] [300/331] Loss_D: 1.15280, Loss_G: 0.98377, Loss_KL: 0.02707\n",
      "[3/600] Loss_D: 1.27157, Loss_G: 1.27593, Loss_KL: 0.03136\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[4/600] [0/331] Loss_D: 1.29467, Loss_G: 1.65175, Loss_KL: 0.02714\n",
      "[4/600] [100/331] Loss_D: 1.19645, Loss_G: 1.86310, Loss_KL: 0.03693\n",
      "[4/600] [200/331] Loss_D: 1.49613, Loss_G: 1.06003, Loss_KL: 0.03321\n",
      "[4/600] [300/331] Loss_D: 1.26386, Loss_G: 1.88312, Loss_KL: 0.03051\n",
      "[4/600] Loss_D: 1.22383, Loss_G: 1.37459, Loss_KL: 0.03453\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[5/600] [0/331] Loss_D: 1.18590, Loss_G: 1.28990, Loss_KL: 0.02977\n",
      "[5/600] [100/331] Loss_D: 1.04327, Loss_G: 1.98672, Loss_KL: 0.03900\n",
      "[5/600] [200/331] Loss_D: 1.49318, Loss_G: 0.59582, Loss_KL: 0.02725\n",
      "[5/600] [300/331] Loss_D: 1.17047, Loss_G: 1.28288, Loss_KL: 0.04164\n",
      "[5/600] Loss_D: 1.18823, Loss_G: 1.49451, Loss_KL: 0.03760\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[6/600] [0/331] Loss_D: 1.23542, Loss_G: 0.79570, Loss_KL: 0.03526\n",
      "[6/600] [100/331] Loss_D: 1.08485, Loss_G: 1.72763, Loss_KL: 0.04136\n",
      "[6/600] [200/331] Loss_D: 1.16567, Loss_G: 0.91766, Loss_KL: 0.04091\n",
      "[6/600] [300/331] Loss_D: 1.05121, Loss_G: 1.63157, Loss_KL: 0.04295\n",
      "[6/600] Loss_D: 1.13708, Loss_G: 1.71441, Loss_KL: 0.04176\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[7/600] [0/331] Loss_D: 1.40908, Loss_G: 0.50127, Loss_KL: 0.03956\n",
      "[7/600] [100/331] Loss_D: 0.95670, Loss_G: 1.34586, Loss_KL: 0.03790\n",
      "[7/600] [200/331] Loss_D: 1.20381, Loss_G: 1.65018, Loss_KL: 0.05050\n",
      "[7/600] [300/331] Loss_D: 0.96095, Loss_G: 1.60917, Loss_KL: 0.04963\n",
      "[7/600] Loss_D: 1.16603, Loss_G: 1.58246, Loss_KL: 0.04269\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[8/600] [0/331] Loss_D: 0.97339, Loss_G: 1.65046, Loss_KL: 0.04353\n",
      "[8/600] [100/331] Loss_D: 1.39916, Loss_G: 0.49792, Loss_KL: 0.03747\n",
      "[8/600] [200/331] Loss_D: 1.26163, Loss_G: 1.40646, Loss_KL: 0.03936\n",
      "[8/600] [300/331] Loss_D: 1.12687, Loss_G: 1.41722, Loss_KL: 0.05044\n",
      "[8/600] Loss_D: 1.13464, Loss_G: 1.62949, Loss_KL: 0.04439\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[9/600] [0/331] Loss_D: 1.12057, Loss_G: 2.69918, Loss_KL: 0.03552\n",
      "[9/600] [100/331] Loss_D: 1.22851, Loss_G: 2.04814, Loss_KL: 0.06134\n",
      "[9/600] [200/331] Loss_D: 1.18424, Loss_G: 1.00479, Loss_KL: 0.03272\n",
      "[9/600] [300/331] Loss_D: 0.76163, Loss_G: 2.03705, Loss_KL: 0.03193\n",
      "[9/600] Loss_D: 1.10946, Loss_G: 1.73066, Loss_KL: 0.04472\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[10/600] [0/331] Loss_D: 1.09197, Loss_G: 1.86521, Loss_KL: 0.04527\n",
      "[10/600] [100/331] Loss_D: 0.76105, Loss_G: 2.79936, Loss_KL: 0.05989\n",
      "[10/600] [200/331] Loss_D: 1.40920, Loss_G: 1.03820, Loss_KL: 0.04892\n",
      "[10/600] [300/331] Loss_D: 1.20723, Loss_G: 1.14456, Loss_KL: 0.08067\n",
      "[10/600] Loss_D: 1.07433, Loss_G: 1.86660, Loss_KL: 0.05023\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[11/600] [0/331] Loss_D: 1.06574, Loss_G: 1.13559, Loss_KL: 0.05449\n",
      "[11/600] [100/331] Loss_D: 1.21047, Loss_G: 2.08065, Loss_KL: 0.07368\n",
      "[11/600] [200/331] Loss_D: 0.71387, Loss_G: 2.50388, Loss_KL: 0.05933\n",
      "[11/600] [300/331] Loss_D: 1.07876, Loss_G: 1.90893, Loss_KL: 0.07097\n",
      "[11/600] Loss_D: 1.05364, Loss_G: 1.95677, Loss_KL: 0.05602\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[12/600] [0/331] Loss_D: 1.22421, Loss_G: 1.80215, Loss_KL: 0.05669\n",
      "[12/600] [100/331] Loss_D: 0.94829, Loss_G: 2.73980, Loss_KL: 0.05864\n",
      "[12/600] [200/331] Loss_D: 1.31670, Loss_G: 1.71219, Loss_KL: 0.07150\n",
      "[12/600] [300/331] Loss_D: 1.04903, Loss_G: 1.63140, Loss_KL: 0.04336\n",
      "[12/600] Loss_D: 1.06213, Loss_G: 1.98398, Loss_KL: 0.05867\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[13/600] [0/331] Loss_D: 1.23069, Loss_G: 1.28437, Loss_KL: 0.04493\n",
      "[13/600] [100/331] Loss_D: 1.52333, Loss_G: 2.58339, Loss_KL: 0.06317\n",
      "[13/600] [200/331] Loss_D: 0.90914, Loss_G: 3.39426, Loss_KL: 0.04835\n",
      "[13/600] [300/331] Loss_D: 1.14851, Loss_G: 2.27123, Loss_KL: 0.04557\n",
      "[13/600] Loss_D: 1.00295, Loss_G: 2.20726, Loss_KL: 0.05466\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[14/600] [0/331] Loss_D: 0.84423, Loss_G: 1.93312, Loss_KL: 0.05287\n",
      "[14/600] [100/331] Loss_D: 0.93295, Loss_G: 3.93140, Loss_KL: 0.04722\n",
      "[14/600] [200/331] Loss_D: 0.85441, Loss_G: 2.92907, Loss_KL: 0.05342\n",
      "[14/600] [300/331] Loss_D: 0.88424, Loss_G: 1.71090, Loss_KL: 0.08135\n",
      "[14/600] Loss_D: 0.97433, Loss_G: 2.31214, Loss_KL: 0.06134\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[15/600] [0/331] Loss_D: 0.75800, Loss_G: 2.29049, Loss_KL: 0.10755\n",
      "[15/600] [100/331] Loss_D: 0.91675, Loss_G: 2.58196, Loss_KL: 0.07921\n",
      "[15/600] [200/331] Loss_D: 1.19916, Loss_G: 2.94822, Loss_KL: 0.05883\n",
      "[15/600] [300/331] Loss_D: 0.66992, Loss_G: 2.42604, Loss_KL: 0.07951\n",
      "[15/600] Loss_D: 1.00823, Loss_G: 2.24538, Loss_KL: 0.06431\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[16/600] [0/331] Loss_D: 0.74471, Loss_G: 2.48148, Loss_KL: 0.08518\n",
      "[16/600] [100/331] Loss_D: 0.91932, Loss_G: 2.03892, Loss_KL: 0.09445\n",
      "[16/600] [200/331] Loss_D: 0.98212, Loss_G: 2.85120, Loss_KL: 0.06758\n",
      "[16/600] [300/331] Loss_D: 1.14853, Loss_G: 2.37884, Loss_KL: 0.07992\n",
      "[16/600] Loss_D: 0.92972, Loss_G: 2.40370, Loss_KL: 0.06885\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[17/600] [0/331] Loss_D: 0.80233, Loss_G: 3.95714, Loss_KL: 0.06895\n",
      "[17/600] [100/331] Loss_D: 0.72581, Loss_G: 3.32671, Loss_KL: 0.05618\n",
      "[17/600] [200/331] Loss_D: 1.21664, Loss_G: 3.90900, Loss_KL: 0.06176\n",
      "[17/600] [300/331] Loss_D: 0.76007, Loss_G: 2.70003, Loss_KL: 0.06873\n",
      "[17/600] Loss_D: 0.94603, Loss_G: 2.55037, Loss_KL: 0.07027\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[18/600] [0/331] Loss_D: 0.74206, Loss_G: 2.47838, Loss_KL: 0.08021\n",
      "[18/600] [100/331] Loss_D: 1.47003, Loss_G: 2.94384, Loss_KL: 0.06792\n",
      "[18/600] [200/331] Loss_D: 1.14926, Loss_G: 1.95945, Loss_KL: 0.07247\n",
      "[18/600] [300/331] Loss_D: 0.90473, Loss_G: 2.20165, Loss_KL: 0.06586\n",
      "[18/600] Loss_D: 0.95645, Loss_G: 2.38615, Loss_KL: 0.06882\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[19/600] [0/331] Loss_D: 0.98199, Loss_G: 1.60925, Loss_KL: 0.05835\n",
      "[19/600] [100/331] Loss_D: 0.67685, Loss_G: 3.82377, Loss_KL: 0.07909\n",
      "[19/600] [200/331] Loss_D: 0.86971, Loss_G: 1.62916, Loss_KL: 0.07872\n",
      "[19/600] [300/331] Loss_D: 0.70838, Loss_G: 2.97197, Loss_KL: 0.08470\n",
      "[19/600] Loss_D: 0.94856, Loss_G: 2.41431, Loss_KL: 0.06910\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[20/600] [0/331] Loss_D: 0.87487, Loss_G: 1.89179, Loss_KL: 0.08405\n",
      "[20/600] [100/331] Loss_D: 0.88037, Loss_G: 2.91211, Loss_KL: 0.10169\n",
      "[20/600] [200/331] Loss_D: 1.07539, Loss_G: 1.20327, Loss_KL: 0.06979\n",
      "[20/600] [300/331] Loss_D: 1.02310, Loss_G: 1.98079, Loss_KL: 0.06536\n",
      "[20/600] Loss_D: 0.90401, Loss_G: 2.49702, Loss_KL: 0.07027\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[21/600] [0/331] Loss_D: 0.56016, Loss_G: 1.55941, Loss_KL: 0.07301\n",
      "[21/600] [100/331] Loss_D: 0.68472, Loss_G: 3.31539, Loss_KL: 0.08064\n",
      "[21/600] [200/331] Loss_D: 0.89317, Loss_G: 3.57844, Loss_KL: 0.06058\n",
      "[21/600] [300/331] Loss_D: 1.05263, Loss_G: 1.88323, Loss_KL: 0.06396\n",
      "[21/600] Loss_D: 0.93238, Loss_G: 2.53328, Loss_KL: 0.06849\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[22/600] [0/331] Loss_D: 1.09319, Loss_G: 3.22436, Loss_KL: 0.09279\n",
      "[22/600] [100/331] Loss_D: 0.81137, Loss_G: 2.14743, Loss_KL: 0.06422\n",
      "[22/600] [200/331] Loss_D: 0.91614, Loss_G: 2.69809, Loss_KL: 0.07461\n",
      "[22/600] [300/331] Loss_D: 0.82694, Loss_G: 2.19321, Loss_KL: 0.08215\n",
      "[22/600] Loss_D: 0.89894, Loss_G: 2.64960, Loss_KL: 0.07089\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[23/600] [0/331] Loss_D: 0.43682, Loss_G: 2.51218, Loss_KL: 0.09011\n",
      "[23/600] [100/331] Loss_D: 1.13017, Loss_G: 1.52255, Loss_KL: 0.06946\n",
      "[23/600] [200/331] Loss_D: 0.53220, Loss_G: 2.85856, Loss_KL: 0.05308\n",
      "[23/600] [300/331] Loss_D: 0.74065, Loss_G: 2.80896, Loss_KL: 0.05450\n",
      "[23/600] Loss_D: 0.87194, Loss_G: 2.68635, Loss_KL: 0.07533\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[24/600] [0/331] Loss_D: 1.09546, Loss_G: 3.82428, Loss_KL: 0.07473\n",
      "[24/600] [100/331] Loss_D: 0.72266, Loss_G: 2.60462, Loss_KL: 0.09208\n",
      "[24/600] [200/331] Loss_D: 1.04450, Loss_G: 3.16413, Loss_KL: 0.06684\n",
      "[24/600] [300/331] Loss_D: 0.67509, Loss_G: 5.02213, Loss_KL: 0.07786\n",
      "[24/600] Loss_D: 0.89399, Loss_G: 2.65699, Loss_KL: 0.07624\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[25/600] [0/331] Loss_D: 0.64455, Loss_G: 4.00836, Loss_KL: 0.10028\n",
      "[25/600] [100/331] Loss_D: 0.91611, Loss_G: 2.94212, Loss_KL: 0.06936\n",
      "[25/600] [200/331] Loss_D: 0.86120, Loss_G: 3.34384, Loss_KL: 0.09523\n",
      "[25/600] [300/331] Loss_D: 0.76218, Loss_G: 2.78082, Loss_KL: 0.06516\n",
      "[25/600] Loss_D: 0.87562, Loss_G: 2.75923, Loss_KL: 0.07436\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[26/600] [0/331] Loss_D: 0.78059, Loss_G: 2.49035, Loss_KL: 0.06893\n",
      "[26/600] [100/331] Loss_D: 0.61660, Loss_G: 2.61260, Loss_KL: 0.07996\n",
      "[26/600] [200/331] Loss_D: 1.04009, Loss_G: 1.68220, Loss_KL: 0.10798\n",
      "[26/600] [300/331] Loss_D: 0.76366, Loss_G: 2.22488, Loss_KL: 0.07216\n",
      "[26/600] Loss_D: 0.85415, Loss_G: 2.81660, Loss_KL: 0.07481\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[27/600] [0/331] Loss_D: 0.72327, Loss_G: 3.06851, Loss_KL: 0.07395\n",
      "[27/600] [100/331] Loss_D: 0.65376, Loss_G: 4.26961, Loss_KL: 0.08619\n",
      "[27/600] [200/331] Loss_D: 0.87142, Loss_G: 3.37233, Loss_KL: 0.06558\n",
      "[27/600] [300/331] Loss_D: 0.74914, Loss_G: 3.51394, Loss_KL: 0.07834\n",
      "[27/600] Loss_D: 0.83423, Loss_G: 2.85161, Loss_KL: 0.07697\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[28/600] [0/331] Loss_D: 0.81930, Loss_G: 1.49947, Loss_KL: 0.06527\n",
      "[28/600] [100/331] Loss_D: 0.81338, Loss_G: 2.48851, Loss_KL: 0.09894\n",
      "[28/600] [200/331] Loss_D: 0.78657, Loss_G: 2.94055, Loss_KL: 0.08586\n",
      "[28/600] [300/331] Loss_D: 0.86485, Loss_G: 2.60101, Loss_KL: 0.06556\n",
      "[28/600] Loss_D: 0.81687, Loss_G: 2.95805, Loss_KL: 0.08017\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[29/600] [0/331] Loss_D: 0.44845, Loss_G: 2.44164, Loss_KL: 0.07956\n",
      "[29/600] [100/331] Loss_D: 0.87169, Loss_G: 2.97139, Loss_KL: 0.06880\n",
      "[29/600] [200/331] Loss_D: 0.93429, Loss_G: 2.75944, Loss_KL: 0.08636\n",
      "[29/600] [300/331] Loss_D: 0.50799, Loss_G: 2.64393, Loss_KL: 0.07978\n",
      "[29/600] Loss_D: 0.82142, Loss_G: 2.88295, Loss_KL: 0.08302\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[30/600] [0/331] Loss_D: 0.89952, Loss_G: 1.92106, Loss_KL: 0.08838\n",
      "[30/600] [100/331] Loss_D: 0.33907, Loss_G: 2.84629, Loss_KL: 0.06327\n",
      "[30/600] [200/331] Loss_D: 0.71739, Loss_G: 1.76111, Loss_KL: 0.05958\n",
      "[30/600] [300/331] Loss_D: 0.77565, Loss_G: 2.54644, Loss_KL: 0.11682\n",
      "[30/600] Loss_D: 0.82312, Loss_G: 2.90784, Loss_KL: 0.08282\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[31/600] [0/331] Loss_D: 0.94089, Loss_G: 2.60066, Loss_KL: 0.07413\n",
      "[31/600] [100/331] Loss_D: 0.34935, Loss_G: 3.69309, Loss_KL: 0.07661\n",
      "[31/600] [200/331] Loss_D: 0.70947, Loss_G: 1.62772, Loss_KL: 0.08225\n",
      "[31/600] [300/331] Loss_D: 0.70272, Loss_G: 3.83950, Loss_KL: 0.08160\n",
      "[31/600] Loss_D: 0.81632, Loss_G: 2.98537, Loss_KL: 0.08121\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[32/600] [0/331] Loss_D: 0.58272, Loss_G: 3.82193, Loss_KL: 0.07948\n",
      "[32/600] [100/331] Loss_D: 0.87862, Loss_G: 2.89419, Loss_KL: 0.09060\n",
      "[32/600] [200/331] Loss_D: 0.87409, Loss_G: 1.76016, Loss_KL: 0.08044\n",
      "[32/600] [300/331] Loss_D: 0.80656, Loss_G: 2.66200, Loss_KL: 0.12683\n",
      "[32/600] Loss_D: 0.80689, Loss_G: 2.86302, Loss_KL: 0.08507\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[33/600] [0/331] Loss_D: 0.87848, Loss_G: 5.25936, Loss_KL: 0.06771\n",
      "[33/600] [100/331] Loss_D: 1.36925, Loss_G: 3.34682, Loss_KL: 0.09202\n",
      "[33/600] [200/331] Loss_D: 0.92389, Loss_G: 2.87312, Loss_KL: 0.08966\n",
      "[33/600] [300/331] Loss_D: 1.18415, Loss_G: 3.89018, Loss_KL: 0.07427\n",
      "[33/600] Loss_D: 0.80082, Loss_G: 3.00634, Loss_KL: 0.08141\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[34/600] [0/331] Loss_D: 0.99356, Loss_G: 3.13374, Loss_KL: 0.08389\n",
      "[34/600] [100/331] Loss_D: 0.34167, Loss_G: 4.93811, Loss_KL: 0.07430\n",
      "[34/600] [200/331] Loss_D: 0.90877, Loss_G: 2.23110, Loss_KL: 0.07641\n",
      "[34/600] [300/331] Loss_D: 0.82690, Loss_G: 1.96411, Loss_KL: 0.08636\n",
      "[34/600] Loss_D: 0.76402, Loss_G: 3.08700, Loss_KL: 0.08471\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[35/600] [0/331] Loss_D: 1.37828, Loss_G: 3.53489, Loss_KL: 0.07645\n",
      "[35/600] [100/331] Loss_D: 0.71404, Loss_G: 5.09284, Loss_KL: 0.07136\n",
      "[35/600] [200/331] Loss_D: 0.79050, Loss_G: 2.76652, Loss_KL: 0.06815\n",
      "[35/600] [300/331] Loss_D: 0.77674, Loss_G: 2.01519, Loss_KL: 0.07208\n",
      "[35/600] Loss_D: 0.79273, Loss_G: 3.00988, Loss_KL: 0.08171\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[36/600] [0/331] Loss_D: 0.51108, Loss_G: 2.74004, Loss_KL: 0.09078\n",
      "[36/600] [100/331] Loss_D: 0.74360, Loss_G: 2.19730, Loss_KL: 0.08058\n",
      "[36/600] [200/331] Loss_D: 0.40139, Loss_G: 4.11203, Loss_KL: 0.08002\n",
      "[36/600] [300/331] Loss_D: 0.63865, Loss_G: 3.35177, Loss_KL: 0.07253\n",
      "[36/600] Loss_D: 0.79740, Loss_G: 2.98733, Loss_KL: 0.08164\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[37/600] [0/331] Loss_D: 0.88648, Loss_G: 3.70409, Loss_KL: 0.09154\n",
      "[37/600] [100/331] Loss_D: 0.52487, Loss_G: 2.43822, Loss_KL: 0.07711\n",
      "[37/600] [200/331] Loss_D: 0.68875, Loss_G: 2.56881, Loss_KL: 0.06318\n",
      "[37/600] [300/331] Loss_D: 0.58137, Loss_G: 2.68725, Loss_KL: 0.09302\n",
      "[37/600] Loss_D: 0.80411, Loss_G: 2.99464, Loss_KL: 0.08333\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[38/600] [0/331] Loss_D: 0.67397, Loss_G: 4.83024, Loss_KL: 0.09470\n",
      "[38/600] [100/331] Loss_D: 0.76418, Loss_G: 1.89878, Loss_KL: 0.08948\n",
      "[38/600] [200/331] Loss_D: 0.72484, Loss_G: 2.97536, Loss_KL: 0.07211\n",
      "[38/600] [300/331] Loss_D: 0.40734, Loss_G: 3.89390, Loss_KL: 0.06577\n",
      "[38/600] Loss_D: 0.80832, Loss_G: 2.97661, Loss_KL: 0.08014\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[39/600] [0/331] Loss_D: 0.62895, Loss_G: 4.02389, Loss_KL: 0.07104\n",
      "[39/600] [100/331] Loss_D: 1.00268, Loss_G: 0.69853, Loss_KL: 0.09650\n",
      "[39/600] [200/331] Loss_D: 0.83328, Loss_G: 2.89381, Loss_KL: 0.06786\n",
      "[39/600] [300/331] Loss_D: 0.70565, Loss_G: 2.31202, Loss_KL: 0.08303\n",
      "[39/600] Loss_D: 0.77912, Loss_G: 3.05334, Loss_KL: 0.07947\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[40/600] [0/331] Loss_D: 0.57643, Loss_G: 3.75799, Loss_KL: 0.08054\n",
      "[40/600] [100/331] Loss_D: 0.70840, Loss_G: 1.71805, Loss_KL: 0.08000\n",
      "[40/600] [200/331] Loss_D: 0.71285, Loss_G: 3.02867, Loss_KL: 0.08418\n",
      "[40/600] [300/331] Loss_D: 0.34737, Loss_G: 2.19351, Loss_KL: 0.08209\n",
      "[40/600] Loss_D: 0.78723, Loss_G: 3.00863, Loss_KL: 0.07983\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[41/600] [0/331] Loss_D: 0.87036, Loss_G: 2.83567, Loss_KL: 0.09721\n",
      "[41/600] [100/331] Loss_D: 0.75415, Loss_G: 3.49076, Loss_KL: 0.09694\n",
      "[41/600] [200/331] Loss_D: 0.86753, Loss_G: 2.73193, Loss_KL: 0.09428\n",
      "[41/600] [300/331] Loss_D: 0.76176, Loss_G: 3.09565, Loss_KL: 0.07339\n",
      "[41/600] Loss_D: 0.78692, Loss_G: 2.96035, Loss_KL: 0.08276\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[42/600] [0/331] Loss_D: 0.62338, Loss_G: 3.21569, Loss_KL: 0.08645\n",
      "[42/600] [100/331] Loss_D: 0.79108, Loss_G: 3.17404, Loss_KL: 0.06588\n",
      "[42/600] [200/331] Loss_D: 0.97665, Loss_G: 4.29939, Loss_KL: 0.07116\n",
      "[42/600] [300/331] Loss_D: 1.02746, Loss_G: 6.00972, Loss_KL: 0.08122\n",
      "[42/600] Loss_D: 0.76553, Loss_G: 3.09771, Loss_KL: 0.08284\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[43/600] [0/331] Loss_D: 0.91720, Loss_G: 2.31768, Loss_KL: 0.08100\n",
      "[43/600] [100/331] Loss_D: 0.59439, Loss_G: 2.37416, Loss_KL: 0.10072\n",
      "[43/600] [200/331] Loss_D: 0.83064, Loss_G: 4.39497, Loss_KL: 0.06696\n",
      "[43/600] [300/331] Loss_D: 0.78657, Loss_G: 3.93363, Loss_KL: 0.08834\n",
      "[43/600] Loss_D: 0.78119, Loss_G: 3.01632, Loss_KL: 0.08124\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[44/600] [0/331] Loss_D: 1.35843, Loss_G: 3.65244, Loss_KL: 0.06635\n",
      "[44/600] [100/331] Loss_D: 0.82031, Loss_G: 4.28005, Loss_KL: 0.08778\n",
      "[44/600] [200/331] Loss_D: 0.67018, Loss_G: 2.48449, Loss_KL: 0.06553\n",
      "[44/600] [300/331] Loss_D: 0.88832, Loss_G: 4.09866, Loss_KL: 0.08843\n",
      "[44/600] Loss_D: 0.75049, Loss_G: 3.14443, Loss_KL: 0.08173\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[45/600] [0/331] Loss_D: 0.84165, Loss_G: 3.93575, Loss_KL: 0.08342\n",
      "[45/600] [100/331] Loss_D: 0.81329, Loss_G: 3.04124, Loss_KL: 0.07883\n",
      "[45/600] [200/331] Loss_D: 0.58341, Loss_G: 4.15774, Loss_KL: 0.09317\n",
      "[45/600] [300/331] Loss_D: 0.98480, Loss_G: 3.78587, Loss_KL: 0.08571\n",
      "[45/600] Loss_D: 0.75998, Loss_G: 3.19347, Loss_KL: 0.08274\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[46/600] [0/331] Loss_D: 0.49883, Loss_G: 5.36800, Loss_KL: 0.07645\n",
      "[46/600] [100/331] Loss_D: 0.56806, Loss_G: 3.21354, Loss_KL: 0.07224\n",
      "[46/600] [200/331] Loss_D: 0.71310, Loss_G: 2.31035, Loss_KL: 0.06956\n",
      "[46/600] [300/331] Loss_D: 0.75158, Loss_G: 2.59817, Loss_KL: 0.06071\n",
      "[46/600] Loss_D: 0.74535, Loss_G: 3.09853, Loss_KL: 0.08165\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[47/600] [0/331] Loss_D: 0.92213, Loss_G: 3.78147, Loss_KL: 0.07427\n",
      "[47/600] [100/331] Loss_D: 0.76509, Loss_G: 2.22154, Loss_KL: 0.05998\n",
      "[47/600] [200/331] Loss_D: 0.76797, Loss_G: 3.67632, Loss_KL: 0.06792\n",
      "[47/600] [300/331] Loss_D: 0.37701, Loss_G: 3.15873, Loss_KL: 0.08546\n",
      "[47/600] Loss_D: 0.74550, Loss_G: 3.20397, Loss_KL: 0.07689\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[48/600] [0/331] Loss_D: 0.90626, Loss_G: 2.01685, Loss_KL: 0.07478\n",
      "[48/600] [100/331] Loss_D: 0.64933, Loss_G: 3.01925, Loss_KL: 0.08663\n",
      "[48/600] [200/331] Loss_D: 0.70948, Loss_G: 5.73859, Loss_KL: 0.10633\n",
      "[48/600] [300/331] Loss_D: 0.73093, Loss_G: 2.45570, Loss_KL: 0.07831\n",
      "[48/600] Loss_D: 0.73635, Loss_G: 3.16528, Loss_KL: 0.08931\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[49/600] [0/331] Loss_D: 0.73100, Loss_G: 2.20655, Loss_KL: 0.09355\n",
      "[49/600] [100/331] Loss_D: 0.95621, Loss_G: 1.83006, Loss_KL: 0.06748\n",
      "[49/600] [200/331] Loss_D: 0.77168, Loss_G: 2.32159, Loss_KL: 0.08662\n",
      "[49/600] [300/331] Loss_D: 0.73836, Loss_G: 1.79236, Loss_KL: 0.08297\n",
      "[49/600] Loss_D: 0.76073, Loss_G: 3.14847, Loss_KL: 0.08563\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[50/600] [0/331] Loss_D: 0.74742, Loss_G: 3.00534, Loss_KL: 0.10974\n",
      "[50/600] [100/331] Loss_D: 0.41534, Loss_G: 3.02359, Loss_KL: 0.08273\n",
      "[50/600] [200/331] Loss_D: 0.90749, Loss_G: 3.86242, Loss_KL: 0.09014\n",
      "[50/600] [300/331] Loss_D: 0.71710, Loss_G: 4.65671, Loss_KL: 0.10506\n",
      "[50/600] Loss_D: 0.72650, Loss_G: 3.19562, Loss_KL: 0.08767\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[51/600] [0/331] Loss_D: 0.86024, Loss_G: 2.68291, Loss_KL: 0.08664\n",
      "[51/600] [100/331] Loss_D: 0.61826, Loss_G: 3.14549, Loss_KL: 0.08443\n",
      "[51/600] [200/331] Loss_D: 0.77396, Loss_G: 3.18633, Loss_KL: 0.08685\n",
      "[51/600] [300/331] Loss_D: 0.49152, Loss_G: 4.20998, Loss_KL: 0.05549\n",
      "[51/600] Loss_D: 0.71593, Loss_G: 3.30656, Loss_KL: 0.08983\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[52/600] [0/331] Loss_D: 0.57713, Loss_G: 3.62239, Loss_KL: 0.07610\n",
      "[52/600] [100/331] Loss_D: 0.46559, Loss_G: 4.18755, Loss_KL: 0.11802\n",
      "[52/600] [200/331] Loss_D: 0.72311, Loss_G: 5.35432, Loss_KL: 0.08764\n",
      "[52/600] [300/331] Loss_D: 1.23403, Loss_G: 3.43300, Loss_KL: 0.11394\n",
      "[52/600] Loss_D: 0.75917, Loss_G: 3.15639, Loss_KL: 0.09006\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[53/600] [0/331] Loss_D: 0.43903, Loss_G: 3.85509, Loss_KL: 0.11385\n",
      "[53/600] [100/331] Loss_D: 0.96955, Loss_G: 3.85865, Loss_KL: 0.07059\n",
      "[53/600] [200/331] Loss_D: 0.79155, Loss_G: 5.25969, Loss_KL: 0.07442\n",
      "[53/600] [300/331] Loss_D: 0.70837, Loss_G: 3.16990, Loss_KL: 0.04594\n",
      "[53/600] Loss_D: 0.77825, Loss_G: 3.09712, Loss_KL: 0.08346\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[54/600] [0/331] Loss_D: 0.82897, Loss_G: 3.41800, Loss_KL: 0.10809\n",
      "[54/600] [100/331] Loss_D: 0.65392, Loss_G: 4.43108, Loss_KL: 0.08912\n",
      "[54/600] [200/331] Loss_D: 0.57075, Loss_G: 3.83193, Loss_KL: 0.08179\n",
      "[54/600] [300/331] Loss_D: 0.49215, Loss_G: 3.16571, Loss_KL: 0.06799\n",
      "[54/600] Loss_D: 0.73803, Loss_G: 3.17745, Loss_KL: 0.09076\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[55/600] [0/331] Loss_D: 0.78300, Loss_G: 3.97714, Loss_KL: 0.09362\n",
      "[55/600] [100/331] Loss_D: 0.72017, Loss_G: 2.51850, Loss_KL: 0.09890\n",
      "[55/600] [200/331] Loss_D: 0.88817, Loss_G: 1.36048, Loss_KL: 0.08161\n",
      "[55/600] [300/331] Loss_D: 0.82735, Loss_G: 2.91599, Loss_KL: 0.09060\n",
      "[55/600] Loss_D: 0.73907, Loss_G: 3.16296, Loss_KL: 0.08839\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[56/600] [0/331] Loss_D: 0.40578, Loss_G: 3.94494, Loss_KL: 0.06508\n",
      "[56/600] [100/331] Loss_D: 0.48220, Loss_G: 2.51622, Loss_KL: 0.09145\n",
      "[56/600] [200/331] Loss_D: 0.74539, Loss_G: 3.37109, Loss_KL: 0.08203\n",
      "[56/600] [300/331] Loss_D: 0.83310, Loss_G: 2.87298, Loss_KL: 0.09590\n",
      "[56/600] Loss_D: 0.73581, Loss_G: 3.21745, Loss_KL: 0.08597\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[57/600] [0/331] Loss_D: 0.72413, Loss_G: 3.10072, Loss_KL: 0.11942\n",
      "[57/600] [100/331] Loss_D: 0.78670, Loss_G: 2.47395, Loss_KL: 0.09952\n",
      "[57/600] [200/331] Loss_D: 0.84223, Loss_G: 4.35906, Loss_KL: 0.08280\n",
      "[57/600] [300/331] Loss_D: 0.74482, Loss_G: 3.64009, Loss_KL: 0.12272\n",
      "[57/600] Loss_D: 0.72320, Loss_G: 3.25976, Loss_KL: 0.08886\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[58/600] [0/331] Loss_D: 0.38702, Loss_G: 1.81280, Loss_KL: 0.09316\n",
      "[58/600] [100/331] Loss_D: 0.36066, Loss_G: 3.69960, Loss_KL: 0.07864\n",
      "[58/600] [200/331] Loss_D: 0.63210, Loss_G: 4.34663, Loss_KL: 0.07605\n",
      "[58/600] [300/331] Loss_D: 0.71804, Loss_G: 1.71686, Loss_KL: 0.07713\n",
      "[58/600] Loss_D: 0.73448, Loss_G: 3.31452, Loss_KL: 0.08233\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[59/600] [0/331] Loss_D: 0.83142, Loss_G: 2.67977, Loss_KL: 0.08562\n",
      "[59/600] [100/331] Loss_D: 0.70481, Loss_G: 2.19666, Loss_KL: 0.08914\n",
      "[59/600] [200/331] Loss_D: 0.74021, Loss_G: 3.18368, Loss_KL: 0.10190\n",
      "[59/600] [300/331] Loss_D: 0.86800, Loss_G: 3.38774, Loss_KL: 0.07629\n",
      "[59/600] Loss_D: 0.70922, Loss_G: 3.26302, Loss_KL: 0.08340\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[60/600] [0/331] Loss_D: 1.09549, Loss_G: 3.97601, Loss_KL: 0.09035\n",
      "[60/600] [100/331] Loss_D: 0.88007, Loss_G: 1.78191, Loss_KL: 0.08427\n",
      "[60/600] [200/331] Loss_D: 0.78299, Loss_G: 2.26851, Loss_KL: 0.08405\n",
      "[60/600] [300/331] Loss_D: 0.69906, Loss_G: 2.88656, Loss_KL: 0.07665\n",
      "[60/600] Loss_D: 0.71207, Loss_G: 3.29805, Loss_KL: 0.08802\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[61/600] [0/331] Loss_D: 0.30152, Loss_G: 2.93464, Loss_KL: 0.08520\n",
      "[61/600] [100/331] Loss_D: 1.06675, Loss_G: 1.36457, Loss_KL: 0.11203\n",
      "[61/600] [200/331] Loss_D: 0.83630, Loss_G: 1.65841, Loss_KL: 0.06522\n",
      "[61/600] [300/331] Loss_D: 0.64268, Loss_G: 2.55496, Loss_KL: 0.07082\n",
      "[61/600] Loss_D: 0.69052, Loss_G: 3.40922, Loss_KL: 0.08849\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[62/600] [0/331] Loss_D: 0.50098, Loss_G: 2.80080, Loss_KL: 0.09571\n",
      "[62/600] [100/331] Loss_D: 0.30345, Loss_G: 4.74408, Loss_KL: 0.08268\n",
      "[62/600] [200/331] Loss_D: 0.53898, Loss_G: 3.20096, Loss_KL: 0.08447\n",
      "[62/600] [300/331] Loss_D: 0.79678, Loss_G: 2.70522, Loss_KL: 0.08437\n",
      "[62/600] Loss_D: 0.72519, Loss_G: 3.21364, Loss_KL: 0.08920\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[63/600] [0/331] Loss_D: 0.49486, Loss_G: 3.48301, Loss_KL: 0.08952\n",
      "[63/600] [100/331] Loss_D: 0.78508, Loss_G: 4.36976, Loss_KL: 0.09373\n",
      "[63/600] [200/331] Loss_D: 0.74783, Loss_G: 2.12669, Loss_KL: 0.08111\n",
      "[63/600] [300/331] Loss_D: 0.85318, Loss_G: 2.53781, Loss_KL: 0.06229\n",
      "[63/600] Loss_D: 0.71534, Loss_G: 3.21273, Loss_KL: 0.08289\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[64/600] [0/331] Loss_D: 0.33560, Loss_G: 3.09255, Loss_KL: 0.07598\n",
      "[64/600] [100/331] Loss_D: 0.61025, Loss_G: 4.02287, Loss_KL: 0.12135\n",
      "[64/600] [200/331] Loss_D: 0.23519, Loss_G: 3.56551, Loss_KL: 0.11670\n",
      "[64/600] [300/331] Loss_D: 0.59866, Loss_G: 2.70258, Loss_KL: 0.08512\n",
      "[64/600] Loss_D: 0.69547, Loss_G: 3.26224, Loss_KL: 0.08824\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[65/600] [0/331] Loss_D: 0.70945, Loss_G: 4.54572, Loss_KL: 0.06371\n",
      "[65/600] [100/331] Loss_D: 0.98325, Loss_G: 3.24007, Loss_KL: 0.09119\n",
      "[65/600] [200/331] Loss_D: 1.10230, Loss_G: 0.51519, Loss_KL: 0.08940\n",
      "[65/600] [300/331] Loss_D: 0.43053, Loss_G: 2.94677, Loss_KL: 0.06626\n",
      "[65/600] Loss_D: 0.69834, Loss_G: 3.33129, Loss_KL: 0.08211\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[66/600] [0/331] Loss_D: 0.67916, Loss_G: 2.36122, Loss_KL: 0.07114\n",
      "[66/600] [100/331] Loss_D: 0.22976, Loss_G: 3.60573, Loss_KL: 0.12639\n",
      "[66/600] [200/331] Loss_D: 0.44277, Loss_G: 3.47799, Loss_KL: 0.08912\n",
      "[66/600] [300/331] Loss_D: 1.10499, Loss_G: 0.73789, Loss_KL: 0.09528\n",
      "[66/600] Loss_D: 0.70451, Loss_G: 3.21007, Loss_KL: 0.08230\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[67/600] [0/331] Loss_D: 1.18631, Loss_G: 0.61070, Loss_KL: 0.09130\n",
      "[67/600] [100/331] Loss_D: 2.05591, Loss_G: 0.66614, Loss_KL: 0.07473\n",
      "[67/600] [200/331] Loss_D: 1.39628, Loss_G: 3.87677, Loss_KL: 0.10198\n",
      "[67/600] [300/331] Loss_D: 0.94121, Loss_G: 2.37610, Loss_KL: 0.10730\n",
      "[67/600] Loss_D: 0.71695, Loss_G: 3.14770, Loss_KL: 0.08797\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[68/600] [0/331] Loss_D: 0.68359, Loss_G: 3.47302, Loss_KL: 0.06353\n",
      "[68/600] [100/331] Loss_D: 0.37630, Loss_G: 4.43080, Loss_KL: 0.11708\n",
      "[68/600] [200/331] Loss_D: 0.83487, Loss_G: 2.25270, Loss_KL: 0.08552\n",
      "[68/600] [300/331] Loss_D: 0.85097, Loss_G: 3.74814, Loss_KL: 0.09405\n",
      "[68/600] Loss_D: 0.69251, Loss_G: 3.45113, Loss_KL: 0.09274\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[69/600] [0/331] Loss_D: 0.82912, Loss_G: 2.29471, Loss_KL: 0.07990\n",
      "[69/600] [100/331] Loss_D: 0.61404, Loss_G: 3.95651, Loss_KL: 0.08237\n",
      "[69/600] [200/331] Loss_D: 1.06043, Loss_G: 3.58389, Loss_KL: 0.12234\n",
      "[69/600] [300/331] Loss_D: 0.68660, Loss_G: 2.82337, Loss_KL: 0.10686\n",
      "[69/600] Loss_D: 0.70686, Loss_G: 3.20691, Loss_KL: 0.10225\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[70/600] [0/331] Loss_D: 0.65053, Loss_G: 5.02997, Loss_KL: 0.09728\n",
      "[70/600] [100/331] Loss_D: 0.66890, Loss_G: 1.88233, Loss_KL: 0.10065\n",
      "[70/600] [200/331] Loss_D: 0.69680, Loss_G: 2.04620, Loss_KL: 0.09655\n",
      "[70/600] [300/331] Loss_D: 0.84086, Loss_G: 4.25064, Loss_KL: 0.09117\n",
      "[70/600] Loss_D: 0.71527, Loss_G: 3.18544, Loss_KL: 0.09679\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[71/600] [0/331] Loss_D: 0.72076, Loss_G: 2.43393, Loss_KL: 0.08926\n",
      "[71/600] [100/331] Loss_D: 0.44686, Loss_G: 3.58236, Loss_KL: 0.10691\n",
      "[71/600] [200/331] Loss_D: 0.92589, Loss_G: 3.83876, Loss_KL: 0.07812\n",
      "[71/600] [300/331] Loss_D: 0.71618, Loss_G: 1.65182, Loss_KL: 0.09498\n",
      "[71/600] Loss_D: 0.69267, Loss_G: 3.27483, Loss_KL: 0.08967\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[72/600] [0/331] Loss_D: 0.61628, Loss_G: 2.20607, Loss_KL: 0.06971\n",
      "[72/600] [100/331] Loss_D: 0.43219, Loss_G: 3.38365, Loss_KL: 0.08346\n",
      "[72/600] [200/331] Loss_D: 0.83381, Loss_G: 2.14622, Loss_KL: 0.10388\n",
      "[72/600] [300/331] Loss_D: 0.83941, Loss_G: 3.55724, Loss_KL: 0.10784\n",
      "[72/600] Loss_D: 0.69107, Loss_G: 3.27558, Loss_KL: 0.08853\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[73/600] [0/331] Loss_D: 0.59395, Loss_G: 3.24707, Loss_KL: 0.10435\n",
      "[73/600] [100/331] Loss_D: 0.60350, Loss_G: 2.77687, Loss_KL: 0.08523\n",
      "[73/600] [200/331] Loss_D: 0.52254, Loss_G: 2.48010, Loss_KL: 0.10729\n",
      "[73/600] [300/331] Loss_D: 0.65674, Loss_G: 3.51584, Loss_KL: 0.07506\n",
      "[73/600] Loss_D: 0.68799, Loss_G: 3.27286, Loss_KL: 0.08986\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[74/600] [0/331] Loss_D: 0.67062, Loss_G: 3.44540, Loss_KL: 0.06813\n",
      "[74/600] [100/331] Loss_D: 0.94134, Loss_G: 4.41265, Loss_KL: 0.09844\n",
      "[74/600] [200/331] Loss_D: 1.03111, Loss_G: 4.00706, Loss_KL: 0.09647\n",
      "[74/600] [300/331] Loss_D: 1.15798, Loss_G: 3.71745, Loss_KL: 0.07756\n",
      "[74/600] Loss_D: 0.70156, Loss_G: 3.21364, Loss_KL: 0.09412\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[75/600] [0/331] Loss_D: 0.83700, Loss_G: 3.45475, Loss_KL: 0.13594\n",
      "[75/600] [100/331] Loss_D: 0.83826, Loss_G: 2.35701, Loss_KL: 0.08741\n",
      "[75/600] [200/331] Loss_D: 0.86295, Loss_G: 1.58220, Loss_KL: 0.11913\n",
      "[75/600] [300/331] Loss_D: 0.51027, Loss_G: 4.36851, Loss_KL: 0.08588\n",
      "[75/600] Loss_D: 0.71917, Loss_G: 3.15430, Loss_KL: 0.09912\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[76/600] [0/331] Loss_D: 0.75899, Loss_G: 3.68174, Loss_KL: 0.10001\n",
      "[76/600] [100/331] Loss_D: 1.17790, Loss_G: 5.19058, Loss_KL: 0.08744\n",
      "[76/600] [200/331] Loss_D: 0.73613, Loss_G: 3.09039, Loss_KL: 0.11407\n",
      "[76/600] [300/331] Loss_D: 0.56167, Loss_G: 4.12182, Loss_KL: 0.10114\n",
      "[76/600] Loss_D: 0.68928, Loss_G: 3.19460, Loss_KL: 0.09538\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[77/600] [0/331] Loss_D: 0.92234, Loss_G: 4.92113, Loss_KL: 0.12110\n",
      "[77/600] [100/331] Loss_D: 0.78189, Loss_G: 1.91815, Loss_KL: 0.09909\n",
      "[77/600] [200/331] Loss_D: 0.47207, Loss_G: 4.12126, Loss_KL: 0.10277\n",
      "[77/600] [300/331] Loss_D: 0.73393, Loss_G: 2.08330, Loss_KL: 0.10828\n",
      "[77/600] Loss_D: 0.69078, Loss_G: 3.19497, Loss_KL: 0.09319\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[78/600] [0/331] Loss_D: 0.64994, Loss_G: 3.20628, Loss_KL: 0.08161\n",
      "[78/600] [100/331] Loss_D: 0.41032, Loss_G: 3.93141, Loss_KL: 0.09530\n",
      "[78/600] [200/331] Loss_D: 1.14984, Loss_G: 0.33835, Loss_KL: 0.07964\n",
      "[78/600] [300/331] Loss_D: 0.41483, Loss_G: 2.98987, Loss_KL: 0.09838\n",
      "[78/600] Loss_D: 0.68430, Loss_G: 3.22804, Loss_KL: 0.08981\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[79/600] [0/331] Loss_D: 0.98012, Loss_G: 1.34233, Loss_KL: 0.08434\n",
      "[79/600] [100/331] Loss_D: 0.73453, Loss_G: 3.05324, Loss_KL: 0.09616\n",
      "[79/600] [200/331] Loss_D: 0.38534, Loss_G: 2.99548, Loss_KL: 0.10824\n",
      "[79/600] [300/331] Loss_D: 0.76419, Loss_G: 2.24051, Loss_KL: 0.10541\n",
      "[79/600] Loss_D: 0.68224, Loss_G: 3.29017, Loss_KL: 0.09191\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[80/600] [0/331] Loss_D: 0.67730, Loss_G: 5.74521, Loss_KL: 0.08567\n",
      "[80/600] [100/331] Loss_D: 0.37374, Loss_G: 2.71711, Loss_KL: 0.09299\n",
      "[80/600] [200/331] Loss_D: 0.78974, Loss_G: 2.63570, Loss_KL: 0.12350\n",
      "[80/600] [300/331] Loss_D: 0.54751, Loss_G: 3.62674, Loss_KL: 0.10483\n",
      "[80/600] Loss_D: 0.68325, Loss_G: 3.20959, Loss_KL: 0.09495\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Save G1/D1 models\n",
      "[81/600] [0/331] Loss_D: 0.70834, Loss_G: 2.64984, Loss_KL: 0.14035\n",
      "[81/600] [100/331] Loss_D: 0.75568, Loss_G: 3.26037, Loss_KL: 0.09841\n",
      "[81/600] [200/331] Loss_D: 0.71516, Loss_G: 2.21556, Loss_KL: 0.10532\n",
      "[81/600] [300/331] Loss_D: 0.83125, Loss_G: 3.88557, Loss_KL: 0.09928\n",
      "[81/600] Loss_D: 0.66962, Loss_G: 3.23910, Loss_KL: 0.09322\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[82/600] [0/331] Loss_D: 0.51371, Loss_G: 2.49117, Loss_KL: 0.08263\n",
      "[82/600] [100/331] Loss_D: 0.75510, Loss_G: 3.47008, Loss_KL: 0.06178\n",
      "[82/600] [200/331] Loss_D: 0.57608, Loss_G: 2.35525, Loss_KL: 0.08226\n",
      "[82/600] [300/331] Loss_D: 0.81369, Loss_G: 2.11993, Loss_KL: 0.05547\n",
      "[82/600] Loss_D: 0.66811, Loss_G: 3.39611, Loss_KL: 0.08861\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[83/600] [0/331] Loss_D: 0.71038, Loss_G: 2.10390, Loss_KL: 0.08352\n",
      "[83/600] [100/331] Loss_D: 0.46353, Loss_G: 3.37218, Loss_KL: 0.09846\n",
      "[83/600] [200/331] Loss_D: 0.84379, Loss_G: 3.34190, Loss_KL: 0.09476\n",
      "[83/600] [300/331] Loss_D: 0.76888, Loss_G: 4.61405, Loss_KL: 0.10127\n",
      "[83/600] Loss_D: 0.67551, Loss_G: 3.27582, Loss_KL: 0.08790\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[84/600] [0/331] Loss_D: 0.66637, Loss_G: 1.79109, Loss_KL: 0.08606\n",
      "[84/600] [100/331] Loss_D: 0.64041, Loss_G: 4.17613, Loss_KL: 0.08751\n",
      "[84/600] [200/331] Loss_D: 0.84088, Loss_G: 2.45242, Loss_KL: 0.08532\n",
      "[84/600] [300/331] Loss_D: 1.02281, Loss_G: 1.40971, Loss_KL: 0.09915\n",
      "[84/600] Loss_D: 0.68801, Loss_G: 3.24930, Loss_KL: 0.08439\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[85/600] [0/331] Loss_D: 0.59055, Loss_G: 2.92318, Loss_KL: 0.07532\n",
      "[85/600] [100/331] Loss_D: 0.47086, Loss_G: 2.77766, Loss_KL: 0.10379\n",
      "[85/600] [200/331] Loss_D: 0.35332, Loss_G: 4.19915, Loss_KL: 0.07136\n",
      "[85/600] [300/331] Loss_D: 0.37073, Loss_G: 2.26676, Loss_KL: 0.10321\n",
      "[85/600] Loss_D: 0.68172, Loss_G: 3.28256, Loss_KL: 0.08807\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[86/600] [0/331] Loss_D: 0.90107, Loss_G: 3.18905, Loss_KL: 0.07958\n",
      "[86/600] [100/331] Loss_D: 0.74621, Loss_G: 2.34475, Loss_KL: 0.09020\n",
      "[86/600] [200/331] Loss_D: 0.58746, Loss_G: 5.22616, Loss_KL: 0.10042\n",
      "[86/600] [300/331] Loss_D: 0.47713, Loss_G: 3.10296, Loss_KL: 0.07659\n",
      "[86/600] Loss_D: 0.67942, Loss_G: 3.33089, Loss_KL: 0.09108\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[87/600] [0/331] Loss_D: 0.85886, Loss_G: 2.00102, Loss_KL: 0.10191\n",
      "[87/600] [100/331] Loss_D: 0.87413, Loss_G: 5.30506, Loss_KL: 0.09117\n",
      "[87/600] [200/331] Loss_D: 1.08987, Loss_G: 3.64515, Loss_KL: 0.09437\n",
      "[87/600] [300/331] Loss_D: 0.63080, Loss_G: 1.38226, Loss_KL: 0.07700\n",
      "[87/600] Loss_D: 0.68190, Loss_G: 3.41568, Loss_KL: 0.08333\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[88/600] [0/331] Loss_D: 0.64379, Loss_G: 2.03242, Loss_KL: 0.06191\n",
      "[88/600] [100/331] Loss_D: 0.31458, Loss_G: 3.60478, Loss_KL: 0.08510\n",
      "[88/600] [200/331] Loss_D: 0.72174, Loss_G: 2.80167, Loss_KL: 0.09616\n",
      "[88/600] [300/331] Loss_D: 0.66907, Loss_G: 2.40223, Loss_KL: 0.08352\n",
      "[88/600] Loss_D: 0.67649, Loss_G: 3.29572, Loss_KL: 0.08895\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[89/600] [0/331] Loss_D: 0.61702, Loss_G: 2.94373, Loss_KL: 0.11930\n",
      "[89/600] [100/331] Loss_D: 0.80310, Loss_G: 3.07876, Loss_KL: 0.08294\n",
      "[89/600] [200/331] Loss_D: 1.07709, Loss_G: 3.15686, Loss_KL: 0.07744\n",
      "[89/600] [300/331] Loss_D: 0.65381, Loss_G: 3.19711, Loss_KL: 0.06496\n",
      "[89/600] Loss_D: 0.66910, Loss_G: 3.24832, Loss_KL: 0.09059\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[90/600] [0/331] Loss_D: 0.53480, Loss_G: 3.77312, Loss_KL: 0.06730\n",
      "[90/600] [100/331] Loss_D: 0.62404, Loss_G: 2.81630, Loss_KL: 0.09390\n",
      "[90/600] [200/331] Loss_D: 0.48683, Loss_G: 3.85873, Loss_KL: 0.07893\n",
      "[90/600] [300/331] Loss_D: 0.30327, Loss_G: 2.68663, Loss_KL: 0.07886\n",
      "[90/600] Loss_D: 0.68996, Loss_G: 3.21896, Loss_KL: 0.08305\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[91/600] [0/331] Loss_D: 0.85190, Loss_G: 4.07889, Loss_KL: 0.08052\n",
      "[91/600] [100/331] Loss_D: 0.64855, Loss_G: 3.68251, Loss_KL: 0.07829\n",
      "[91/600] [200/331] Loss_D: 0.72678, Loss_G: 2.16687, Loss_KL: 0.06383\n",
      "[91/600] [300/331] Loss_D: 0.92042, Loss_G: 4.58956, Loss_KL: 0.07733\n",
      "[91/600] Loss_D: 0.68497, Loss_G: 3.15072, Loss_KL: 0.08286\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[92/600] [0/331] Loss_D: 0.78955, Loss_G: 2.82108, Loss_KL: 0.09482\n",
      "[92/600] [100/331] Loss_D: 1.00485, Loss_G: 1.70782, Loss_KL: 0.07091\n",
      "[92/600] [200/331] Loss_D: 0.40904, Loss_G: 2.09158, Loss_KL: 0.07213\n",
      "[92/600] [300/331] Loss_D: 0.37236, Loss_G: 2.63819, Loss_KL: 0.07715\n",
      "[92/600] Loss_D: 0.66268, Loss_G: 3.32751, Loss_KL: 0.08597\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[93/600] [0/331] Loss_D: 0.87923, Loss_G: 3.17659, Loss_KL: 0.06759\n",
      "[93/600] [100/331] Loss_D: 0.44732, Loss_G: 3.74122, Loss_KL: 0.09407\n",
      "[93/600] [200/331] Loss_D: 0.60537, Loss_G: 3.14944, Loss_KL: 0.07017\n",
      "[93/600] [300/331] Loss_D: 0.67015, Loss_G: 2.83138, Loss_KL: 0.08029\n",
      "[93/600] Loss_D: 0.68471, Loss_G: 3.20715, Loss_KL: 0.08350\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[94/600] [0/331] Loss_D: 0.60014, Loss_G: 2.73033, Loss_KL: 0.11541\n",
      "[94/600] [100/331] Loss_D: 0.95278, Loss_G: 4.51057, Loss_KL: 0.06026\n",
      "[94/600] [200/331] Loss_D: 0.93263, Loss_G: 2.65759, Loss_KL: 0.07612\n",
      "[94/600] [300/331] Loss_D: 0.29418, Loss_G: 3.60127, Loss_KL: 0.06327\n",
      "[94/600] Loss_D: 0.68046, Loss_G: 3.32711, Loss_KL: 0.08297\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[95/600] [0/331] Loss_D: 0.86765, Loss_G: 4.30112, Loss_KL: 0.09738\n",
      "[95/600] [100/331] Loss_D: 0.76000, Loss_G: 2.96392, Loss_KL: 0.10738\n",
      "[95/600] [200/331] Loss_D: 0.40763, Loss_G: 3.89906, Loss_KL: 0.08370\n",
      "[95/600] [300/331] Loss_D: 0.82742, Loss_G: 3.11120, Loss_KL: 0.06946\n",
      "[95/600] Loss_D: 0.68456, Loss_G: 3.19170, Loss_KL: 0.08924\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[96/600] [0/331] Loss_D: 0.54150, Loss_G: 3.10666, Loss_KL: 0.09621\n",
      "[96/600] [100/331] Loss_D: 0.96541, Loss_G: 2.94774, Loss_KL: 0.09375\n",
      "[96/600] [200/331] Loss_D: 1.15705, Loss_G: 0.63907, Loss_KL: 0.10138\n",
      "[96/600] [300/331] Loss_D: 0.62406, Loss_G: 3.42809, Loss_KL: 0.07031\n",
      "[96/600] Loss_D: 0.66960, Loss_G: 3.19223, Loss_KL: 0.08851\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[97/600] [0/331] Loss_D: 0.85230, Loss_G: 2.42724, Loss_KL: 0.08019\n",
      "[97/600] [100/331] Loss_D: 0.73773, Loss_G: 4.08255, Loss_KL: 0.08934\n",
      "[97/600] [200/331] Loss_D: 0.75588, Loss_G: 3.88922, Loss_KL: 0.07824\n",
      "[97/600] [300/331] Loss_D: 0.74168, Loss_G: 4.02445, Loss_KL: 0.09004\n",
      "[97/600] Loss_D: 0.66528, Loss_G: 3.26817, Loss_KL: 0.09123\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[98/600] [0/331] Loss_D: 0.62453, Loss_G: 2.61616, Loss_KL: 0.09425\n",
      "[98/600] [100/331] Loss_D: 0.96805, Loss_G: 4.48018, Loss_KL: 0.10023\n",
      "[98/600] [200/331] Loss_D: 0.81212, Loss_G: 3.58451, Loss_KL: 0.08274\n",
      "[98/600] [300/331] Loss_D: 0.48804, Loss_G: 3.37078, Loss_KL: 0.10906\n",
      "[98/600] Loss_D: 0.68302, Loss_G: 3.18264, Loss_KL: 0.09053\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[99/600] [0/331] Loss_D: 0.90111, Loss_G: 2.76123, Loss_KL: 0.08791\n",
      "[99/600] [100/331] Loss_D: 0.81239, Loss_G: 3.02549, Loss_KL: 0.08520\n",
      "[99/600] [200/331] Loss_D: 0.69658, Loss_G: 2.26480, Loss_KL: 0.07854\n",
      "[99/600] [300/331] Loss_D: 0.63019, Loss_G: 4.42836, Loss_KL: 0.10858\n",
      "[99/600] Loss_D: 0.67050, Loss_G: 3.13228, Loss_KL: 0.08918\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "Adjusting learning rate of group 0 to 2.0000e-04.\n",
      "[100/600] [0/331] Loss_D: 0.77836, Loss_G: 2.70715, Loss_KL: 0.09074\n",
      "[100/600] [100/331] Loss_D: 0.50545, Loss_G: 5.19631, Loss_KL: 0.07435\n",
      "[100/600] [200/331] Loss_D: 0.66395, Loss_G: 2.67307, Loss_KL: 0.07090\n",
      "[100/600] [300/331] Loss_D: 0.39156, Loss_G: 4.40484, Loss_KL: 0.06191\n",
      "[100/600] Loss_D: 0.68039, Loss_G: 3.11838, Loss_KL: 0.08433\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[101/600] [0/331] Loss_D: 0.69109, Loss_G: 3.11457, Loss_KL: 0.08124\n",
      "[101/600] [100/331] Loss_D: 0.87232, Loss_G: 2.29626, Loss_KL: 0.10700\n",
      "[101/600] [200/331] Loss_D: 0.73401, Loss_G: 2.60363, Loss_KL: 0.09115\n",
      "[101/600] [300/331] Loss_D: 0.71310, Loss_G: 2.68460, Loss_KL: 0.06683\n",
      "[101/600] Loss_D: 0.65594, Loss_G: 2.79277, Loss_KL: 0.08553\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[102/600] [0/331] Loss_D: 0.73291, Loss_G: 2.87481, Loss_KL: 0.07641\n",
      "[102/600] [100/331] Loss_D: 0.73655, Loss_G: 2.43101, Loss_KL: 0.05575\n",
      "[102/600] [200/331] Loss_D: 0.55559, Loss_G: 3.63394, Loss_KL: 0.06524\n",
      "[102/600] [300/331] Loss_D: 0.58166, Loss_G: 1.78738, Loss_KL: 0.07155\n",
      "[102/600] Loss_D: 0.63930, Loss_G: 2.81439, Loss_KL: 0.07693\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[103/600] [0/331] Loss_D: 0.40126, Loss_G: 2.80586, Loss_KL: 0.07308\n",
      "[103/600] [100/331] Loss_D: 0.54842, Loss_G: 2.33609, Loss_KL: 0.05579\n",
      "[103/600] [200/331] Loss_D: 0.50341, Loss_G: 2.40577, Loss_KL: 0.06387\n",
      "[103/600] [300/331] Loss_D: 0.81857, Loss_G: 3.70647, Loss_KL: 0.07425\n",
      "[103/600] Loss_D: 0.65933, Loss_G: 2.80494, Loss_KL: 0.07464\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[104/600] [0/331] Loss_D: 0.49496, Loss_G: 3.11390, Loss_KL: 0.09582\n",
      "[104/600] [100/331] Loss_D: 0.88784, Loss_G: 3.38186, Loss_KL: 0.05561\n",
      "[104/600] [200/331] Loss_D: 0.78288, Loss_G: 3.92355, Loss_KL: 0.07666\n",
      "[104/600] [300/331] Loss_D: 0.70811, Loss_G: 3.63533, Loss_KL: 0.09010\n",
      "[104/600] Loss_D: 0.66985, Loss_G: 2.82079, Loss_KL: 0.07959\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[105/600] [0/331] Loss_D: 0.28544, Loss_G: 3.42498, Loss_KL: 0.08205\n",
      "[105/600] [100/331] Loss_D: 0.76209, Loss_G: 2.78509, Loss_KL: 0.07410\n",
      "[105/600] [200/331] Loss_D: 0.88703, Loss_G: 2.86205, Loss_KL: 0.07256\n",
      "[105/600] [300/331] Loss_D: 0.82023, Loss_G: 2.27317, Loss_KL: 0.07778\n",
      "[105/600] Loss_D: 0.62428, Loss_G: 2.81011, Loss_KL: 0.08255\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[106/600] [0/331] Loss_D: 0.69541, Loss_G: 2.95739, Loss_KL: 0.11607\n",
      "[106/600] [100/331] Loss_D: 0.83908, Loss_G: 2.42458, Loss_KL: 0.09075\n",
      "[106/600] [200/331] Loss_D: 0.95168, Loss_G: 3.57800, Loss_KL: 0.09004\n",
      "[106/600] [300/331] Loss_D: 0.72707, Loss_G: 1.88419, Loss_KL: 0.06938\n",
      "[106/600] Loss_D: 0.62016, Loss_G: 3.00400, Loss_KL: 0.08911\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[107/600] [0/331] Loss_D: 0.59748, Loss_G: 2.84502, Loss_KL: 0.06308\n",
      "[107/600] [100/331] Loss_D: 0.74795, Loss_G: 2.47767, Loss_KL: 0.06870\n",
      "[107/600] [200/331] Loss_D: 0.83795, Loss_G: 2.77600, Loss_KL: 0.07555\n",
      "[107/600] [300/331] Loss_D: 0.25195, Loss_G: 3.55839, Loss_KL: 0.04979\n",
      "[107/600] Loss_D: 0.63031, Loss_G: 2.94928, Loss_KL: 0.08242\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[108/600] [0/331] Loss_D: 0.64102, Loss_G: 2.92805, Loss_KL: 0.07569\n",
      "[108/600] [100/331] Loss_D: 0.72256, Loss_G: 2.64478, Loss_KL: 0.09489\n",
      "[108/600] [200/331] Loss_D: 0.62829, Loss_G: 2.98280, Loss_KL: 0.09530\n",
      "[108/600] [300/331] Loss_D: 0.89228, Loss_G: 2.06025, Loss_KL: 0.07829\n",
      "[108/600] Loss_D: 0.62044, Loss_G: 2.90042, Loss_KL: 0.08012\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[109/600] [0/331] Loss_D: 1.10905, Loss_G: 3.45708, Loss_KL: 0.05950\n",
      "[109/600] [100/331] Loss_D: 0.16458, Loss_G: 3.43995, Loss_KL: 0.08695\n",
      "[109/600] [200/331] Loss_D: 0.66458, Loss_G: 3.52699, Loss_KL: 0.08805\n",
      "[109/600] [300/331] Loss_D: 0.69962, Loss_G: 2.74222, Loss_KL: 0.07322\n",
      "[109/600] Loss_D: 0.63679, Loss_G: 2.93712, Loss_KL: 0.08168\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[110/600] [0/331] Loss_D: 0.48514, Loss_G: 2.20486, Loss_KL: 0.06456\n",
      "[110/600] [100/331] Loss_D: 0.66536, Loss_G: 3.78208, Loss_KL: 0.08940\n",
      "[110/600] [200/331] Loss_D: 0.41140, Loss_G: 1.86648, Loss_KL: 0.08194\n",
      "[110/600] [300/331] Loss_D: 0.38057, Loss_G: 3.69667, Loss_KL: 0.12173\n",
      "[110/600] Loss_D: 0.61435, Loss_G: 2.97033, Loss_KL: 0.08394\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[111/600] [0/331] Loss_D: 0.92053, Loss_G: 2.34021, Loss_KL: 0.09049\n",
      "[111/600] [100/331] Loss_D: 0.69902, Loss_G: 3.70611, Loss_KL: 0.08488\n",
      "[111/600] [200/331] Loss_D: 0.66777, Loss_G: 2.82624, Loss_KL: 0.07404\n",
      "[111/600] [300/331] Loss_D: 0.72110, Loss_G: 5.47443, Loss_KL: 0.06639\n",
      "[111/600] Loss_D: 0.63445, Loss_G: 3.03179, Loss_KL: 0.08443\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[112/600] [0/331] Loss_D: 0.76122, Loss_G: 4.37613, Loss_KL: 0.09242\n",
      "[112/600] [100/331] Loss_D: 0.72563, Loss_G: 3.04872, Loss_KL: 0.10822\n",
      "[112/600] [200/331] Loss_D: 0.75829, Loss_G: 2.85312, Loss_KL: 0.10030\n",
      "[112/600] [300/331] Loss_D: 0.82637, Loss_G: 2.58861, Loss_KL: 0.09033\n",
      "[112/600] Loss_D: 0.63819, Loss_G: 2.87340, Loss_KL: 0.08734\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[113/600] [0/331] Loss_D: 0.50487, Loss_G: 3.51084, Loss_KL: 0.09787\n",
      "[113/600] [100/331] Loss_D: 0.63709, Loss_G: 2.64845, Loss_KL: 0.06986\n",
      "[113/600] [200/331] Loss_D: 0.42568, Loss_G: 3.67140, Loss_KL: 0.08329\n",
      "[113/600] [300/331] Loss_D: 0.45806, Loss_G: 4.13184, Loss_KL: 0.07722\n",
      "[113/600] Loss_D: 0.63138, Loss_G: 2.95015, Loss_KL: 0.08810\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[114/600] [0/331] Loss_D: 0.21015, Loss_G: 4.15837, Loss_KL: 0.05516\n",
      "[114/600] [100/331] Loss_D: 0.28577, Loss_G: 2.96904, Loss_KL: 0.09487\n",
      "[114/600] [200/331] Loss_D: 0.63279, Loss_G: 2.39164, Loss_KL: 0.11379\n",
      "[114/600] [300/331] Loss_D: 0.80162, Loss_G: 2.54693, Loss_KL: 0.06672\n",
      "[114/600] Loss_D: 0.63788, Loss_G: 2.86909, Loss_KL: 0.08488\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[115/600] [0/331] Loss_D: 0.57887, Loss_G: 3.04769, Loss_KL: 0.09529\n",
      "[115/600] [100/331] Loss_D: 0.56279, Loss_G: 1.73234, Loss_KL: 0.06297\n",
      "[115/600] [200/331] Loss_D: 0.61518, Loss_G: 2.89539, Loss_KL: 0.10052\n",
      "[115/600] [300/331] Loss_D: 0.74933, Loss_G: 3.42116, Loss_KL: 0.10907\n",
      "[115/600] Loss_D: 0.63246, Loss_G: 3.01041, Loss_KL: 0.08481\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[116/600] [0/331] Loss_D: 0.39695, Loss_G: 2.89949, Loss_KL: 0.08238\n",
      "[116/600] [100/331] Loss_D: 0.76414, Loss_G: 2.92258, Loss_KL: 0.09034\n",
      "[116/600] [200/331] Loss_D: 0.75446, Loss_G: 2.45408, Loss_KL: 0.05900\n",
      "[116/600] [300/331] Loss_D: 0.77523, Loss_G: 2.85424, Loss_KL: 0.10480\n",
      "[116/600] Loss_D: 0.62143, Loss_G: 2.92578, Loss_KL: 0.08030\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[117/600] [0/331] Loss_D: 0.52749, Loss_G: 2.69213, Loss_KL: 0.07769\n",
      "[117/600] [100/331] Loss_D: 0.76162, Loss_G: 2.91657, Loss_KL: 0.09136\n",
      "[117/600] [200/331] Loss_D: 0.84209, Loss_G: 2.51091, Loss_KL: 0.07896\n",
      "[117/600] [300/331] Loss_D: 0.73587, Loss_G: 3.29535, Loss_KL: 0.07904\n",
      "[117/600] Loss_D: 0.62565, Loss_G: 2.85682, Loss_KL: 0.08030\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[118/600] [0/331] Loss_D: 0.65538, Loss_G: 2.67550, Loss_KL: 0.09475\n",
      "[118/600] [100/331] Loss_D: 0.86185, Loss_G: 2.21827, Loss_KL: 0.07833\n",
      "[118/600] [200/331] Loss_D: 0.60990, Loss_G: 2.10485, Loss_KL: 0.11014\n",
      "[118/600] [300/331] Loss_D: 0.76376, Loss_G: 4.27970, Loss_KL: 0.09085\n",
      "[118/600] Loss_D: 0.62697, Loss_G: 2.97155, Loss_KL: 0.08000\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[119/600] [0/331] Loss_D: 0.73611, Loss_G: 1.99566, Loss_KL: 0.10344\n",
      "[119/600] [100/331] Loss_D: 0.41763, Loss_G: 2.49389, Loss_KL: 0.09522\n",
      "[119/600] [200/331] Loss_D: 0.79339, Loss_G: 2.40743, Loss_KL: 0.08391\n",
      "[119/600] [300/331] Loss_D: 0.63918, Loss_G: 3.29857, Loss_KL: 0.07841\n",
      "[119/600] Loss_D: 0.63382, Loss_G: 2.98204, Loss_KL: 0.08090\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[120/600] [0/331] Loss_D: 0.56107, Loss_G: 4.04073, Loss_KL: 0.09878\n",
      "[120/600] [100/331] Loss_D: 0.82620, Loss_G: 1.88232, Loss_KL: 0.09795\n",
      "[120/600] [200/331] Loss_D: 0.67891, Loss_G: 2.12127, Loss_KL: 0.11744\n",
      "[120/600] [300/331] Loss_D: 0.32880, Loss_G: 3.47405, Loss_KL: 0.06299\n",
      "[120/600] Loss_D: 0.64810, Loss_G: 2.82541, Loss_KL: 0.07833\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[121/600] [0/331] Loss_D: 0.76720, Loss_G: 2.18160, Loss_KL: 0.05920\n",
      "[121/600] [100/331] Loss_D: 0.66894, Loss_G: 3.91018, Loss_KL: 0.08639\n",
      "[121/600] [200/331] Loss_D: 0.61117, Loss_G: 2.09354, Loss_KL: 0.07319\n",
      "[121/600] [300/331] Loss_D: 0.56306, Loss_G: 2.07051, Loss_KL: 0.06908\n",
      "[121/600] Loss_D: 0.62039, Loss_G: 2.96282, Loss_KL: 0.07916\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[122/600] [0/331] Loss_D: 0.91852, Loss_G: 2.90746, Loss_KL: 0.06559\n",
      "[122/600] [100/331] Loss_D: 0.94411, Loss_G: 3.36419, Loss_KL: 0.10697\n",
      "[122/600] [200/331] Loss_D: 0.55483, Loss_G: 3.96629, Loss_KL: 0.07143\n",
      "[122/600] [300/331] Loss_D: 0.73449, Loss_G: 2.63431, Loss_KL: 0.08791\n",
      "[122/600] Loss_D: 0.62870, Loss_G: 2.94108, Loss_KL: 0.08047\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[123/600] [0/331] Loss_D: 0.81009, Loss_G: 2.99386, Loss_KL: 0.07870\n",
      "[123/600] [100/331] Loss_D: 0.74562, Loss_G: 3.68114, Loss_KL: 0.08866\n",
      "[123/600] [200/331] Loss_D: 0.59571, Loss_G: 2.99173, Loss_KL: 0.08454\n",
      "[123/600] [300/331] Loss_D: 0.65096, Loss_G: 4.63580, Loss_KL: 0.09017\n",
      "[123/600] Loss_D: 0.62893, Loss_G: 2.92251, Loss_KL: 0.07892\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[124/600] [0/331] Loss_D: 0.37091, Loss_G: 4.04992, Loss_KL: 0.08024\n",
      "[124/600] [100/331] Loss_D: 0.70047, Loss_G: 2.02958, Loss_KL: 0.07735\n",
      "[124/600] [200/331] Loss_D: 0.67304, Loss_G: 2.70644, Loss_KL: 0.06319\n",
      "[124/600] [300/331] Loss_D: 0.40121, Loss_G: 3.60950, Loss_KL: 0.07806\n",
      "[124/600] Loss_D: 0.61442, Loss_G: 2.96614, Loss_KL: 0.07699\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[125/600] [0/331] Loss_D: 0.70543, Loss_G: 2.23719, Loss_KL: 0.05531\n",
      "[125/600] [100/331] Loss_D: 0.75655, Loss_G: 2.51886, Loss_KL: 0.10941\n",
      "[125/600] [200/331] Loss_D: 0.63091, Loss_G: 3.46486, Loss_KL: 0.07771\n",
      "[125/600] [300/331] Loss_D: 0.37029, Loss_G: 2.54997, Loss_KL: 0.06284\n",
      "[125/600] Loss_D: 0.62854, Loss_G: 2.89931, Loss_KL: 0.07610\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[126/600] [0/331] Loss_D: 0.28499, Loss_G: 2.44464, Loss_KL: 0.06237\n",
      "[126/600] [100/331] Loss_D: 0.71941, Loss_G: 2.69534, Loss_KL: 0.06747\n",
      "[126/600] [200/331] Loss_D: 0.78492, Loss_G: 2.85071, Loss_KL: 0.07340\n",
      "[126/600] [300/331] Loss_D: 0.31260, Loss_G: 3.38588, Loss_KL: 0.06105\n",
      "[126/600] Loss_D: 0.62091, Loss_G: 2.88541, Loss_KL: 0.07603\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[127/600] [0/331] Loss_D: 0.54523, Loss_G: 3.67451, Loss_KL: 0.09717\n",
      "[127/600] [100/331] Loss_D: 0.76168, Loss_G: 2.70620, Loss_KL: 0.07490\n",
      "[127/600] [200/331] Loss_D: 0.65319, Loss_G: 2.41104, Loss_KL: 0.05713\n",
      "[127/600] [300/331] Loss_D: 0.67592, Loss_G: 3.11196, Loss_KL: 0.09539\n",
      "[127/600] Loss_D: 0.62665, Loss_G: 2.90174, Loss_KL: 0.07546\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[128/600] [0/331] Loss_D: 0.69835, Loss_G: 4.10430, Loss_KL: 0.11076\n",
      "[128/600] [100/331] Loss_D: 0.73182, Loss_G: 2.34684, Loss_KL: 0.08138\n",
      "[128/600] [200/331] Loss_D: 0.71037, Loss_G: 2.25326, Loss_KL: 0.07033\n",
      "[128/600] [300/331] Loss_D: 0.53121, Loss_G: 3.42415, Loss_KL: 0.05914\n",
      "[128/600] Loss_D: 0.61540, Loss_G: 2.87529, Loss_KL: 0.07681\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[129/600] [0/331] Loss_D: 0.31126, Loss_G: 2.77298, Loss_KL: 0.08082\n",
      "[129/600] [100/331] Loss_D: 0.73999, Loss_G: 2.65032, Loss_KL: 0.05999\n",
      "[129/600] [200/331] Loss_D: 0.73459, Loss_G: 3.25823, Loss_KL: 0.06166\n",
      "[129/600] [300/331] Loss_D: 0.60315, Loss_G: 2.27423, Loss_KL: 0.08007\n",
      "[129/600] Loss_D: 0.62663, Loss_G: 2.90592, Loss_KL: 0.07882\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[130/600] [0/331] Loss_D: 0.71279, Loss_G: 2.74919, Loss_KL: 0.08320\n",
      "[130/600] [100/331] Loss_D: 0.66364, Loss_G: 2.30922, Loss_KL: 0.05855\n",
      "[130/600] [200/331] Loss_D: 0.79565, Loss_G: 2.15001, Loss_KL: 0.06140\n",
      "[130/600] [300/331] Loss_D: 0.46641, Loss_G: 2.69377, Loss_KL: 0.08751\n",
      "[130/600] Loss_D: 0.61822, Loss_G: 2.88654, Loss_KL: 0.07811\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[131/600] [0/331] Loss_D: 0.56947, Loss_G: 3.87543, Loss_KL: 0.06385\n",
      "[131/600] [100/331] Loss_D: 0.75275, Loss_G: 2.78522, Loss_KL: 0.08438\n",
      "[131/600] [200/331] Loss_D: 0.73306, Loss_G: 2.62240, Loss_KL: 0.06299\n",
      "[131/600] [300/331] Loss_D: 0.79299, Loss_G: 2.19240, Loss_KL: 0.07474\n",
      "[131/600] Loss_D: 0.61903, Loss_G: 2.89907, Loss_KL: 0.07619\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[132/600] [0/331] Loss_D: 0.91346, Loss_G: 3.59114, Loss_KL: 0.07283\n",
      "[132/600] [100/331] Loss_D: 0.41770, Loss_G: 2.22469, Loss_KL: 0.06325\n",
      "[132/600] [200/331] Loss_D: 0.19924, Loss_G: 3.55851, Loss_KL: 0.06370\n",
      "[132/600] [300/331] Loss_D: 0.34454, Loss_G: 3.43232, Loss_KL: 0.08722\n",
      "[132/600] Loss_D: 0.60878, Loss_G: 2.89812, Loss_KL: 0.07660\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[133/600] [0/331] Loss_D: 0.72585, Loss_G: 2.25386, Loss_KL: 0.06460\n",
      "[133/600] [100/331] Loss_D: 0.48456, Loss_G: 2.53222, Loss_KL: 0.09126\n",
      "[133/600] [200/331] Loss_D: 0.65149, Loss_G: 2.81916, Loss_KL: 0.09444\n",
      "[133/600] [300/331] Loss_D: 0.40642, Loss_G: 3.57519, Loss_KL: 0.09234\n",
      "[133/600] Loss_D: 0.60743, Loss_G: 3.00430, Loss_KL: 0.08085\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[134/600] [0/331] Loss_D: 0.73425, Loss_G: 2.64902, Loss_KL: 0.07817\n",
      "[134/600] [100/331] Loss_D: 0.30960, Loss_G: 3.32690, Loss_KL: 0.05689\n",
      "[134/600] [200/331] Loss_D: 0.76588, Loss_G: 2.32621, Loss_KL: 0.08324\n",
      "[134/600] [300/331] Loss_D: 0.54403, Loss_G: 2.43141, Loss_KL: 0.08276\n",
      "[134/600] Loss_D: 0.64155, Loss_G: 2.89164, Loss_KL: 0.08130\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[135/600] [0/331] Loss_D: 0.42975, Loss_G: 3.82644, Loss_KL: 0.06791\n",
      "[135/600] [100/331] Loss_D: 0.95800, Loss_G: 4.12832, Loss_KL: 0.05122\n",
      "[135/600] [200/331] Loss_D: 0.57635, Loss_G: 3.39936, Loss_KL: 0.08212\n",
      "[135/600] [300/331] Loss_D: 0.60680, Loss_G: 2.86139, Loss_KL: 0.09146\n",
      "[135/600] Loss_D: 0.61548, Loss_G: 2.88083, Loss_KL: 0.07911\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[136/600] [0/331] Loss_D: 0.30886, Loss_G: 3.41730, Loss_KL: 0.10888\n",
      "[136/600] [100/331] Loss_D: 0.62610, Loss_G: 3.27293, Loss_KL: 0.10310\n",
      "[136/600] [200/331] Loss_D: 0.38829, Loss_G: 2.74316, Loss_KL: 0.09781\n",
      "[136/600] [300/331] Loss_D: 0.70368, Loss_G: 4.54211, Loss_KL: 0.07526\n",
      "[136/600] Loss_D: 0.61613, Loss_G: 2.95281, Loss_KL: 0.08030\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[137/600] [0/331] Loss_D: 0.67993, Loss_G: 3.43455, Loss_KL: 0.07495\n",
      "[137/600] [100/331] Loss_D: 0.77301, Loss_G: 2.46261, Loss_KL: 0.08198\n",
      "[137/600] [200/331] Loss_D: 0.27697, Loss_G: 3.94339, Loss_KL: 0.06746\n",
      "[137/600] [300/331] Loss_D: 0.72795, Loss_G: 3.05085, Loss_KL: 0.11753\n",
      "[137/600] Loss_D: 0.62049, Loss_G: 2.87116, Loss_KL: 0.07869\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[138/600] [0/331] Loss_D: 0.64185, Loss_G: 2.41945, Loss_KL: 0.06818\n",
      "[138/600] [100/331] Loss_D: 0.68730, Loss_G: 2.87916, Loss_KL: 0.08666\n",
      "[138/600] [200/331] Loss_D: 0.65617, Loss_G: 2.25158, Loss_KL: 0.07938\n",
      "[138/600] [300/331] Loss_D: 0.45784, Loss_G: 2.20196, Loss_KL: 0.07282\n",
      "[138/600] Loss_D: 0.61754, Loss_G: 2.83388, Loss_KL: 0.07644\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[139/600] [0/331] Loss_D: 0.66167, Loss_G: 2.57231, Loss_KL: 0.05740\n",
      "[139/600] [100/331] Loss_D: 0.29584, Loss_G: 2.96074, Loss_KL: 0.08435\n",
      "[139/600] [200/331] Loss_D: 0.64647, Loss_G: 2.83950, Loss_KL: 0.10108\n",
      "[139/600] [300/331] Loss_D: 0.54441, Loss_G: 2.89248, Loss_KL: 0.08268\n",
      "[139/600] Loss_D: 0.61556, Loss_G: 2.93790, Loss_KL: 0.07462\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[140/600] [0/331] Loss_D: 0.60165, Loss_G: 2.18223, Loss_KL: 0.08767\n",
      "[140/600] [100/331] Loss_D: 0.59042, Loss_G: 2.93982, Loss_KL: 0.08451\n",
      "[140/600] [200/331] Loss_D: 0.68894, Loss_G: 2.52654, Loss_KL: 0.08197\n",
      "[140/600] [300/331] Loss_D: 0.86470, Loss_G: 2.97805, Loss_KL: 0.08736\n",
      "[140/600] Loss_D: 0.62183, Loss_G: 2.86688, Loss_KL: 0.07387\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[141/600] [0/331] Loss_D: 0.80802, Loss_G: 2.98558, Loss_KL: 0.11758\n",
      "[141/600] [100/331] Loss_D: 0.65260, Loss_G: 2.67442, Loss_KL: 0.07056\n",
      "[141/600] [200/331] Loss_D: 0.45710, Loss_G: 3.15562, Loss_KL: 0.05336\n",
      "[141/600] [300/331] Loss_D: 0.35745, Loss_G: 3.17631, Loss_KL: 0.08102\n",
      "[141/600] Loss_D: 0.63266, Loss_G: 2.89921, Loss_KL: 0.07423\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[142/600] [0/331] Loss_D: 0.76052, Loss_G: 2.28497, Loss_KL: 0.06297\n",
      "[142/600] [100/331] Loss_D: 0.77279, Loss_G: 3.12841, Loss_KL: 0.08075\n",
      "[142/600] [200/331] Loss_D: 0.56099, Loss_G: 3.25710, Loss_KL: 0.06900\n",
      "[142/600] [300/331] Loss_D: 0.79901, Loss_G: 2.44636, Loss_KL: 0.07250\n",
      "[142/600] Loss_D: 0.61882, Loss_G: 2.85497, Loss_KL: 0.07528\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[143/600] [0/331] Loss_D: 0.49517, Loss_G: 3.26300, Loss_KL: 0.05304\n",
      "[143/600] [100/331] Loss_D: 0.34796, Loss_G: 2.94303, Loss_KL: 0.08391\n",
      "[143/600] [200/331] Loss_D: 0.68960, Loss_G: 2.38202, Loss_KL: 0.06495\n",
      "[143/600] [300/331] Loss_D: 0.75116, Loss_G: 2.68989, Loss_KL: 0.07302\n",
      "[143/600] Loss_D: 0.61516, Loss_G: 2.86626, Loss_KL: 0.07515\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[144/600] [0/331] Loss_D: 0.47820, Loss_G: 4.08315, Loss_KL: 0.09966\n",
      "[144/600] [100/331] Loss_D: 0.74609, Loss_G: 3.04931, Loss_KL: 0.10842\n",
      "[144/600] [200/331] Loss_D: 0.70403, Loss_G: 3.25723, Loss_KL: 0.09570\n",
      "[144/600] [300/331] Loss_D: 1.05709, Loss_G: 2.34869, Loss_KL: 0.07754\n",
      "[144/600] Loss_D: 0.62403, Loss_G: 2.87096, Loss_KL: 0.07454\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[145/600] [0/331] Loss_D: 0.55047, Loss_G: 2.56177, Loss_KL: 0.06770\n",
      "[145/600] [100/331] Loss_D: 0.79473, Loss_G: 3.61009, Loss_KL: 0.07836\n",
      "[145/600] [200/331] Loss_D: 0.83107, Loss_G: 1.50430, Loss_KL: 0.05762\n",
      "[145/600] [300/331] Loss_D: 0.61906, Loss_G: 3.52252, Loss_KL: 0.08908\n",
      "[145/600] Loss_D: 0.61056, Loss_G: 2.91797, Loss_KL: 0.07146\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[146/600] [0/331] Loss_D: 0.54229, Loss_G: 3.32712, Loss_KL: 0.07569\n",
      "[146/600] [100/331] Loss_D: 0.75846, Loss_G: 2.43997, Loss_KL: 0.07927\n",
      "[146/600] [200/331] Loss_D: 0.74611, Loss_G: 2.25283, Loss_KL: 0.05335\n",
      "[146/600] [300/331] Loss_D: 0.45151, Loss_G: 3.69236, Loss_KL: 0.05446\n",
      "[146/600] Loss_D: 0.60461, Loss_G: 2.90385, Loss_KL: 0.07045\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[147/600] [0/331] Loss_D: 0.13933, Loss_G: 3.36843, Loss_KL: 0.06350\n",
      "[147/600] [100/331] Loss_D: 0.90867, Loss_G: 2.03871, Loss_KL: 0.04999\n",
      "[147/600] [200/331] Loss_D: 0.47416, Loss_G: 2.74402, Loss_KL: 0.09289\n",
      "[147/600] [300/331] Loss_D: 0.66258, Loss_G: 3.90391, Loss_KL: 0.06855\n",
      "[147/600] Loss_D: 0.59774, Loss_G: 2.99486, Loss_KL: 0.07116\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[148/600] [0/331] Loss_D: 0.56259, Loss_G: 2.97687, Loss_KL: 0.06286\n",
      "[148/600] [100/331] Loss_D: 0.72949, Loss_G: 2.37031, Loss_KL: 0.07715\n",
      "[148/600] [200/331] Loss_D: 0.50199, Loss_G: 3.27564, Loss_KL: 0.07284\n",
      "[148/600] [300/331] Loss_D: 0.45012, Loss_G: 3.18414, Loss_KL: 0.11004\n",
      "[148/600] Loss_D: 0.61376, Loss_G: 2.92297, Loss_KL: 0.07495\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[149/600] [0/331] Loss_D: 0.29982, Loss_G: 2.91601, Loss_KL: 0.09373\n",
      "[149/600] [100/331] Loss_D: 0.58218, Loss_G: 2.39572, Loss_KL: 0.10187\n",
      "[149/600] [200/331] Loss_D: 0.71185, Loss_G: 2.60910, Loss_KL: 0.08038\n",
      "[149/600] [300/331] Loss_D: 0.68711, Loss_G: 2.67922, Loss_KL: 0.09326\n",
      "[149/600] Loss_D: 0.60584, Loss_G: 2.89248, Loss_KL: 0.07585\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[150/600] [0/331] Loss_D: 0.20615, Loss_G: 2.46380, Loss_KL: 0.07219\n",
      "[150/600] [100/331] Loss_D: 0.57243, Loss_G: 3.05498, Loss_KL: 0.09366\n",
      "[150/600] [200/331] Loss_D: 0.68779, Loss_G: 1.89873, Loss_KL: 0.09095\n",
      "[150/600] [300/331] Loss_D: 0.27979, Loss_G: 2.52658, Loss_KL: 0.10283\n",
      "[150/600] Loss_D: 0.59905, Loss_G: 2.93298, Loss_KL: 0.07788\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[151/600] [0/331] Loss_D: 0.55555, Loss_G: 2.48794, Loss_KL: 0.07056\n",
      "[151/600] [100/331] Loss_D: 0.45560, Loss_G: 2.52033, Loss_KL: 0.09412\n",
      "[151/600] [200/331] Loss_D: 0.99168, Loss_G: 3.19879, Loss_KL: 0.07876\n",
      "[151/600] [300/331] Loss_D: 0.59734, Loss_G: 2.07622, Loss_KL: 0.06711\n",
      "[151/600] Loss_D: 0.62634, Loss_G: 2.86807, Loss_KL: 0.07588\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[152/600] [0/331] Loss_D: 0.74673, Loss_G: 2.45676, Loss_KL: 0.04988\n",
      "[152/600] [100/331] Loss_D: 0.80252, Loss_G: 2.45908, Loss_KL: 0.07500\n",
      "[152/600] [200/331] Loss_D: 0.53298, Loss_G: 2.80398, Loss_KL: 0.06320\n",
      "[152/600] [300/331] Loss_D: 0.85296, Loss_G: 3.58781, Loss_KL: 0.05782\n",
      "[152/600] Loss_D: 0.58946, Loss_G: 2.94589, Loss_KL: 0.07379\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[153/600] [0/331] Loss_D: 0.50908, Loss_G: 3.80286, Loss_KL: 0.05828\n",
      "[153/600] [100/331] Loss_D: 0.60956, Loss_G: 3.43663, Loss_KL: 0.06919\n",
      "[153/600] [200/331] Loss_D: 0.61119, Loss_G: 3.25380, Loss_KL: 0.07436\n",
      "[153/600] [300/331] Loss_D: 0.44559, Loss_G: 2.75662, Loss_KL: 0.08272\n",
      "[153/600] Loss_D: 0.60705, Loss_G: 2.95750, Loss_KL: 0.07165\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[154/600] [0/331] Loss_D: 0.74505, Loss_G: 3.09688, Loss_KL: 0.08655\n",
      "[154/600] [100/331] Loss_D: 0.65214, Loss_G: 2.70508, Loss_KL: 0.05713\n",
      "[154/600] [200/331] Loss_D: 0.53202, Loss_G: 3.19120, Loss_KL: 0.06813\n",
      "[154/600] [300/331] Loss_D: 0.56247, Loss_G: 2.81672, Loss_KL: 0.09505\n",
      "[154/600] Loss_D: 0.60161, Loss_G: 2.96909, Loss_KL: 0.07491\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[155/600] [0/331] Loss_D: 0.28034, Loss_G: 4.70433, Loss_KL: 0.07861\n",
      "[155/600] [100/331] Loss_D: 0.47642, Loss_G: 3.62421, Loss_KL: 0.07854\n",
      "[155/600] [200/331] Loss_D: 0.74457, Loss_G: 2.06124, Loss_KL: 0.07539\n",
      "[155/600] [300/331] Loss_D: 0.78787, Loss_G: 2.62799, Loss_KL: 0.09273\n",
      "[155/600] Loss_D: 0.62970, Loss_G: 2.92955, Loss_KL: 0.07498\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[156/600] [0/331] Loss_D: 0.89586, Loss_G: 3.06040, Loss_KL: 0.09036\n",
      "[156/600] [100/331] Loss_D: 0.72018, Loss_G: 2.71282, Loss_KL: 0.04999\n",
      "[156/600] [200/331] Loss_D: 0.26977, Loss_G: 2.47628, Loss_KL: 0.06619\n",
      "[156/600] [300/331] Loss_D: 0.70442, Loss_G: 2.85115, Loss_KL: 0.09425\n",
      "[156/600] Loss_D: 0.58761, Loss_G: 2.92431, Loss_KL: 0.07469\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[157/600] [0/331] Loss_D: 0.66638, Loss_G: 3.07585, Loss_KL: 0.06226\n",
      "[157/600] [100/331] Loss_D: 0.68738, Loss_G: 2.83220, Loss_KL: 0.06951\n",
      "[157/600] [200/331] Loss_D: 0.87973, Loss_G: 1.48732, Loss_KL: 0.08437\n",
      "[157/600] [300/331] Loss_D: 0.67249, Loss_G: 3.36211, Loss_KL: 0.09414\n",
      "[157/600] Loss_D: 0.62194, Loss_G: 2.89627, Loss_KL: 0.07733\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[158/600] [0/331] Loss_D: 0.77794, Loss_G: 2.32802, Loss_KL: 0.07246\n",
      "[158/600] [100/331] Loss_D: 0.72316, Loss_G: 2.47214, Loss_KL: 0.07952\n",
      "[158/600] [200/331] Loss_D: 0.68318, Loss_G: 2.93738, Loss_KL: 0.05959\n",
      "[158/600] [300/331] Loss_D: 0.64833, Loss_G: 2.81422, Loss_KL: 0.07365\n",
      "[158/600] Loss_D: 0.59289, Loss_G: 2.86149, Loss_KL: 0.07385\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[159/600] [0/331] Loss_D: 0.62751, Loss_G: 3.50114, Loss_KL: 0.07194\n",
      "[159/600] [100/331] Loss_D: 0.73498, Loss_G: 3.91042, Loss_KL: 0.07000\n",
      "[159/600] [200/331] Loss_D: 0.72599, Loss_G: 2.54820, Loss_KL: 0.06639\n",
      "[159/600] [300/331] Loss_D: 0.57229, Loss_G: 3.52656, Loss_KL: 0.08827\n",
      "[159/600] Loss_D: 0.62662, Loss_G: 2.87218, Loss_KL: 0.07507\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[160/600] [0/331] Loss_D: 0.34940, Loss_G: 2.91677, Loss_KL: 0.08405\n",
      "[160/600] [100/331] Loss_D: 0.71796, Loss_G: 2.70875, Loss_KL: 0.07594\n",
      "[160/600] [200/331] Loss_D: 0.73063, Loss_G: 2.28327, Loss_KL: 0.07761\n",
      "[160/600] [300/331] Loss_D: 0.68455, Loss_G: 3.31492, Loss_KL: 0.07819\n",
      "[160/600] Loss_D: 0.60219, Loss_G: 2.87134, Loss_KL: 0.07419\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[161/600] [0/331] Loss_D: 0.79320, Loss_G: 2.66237, Loss_KL: 0.07015\n",
      "[161/600] [100/331] Loss_D: 0.51746, Loss_G: 2.84697, Loss_KL: 0.09404\n",
      "[161/600] [200/331] Loss_D: 0.31397, Loss_G: 3.56147, Loss_KL: 0.06227\n",
      "[161/600] [300/331] Loss_D: 0.76370, Loss_G: 2.68377, Loss_KL: 0.08437\n",
      "[161/600] Loss_D: 0.60416, Loss_G: 2.84815, Loss_KL: 0.07311\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[162/600] [0/331] Loss_D: 0.73977, Loss_G: 3.32902, Loss_KL: 0.06638\n",
      "[162/600] [100/331] Loss_D: 0.50203, Loss_G: 2.75671, Loss_KL: 0.08322\n",
      "[162/600] [200/331] Loss_D: 0.79483, Loss_G: 3.16474, Loss_KL: 0.06127\n",
      "[162/600] [300/331] Loss_D: 0.66395, Loss_G: 3.14553, Loss_KL: 0.10694\n",
      "[162/600] Loss_D: 0.60593, Loss_G: 3.00078, Loss_KL: 0.07128\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[163/600] [0/331] Loss_D: 0.72326, Loss_G: 2.74017, Loss_KL: 0.07446\n",
      "[163/600] [100/331] Loss_D: 0.46732, Loss_G: 3.06148, Loss_KL: 0.06751\n",
      "[163/600] [200/331] Loss_D: 0.37091, Loss_G: 4.36836, Loss_KL: 0.07940\n",
      "[163/600] [300/331] Loss_D: 0.55940, Loss_G: 3.94815, Loss_KL: 0.07601\n",
      "[163/600] Loss_D: 0.60925, Loss_G: 2.90342, Loss_KL: 0.07382\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[164/600] [0/331] Loss_D: 0.68848, Loss_G: 2.80910, Loss_KL: 0.08130\n",
      "[164/600] [100/331] Loss_D: 0.72483, Loss_G: 3.78856, Loss_KL: 0.06540\n",
      "[164/600] [200/331] Loss_D: 0.79083, Loss_G: 3.53025, Loss_KL: 0.09584\n",
      "[164/600] [300/331] Loss_D: 0.35479, Loss_G: 3.49612, Loss_KL: 0.06592\n",
      "[164/600] Loss_D: 0.59658, Loss_G: 2.94297, Loss_KL: 0.07558\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[165/600] [0/331] Loss_D: 0.29497, Loss_G: 3.28722, Loss_KL: 0.06341\n",
      "[165/600] [100/331] Loss_D: 0.31291, Loss_G: 2.79795, Loss_KL: 0.09091\n",
      "[165/600] [200/331] Loss_D: 0.68085, Loss_G: 2.19906, Loss_KL: 0.05244\n",
      "[165/600] [300/331] Loss_D: 0.58095, Loss_G: 3.33499, Loss_KL: 0.08961\n",
      "[165/600] Loss_D: 0.59636, Loss_G: 2.93204, Loss_KL: 0.07539\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[166/600] [0/331] Loss_D: 0.80667, Loss_G: 2.73160, Loss_KL: 0.07752\n",
      "[166/600] [100/331] Loss_D: 0.76355, Loss_G: 2.36074, Loss_KL: 0.04398\n",
      "[166/600] [200/331] Loss_D: 0.51845, Loss_G: 2.59599, Loss_KL: 0.07132\n",
      "[166/600] [300/331] Loss_D: 0.43942, Loss_G: 3.63997, Loss_KL: 0.06929\n",
      "[166/600] Loss_D: 0.60955, Loss_G: 2.84514, Loss_KL: 0.07166\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[167/600] [0/331] Loss_D: 0.53363, Loss_G: 3.51167, Loss_KL: 0.07066\n",
      "[167/600] [100/331] Loss_D: 0.73586, Loss_G: 2.61606, Loss_KL: 0.05291\n",
      "[167/600] [200/331] Loss_D: 0.64521, Loss_G: 2.61172, Loss_KL: 0.06649\n",
      "[167/600] [300/331] Loss_D: 0.42784, Loss_G: 4.17292, Loss_KL: 0.06766\n",
      "[167/600] Loss_D: 0.59090, Loss_G: 2.84693, Loss_KL: 0.07297\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[168/600] [0/331] Loss_D: 0.68334, Loss_G: 3.32805, Loss_KL: 0.09275\n",
      "[168/600] [100/331] Loss_D: 0.75547, Loss_G: 2.89016, Loss_KL: 0.05220\n",
      "[168/600] [200/331] Loss_D: 0.79536, Loss_G: 2.72836, Loss_KL: 0.09978\n",
      "[168/600] [300/331] Loss_D: 0.70522, Loss_G: 2.75315, Loss_KL: 0.05976\n",
      "[168/600] Loss_D: 0.60465, Loss_G: 2.93134, Loss_KL: 0.07393\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[169/600] [0/331] Loss_D: 0.76703, Loss_G: 2.58293, Loss_KL: 0.06947\n",
      "[169/600] [100/331] Loss_D: 0.95328, Loss_G: 1.21610, Loss_KL: 0.06328\n",
      "[169/600] [200/331] Loss_D: 0.29432, Loss_G: 3.31372, Loss_KL: 0.09409\n",
      "[169/600] [300/331] Loss_D: 0.36392, Loss_G: 2.86293, Loss_KL: 0.05632\n",
      "[169/600] Loss_D: 0.58777, Loss_G: 2.93571, Loss_KL: 0.07532\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[170/600] [0/331] Loss_D: 0.68423, Loss_G: 1.88021, Loss_KL: 0.07665\n",
      "[170/600] [100/331] Loss_D: 0.25803, Loss_G: 3.43572, Loss_KL: 0.05561\n",
      "[170/600] [200/331] Loss_D: 0.59246, Loss_G: 2.97081, Loss_KL: 0.05928\n",
      "[170/600] [300/331] Loss_D: 0.77092, Loss_G: 4.24607, Loss_KL: 0.06817\n",
      "[170/600] Loss_D: 0.59708, Loss_G: 2.89612, Loss_KL: 0.07634\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[171/600] [0/331] Loss_D: 0.92516, Loss_G: 2.89558, Loss_KL: 0.04496\n",
      "[171/600] [100/331] Loss_D: 1.08207, Loss_G: 4.15621, Loss_KL: 0.07868\n",
      "[171/600] [200/331] Loss_D: 0.81049, Loss_G: 2.86282, Loss_KL: 0.07036\n",
      "[171/600] [300/331] Loss_D: 0.37637, Loss_G: 2.75244, Loss_KL: 0.11734\n",
      "[171/600] Loss_D: 0.60526, Loss_G: 2.92294, Loss_KL: 0.07594\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[172/600] [0/331] Loss_D: 0.27052, Loss_G: 4.16608, Loss_KL: 0.06586\n",
      "[172/600] [100/331] Loss_D: 0.71890, Loss_G: 3.01382, Loss_KL: 0.05564\n",
      "[172/600] [200/331] Loss_D: 0.68301, Loss_G: 2.46476, Loss_KL: 0.06449\n",
      "[172/600] [300/331] Loss_D: 0.56180, Loss_G: 3.18816, Loss_KL: 0.09330\n",
      "[172/600] Loss_D: 0.59159, Loss_G: 2.84889, Loss_KL: 0.07613\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[173/600] [0/331] Loss_D: 0.73571, Loss_G: 2.51074, Loss_KL: 0.12060\n",
      "[173/600] [100/331] Loss_D: 0.64274, Loss_G: 2.24309, Loss_KL: 0.07156\n",
      "[173/600] [200/331] Loss_D: 0.60785, Loss_G: 2.23134, Loss_KL: 0.06969\n",
      "[173/600] [300/331] Loss_D: 0.77705, Loss_G: 2.51397, Loss_KL: 0.09138\n",
      "[173/600] Loss_D: 0.61466, Loss_G: 2.91728, Loss_KL: 0.07722\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[174/600] [0/331] Loss_D: 0.70250, Loss_G: 2.71229, Loss_KL: 0.08075\n",
      "[174/600] [100/331] Loss_D: 0.77880, Loss_G: 3.10570, Loss_KL: 0.06887\n",
      "[174/600] [200/331] Loss_D: 0.36642, Loss_G: 3.50914, Loss_KL: 0.08255\n",
      "[174/600] [300/331] Loss_D: 0.69966, Loss_G: 2.28200, Loss_KL: 0.07242\n",
      "[174/600] Loss_D: 0.59656, Loss_G: 2.87314, Loss_KL: 0.07654\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[175/600] [0/331] Loss_D: 0.42356, Loss_G: 3.09514, Loss_KL: 0.07386\n",
      "[175/600] [100/331] Loss_D: 0.69824, Loss_G: 2.20310, Loss_KL: 0.08428\n",
      "[175/600] [200/331] Loss_D: 0.76340, Loss_G: 2.68315, Loss_KL: 0.06825\n",
      "[175/600] [300/331] Loss_D: 0.39629, Loss_G: 2.85486, Loss_KL: 0.06085\n",
      "[175/600] Loss_D: 0.58919, Loss_G: 2.82971, Loss_KL: 0.07363\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[176/600] [0/331] Loss_D: 0.48176, Loss_G: 2.72301, Loss_KL: 0.08086\n",
      "[176/600] [100/331] Loss_D: 0.69173, Loss_G: 3.12698, Loss_KL: 0.05704\n",
      "[176/600] [200/331] Loss_D: 0.76931, Loss_G: 2.86662, Loss_KL: 0.06454\n",
      "[176/600] [300/331] Loss_D: 0.71024, Loss_G: 2.81132, Loss_KL: 0.07351\n",
      "[176/600] Loss_D: 0.58033, Loss_G: 2.97472, Loss_KL: 0.07162\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[177/600] [0/331] Loss_D: 0.59165, Loss_G: 2.52043, Loss_KL: 0.09766\n",
      "[177/600] [100/331] Loss_D: 0.68911, Loss_G: 2.59996, Loss_KL: 0.08833\n",
      "[177/600] [200/331] Loss_D: 0.68055, Loss_G: 3.01098, Loss_KL: 0.06526\n",
      "[177/600] [300/331] Loss_D: 0.77957, Loss_G: 1.81622, Loss_KL: 0.07958\n",
      "[177/600] Loss_D: 0.59078, Loss_G: 2.87878, Loss_KL: 0.07222\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[178/600] [0/331] Loss_D: 0.41528, Loss_G: 2.36725, Loss_KL: 0.06638\n",
      "[178/600] [100/331] Loss_D: 0.75014, Loss_G: 3.22704, Loss_KL: 0.05877\n",
      "[178/600] [200/331] Loss_D: 0.79751, Loss_G: 2.71832, Loss_KL: 0.07901\n",
      "[178/600] [300/331] Loss_D: 0.33635, Loss_G: 2.48562, Loss_KL: 0.06195\n",
      "[178/600] Loss_D: 0.60579, Loss_G: 2.88643, Loss_KL: 0.07083\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[179/600] [0/331] Loss_D: 0.72455, Loss_G: 2.52545, Loss_KL: 0.07216\n",
      "[179/600] [100/331] Loss_D: 0.35586, Loss_G: 3.64286, Loss_KL: 0.06776\n",
      "[179/600] [200/331] Loss_D: 0.49873, Loss_G: 2.35869, Loss_KL: 0.04225\n",
      "[179/600] [300/331] Loss_D: 0.65247, Loss_G: 2.43185, Loss_KL: 0.06006\n",
      "[179/600] Loss_D: 0.58678, Loss_G: 2.84501, Loss_KL: 0.07038\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[180/600] [0/331] Loss_D: 0.72024, Loss_G: 2.83493, Loss_KL: 0.06283\n",
      "[180/600] [100/331] Loss_D: 0.39667, Loss_G: 3.04732, Loss_KL: 0.06747\n",
      "[180/600] [200/331] Loss_D: 0.34722, Loss_G: 3.66167, Loss_KL: 0.08066\n",
      "[180/600] [300/331] Loss_D: 0.72789, Loss_G: 2.79982, Loss_KL: 0.07197\n",
      "[180/600] Loss_D: 0.60326, Loss_G: 2.90090, Loss_KL: 0.06938\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Save G1/D1 models\n",
      "[181/600] [0/331] Loss_D: 0.38748, Loss_G: 3.32621, Loss_KL: 0.07006\n",
      "[181/600] [100/331] Loss_D: 0.68804, Loss_G: 3.33701, Loss_KL: 0.06529\n",
      "[181/600] [200/331] Loss_D: 0.37599, Loss_G: 2.89384, Loss_KL: 0.07404\n",
      "[181/600] [300/331] Loss_D: 0.74951, Loss_G: 2.62864, Loss_KL: 0.05965\n",
      "[181/600] Loss_D: 0.60276, Loss_G: 2.89486, Loss_KL: 0.06819\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[182/600] [0/331] Loss_D: 0.80287, Loss_G: 3.08200, Loss_KL: 0.05965\n",
      "[182/600] [100/331] Loss_D: 0.84780, Loss_G: 1.73324, Loss_KL: 0.04200\n",
      "[182/600] [200/331] Loss_D: 0.39198, Loss_G: 2.80189, Loss_KL: 0.06874\n",
      "[182/600] [300/331] Loss_D: 0.42061, Loss_G: 2.82050, Loss_KL: 0.12647\n",
      "[182/600] Loss_D: 0.58827, Loss_G: 2.75049, Loss_KL: 0.07130\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[183/600] [0/331] Loss_D: 0.37939, Loss_G: 3.39543, Loss_KL: 0.06813\n",
      "[183/600] [100/331] Loss_D: 0.38449, Loss_G: 3.04299, Loss_KL: 0.07374\n",
      "[183/600] [200/331] Loss_D: 0.48873, Loss_G: 3.27761, Loss_KL: 0.07933\n",
      "[183/600] [300/331] Loss_D: 0.62553, Loss_G: 3.62883, Loss_KL: 0.09960\n",
      "[183/600] Loss_D: 0.61877, Loss_G: 2.91840, Loss_KL: 0.06950\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[184/600] [0/331] Loss_D: 0.76675, Loss_G: 2.75215, Loss_KL: 0.07344\n",
      "[184/600] [100/331] Loss_D: 0.39570, Loss_G: 2.73012, Loss_KL: 0.05632\n",
      "[184/600] [200/331] Loss_D: 0.71620, Loss_G: 3.13236, Loss_KL: 0.07374\n",
      "[184/600] [300/331] Loss_D: 0.62029, Loss_G: 2.51703, Loss_KL: 0.06109\n",
      "[184/600] Loss_D: 0.59376, Loss_G: 2.91598, Loss_KL: 0.06975\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[185/600] [0/331] Loss_D: 0.41639, Loss_G: 3.42153, Loss_KL: 0.07904\n",
      "[185/600] [100/331] Loss_D: 0.91001, Loss_G: 2.52980, Loss_KL: 0.06010\n",
      "[185/600] [200/331] Loss_D: 0.40006, Loss_G: 2.82939, Loss_KL: 0.09440\n",
      "[185/600] [300/331] Loss_D: 0.66985, Loss_G: 2.66279, Loss_KL: 0.06922\n",
      "[185/600] Loss_D: 0.59367, Loss_G: 2.86567, Loss_KL: 0.06898\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[186/600] [0/331] Loss_D: 0.46261, Loss_G: 2.63602, Loss_KL: 0.08729\n",
      "[186/600] [100/331] Loss_D: 0.53956, Loss_G: 3.49813, Loss_KL: 0.05765\n",
      "[186/600] [200/331] Loss_D: 0.65117, Loss_G: 2.31107, Loss_KL: 0.06086\n",
      "[186/600] [300/331] Loss_D: 0.59968, Loss_G: 2.64043, Loss_KL: 0.04136\n",
      "[186/600] Loss_D: 0.58481, Loss_G: 2.86890, Loss_KL: 0.06805\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[187/600] [0/331] Loss_D: 0.74785, Loss_G: 3.46472, Loss_KL: 0.05814\n",
      "[187/600] [100/331] Loss_D: 0.47002, Loss_G: 3.28315, Loss_KL: 0.06429\n",
      "[187/600] [200/331] Loss_D: 0.67865, Loss_G: 2.50839, Loss_KL: 0.04035\n",
      "[187/600] [300/331] Loss_D: 0.65042, Loss_G: 2.57602, Loss_KL: 0.05616\n",
      "[187/600] Loss_D: 0.61266, Loss_G: 2.86268, Loss_KL: 0.06875\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[188/600] [0/331] Loss_D: 0.78959, Loss_G: 2.47726, Loss_KL: 0.06243\n",
      "[188/600] [100/331] Loss_D: 0.41174, Loss_G: 3.07995, Loss_KL: 0.05395\n",
      "[188/600] [200/331] Loss_D: 0.55143, Loss_G: 3.85612, Loss_KL: 0.05972\n",
      "[188/600] [300/331] Loss_D: 0.74849, Loss_G: 2.31906, Loss_KL: 0.07851\n",
      "[188/600] Loss_D: 0.60001, Loss_G: 2.83547, Loss_KL: 0.06962\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[189/600] [0/331] Loss_D: 0.78133, Loss_G: 2.65288, Loss_KL: 0.06593\n",
      "[189/600] [100/331] Loss_D: 0.61582, Loss_G: 2.73283, Loss_KL: 0.06061\n",
      "[189/600] [200/331] Loss_D: 0.66872, Loss_G: 2.30974, Loss_KL: 0.06658\n",
      "[189/600] [300/331] Loss_D: 0.75855, Loss_G: 2.25911, Loss_KL: 0.07886\n",
      "[189/600] Loss_D: 0.59978, Loss_G: 2.85390, Loss_KL: 0.06753\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[190/600] [0/331] Loss_D: 0.53619, Loss_G: 2.14683, Loss_KL: 0.04579\n",
      "[190/600] [100/331] Loss_D: 0.34639, Loss_G: 3.47764, Loss_KL: 0.07573\n",
      "[190/600] [200/331] Loss_D: 0.54483, Loss_G: 3.60346, Loss_KL: 0.04019\n",
      "[190/600] [300/331] Loss_D: 0.38205, Loss_G: 4.11672, Loss_KL: 0.05862\n",
      "[190/600] Loss_D: 0.60053, Loss_G: 2.87551, Loss_KL: 0.06709\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[191/600] [0/331] Loss_D: 0.40981, Loss_G: 3.10845, Loss_KL: 0.05844\n",
      "[191/600] [100/331] Loss_D: 0.71063, Loss_G: 1.87074, Loss_KL: 0.05838\n",
      "[191/600] [200/331] Loss_D: 0.40551, Loss_G: 2.77742, Loss_KL: 0.07231\n",
      "[191/600] [300/331] Loss_D: 0.69515, Loss_G: 2.58067, Loss_KL: 0.07029\n",
      "[191/600] Loss_D: 0.60938, Loss_G: 2.75937, Loss_KL: 0.06418\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[192/600] [0/331] Loss_D: 0.51639, Loss_G: 3.14739, Loss_KL: 0.03439\n",
      "[192/600] [100/331] Loss_D: 0.79176, Loss_G: 2.68190, Loss_KL: 0.04969\n",
      "[192/600] [200/331] Loss_D: 0.86366, Loss_G: 2.67260, Loss_KL: 0.05702\n",
      "[192/600] [300/331] Loss_D: 0.41526, Loss_G: 3.17407, Loss_KL: 0.05075\n",
      "[192/600] Loss_D: 0.60917, Loss_G: 2.78201, Loss_KL: 0.06116\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[193/600] [0/331] Loss_D: 0.73032, Loss_G: 2.38262, Loss_KL: 0.06002\n",
      "[193/600] [100/331] Loss_D: 0.64640, Loss_G: 3.15876, Loss_KL: 0.07254\n",
      "[193/600] [200/331] Loss_D: 0.69554, Loss_G: 2.54149, Loss_KL: 0.05869\n",
      "[193/600] [300/331] Loss_D: 0.33752, Loss_G: 2.56981, Loss_KL: 0.06402\n",
      "[193/600] Loss_D: 0.57329, Loss_G: 2.87434, Loss_KL: 0.06036\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[194/600] [0/331] Loss_D: 0.30386, Loss_G: 2.61030, Loss_KL: 0.05263\n",
      "[194/600] [100/331] Loss_D: 0.74072, Loss_G: 2.72182, Loss_KL: 0.04966\n",
      "[194/600] [200/331] Loss_D: 0.56944, Loss_G: 2.28783, Loss_KL: 0.06705\n",
      "[194/600] [300/331] Loss_D: 0.77344, Loss_G: 3.75088, Loss_KL: 0.04188\n",
      "[194/600] Loss_D: 0.58428, Loss_G: 2.76790, Loss_KL: 0.05926\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[195/600] [0/331] Loss_D: 0.60045, Loss_G: 2.80785, Loss_KL: 0.05338\n",
      "[195/600] [100/331] Loss_D: 0.80703, Loss_G: 3.31094, Loss_KL: 0.05622\n",
      "[195/600] [200/331] Loss_D: 0.71726, Loss_G: 1.84421, Loss_KL: 0.09118\n",
      "[195/600] [300/331] Loss_D: 0.46452, Loss_G: 1.88473, Loss_KL: 0.05456\n",
      "[195/600] Loss_D: 0.60576, Loss_G: 2.88507, Loss_KL: 0.05817\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[196/600] [0/331] Loss_D: 0.23249, Loss_G: 4.55795, Loss_KL: 0.07204\n",
      "[196/600] [100/331] Loss_D: 0.67136, Loss_G: 2.37445, Loss_KL: 0.07808\n",
      "[196/600] [200/331] Loss_D: 0.57212, Loss_G: 2.84979, Loss_KL: 0.09118\n",
      "[196/600] [300/331] Loss_D: 1.00995, Loss_G: 2.34541, Loss_KL: 0.05487\n",
      "[196/600] Loss_D: 0.58746, Loss_G: 2.82485, Loss_KL: 0.06193\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[197/600] [0/331] Loss_D: 0.27757, Loss_G: 4.10770, Loss_KL: 0.03251\n",
      "[197/600] [100/331] Loss_D: 0.38883, Loss_G: 2.49039, Loss_KL: 0.04138\n",
      "[197/600] [200/331] Loss_D: 0.71864, Loss_G: 2.65751, Loss_KL: 0.05105\n",
      "[197/600] [300/331] Loss_D: 0.61276, Loss_G: 1.99068, Loss_KL: 0.07102\n",
      "[197/600] Loss_D: 0.59802, Loss_G: 2.76382, Loss_KL: 0.06364\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[198/600] [0/331] Loss_D: 0.46507, Loss_G: 1.89349, Loss_KL: 0.03341\n",
      "[198/600] [100/331] Loss_D: 0.62378, Loss_G: 2.55868, Loss_KL: 0.05512\n",
      "[198/600] [200/331] Loss_D: 0.65555, Loss_G: 2.59434, Loss_KL: 0.05514\n",
      "[198/600] [300/331] Loss_D: 0.71657, Loss_G: 2.32360, Loss_KL: 0.06025\n",
      "[198/600] Loss_D: 0.58719, Loss_G: 2.75557, Loss_KL: 0.06037\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[199/600] [0/331] Loss_D: 0.52600, Loss_G: 2.90307, Loss_KL: 0.07807\n",
      "[199/600] [100/331] Loss_D: 0.76025, Loss_G: 3.58390, Loss_KL: 0.06975\n",
      "[199/600] [200/331] Loss_D: 0.28747, Loss_G: 2.64310, Loss_KL: 0.04458\n",
      "[199/600] [300/331] Loss_D: 0.42576, Loss_G: 2.62219, Loss_KL: 0.08544\n",
      "[199/600] Loss_D: 0.59198, Loss_G: 2.79398, Loss_KL: 0.06271\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "[200/600] [0/331] Loss_D: 0.46483, Loss_G: 3.38827, Loss_KL: 0.07053\n",
      "[200/600] [100/331] Loss_D: 0.72775, Loss_G: 2.29608, Loss_KL: 0.05429\n",
      "[200/600] [200/331] Loss_D: 0.66600, Loss_G: 2.58733, Loss_KL: 0.06820\n",
      "[200/600] [300/331] Loss_D: 0.67327, Loss_G: 3.18619, Loss_KL: 0.07214\n",
      "[200/600] Loss_D: 0.59824, Loss_G: 2.86050, Loss_KL: 0.06298\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[201/600] [0/331] Loss_D: 0.31881, Loss_G: 2.74708, Loss_KL: 0.05794\n",
      "[201/600] [100/331] Loss_D: 0.24887, Loss_G: 2.84731, Loss_KL: 0.05966\n",
      "[201/600] [200/331] Loss_D: 0.72067, Loss_G: 2.22876, Loss_KL: 0.06567\n",
      "[201/600] [300/331] Loss_D: 0.81653, Loss_G: 2.53428, Loss_KL: 0.04061\n",
      "[201/600] Loss_D: 0.58230, Loss_G: 2.73547, Loss_KL: 0.06269\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[202/600] [0/331] Loss_D: 0.39976, Loss_G: 3.40623, Loss_KL: 0.04836\n",
      "[202/600] [100/331] Loss_D: 0.80105, Loss_G: 2.72786, Loss_KL: 0.05475\n",
      "[202/600] [200/331] Loss_D: 0.64646, Loss_G: 2.76322, Loss_KL: 0.06100\n",
      "[202/600] [300/331] Loss_D: 0.24782, Loss_G: 2.74794, Loss_KL: 0.09819\n",
      "[202/600] Loss_D: 0.59343, Loss_G: 2.75376, Loss_KL: 0.06289\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[203/600] [0/331] Loss_D: 0.58073, Loss_G: 1.35240, Loss_KL: 0.06264\n",
      "[203/600] [100/331] Loss_D: 0.24688, Loss_G: 3.16220, Loss_KL: 0.05029\n",
      "[203/600] [200/331] Loss_D: 0.63673, Loss_G: 2.51569, Loss_KL: 0.04900\n",
      "[203/600] [300/331] Loss_D: 0.71445, Loss_G: 3.72869, Loss_KL: 0.05097\n",
      "[203/600] Loss_D: 0.58676, Loss_G: 2.71915, Loss_KL: 0.06365\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[204/600] [0/331] Loss_D: 0.71532, Loss_G: 2.38749, Loss_KL: 0.05783\n",
      "[204/600] [100/331] Loss_D: 0.71144, Loss_G: 3.35987, Loss_KL: 0.05582\n",
      "[204/600] [200/331] Loss_D: 0.57121, Loss_G: 2.37108, Loss_KL: 0.07973\n",
      "[204/600] [300/331] Loss_D: 0.49603, Loss_G: 2.50109, Loss_KL: 0.06084\n",
      "[204/600] Loss_D: 0.59703, Loss_G: 2.63591, Loss_KL: 0.06395\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[205/600] [0/331] Loss_D: 0.66118, Loss_G: 3.05379, Loss_KL: 0.07124\n",
      "[205/600] [100/331] Loss_D: 0.60293, Loss_G: 3.71716, Loss_KL: 0.09740\n",
      "[205/600] [200/331] Loss_D: 0.71817, Loss_G: 2.59901, Loss_KL: 0.06227\n",
      "[205/600] [300/331] Loss_D: 0.67768, Loss_G: 3.26064, Loss_KL: 0.06300\n",
      "[205/600] Loss_D: 0.56896, Loss_G: 2.78528, Loss_KL: 0.06466\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[206/600] [0/331] Loss_D: 0.64373, Loss_G: 3.45613, Loss_KL: 0.09257\n",
      "[206/600] [100/331] Loss_D: 0.38394, Loss_G: 2.42903, Loss_KL: 0.07625\n",
      "[206/600] [200/331] Loss_D: 0.47521, Loss_G: 2.58499, Loss_KL: 0.06787\n",
      "[206/600] [300/331] Loss_D: 0.48884, Loss_G: 2.67499, Loss_KL: 0.06305\n",
      "[206/600] Loss_D: 0.57449, Loss_G: 2.71509, Loss_KL: 0.06635\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[207/600] [0/331] Loss_D: 0.78396, Loss_G: 2.09340, Loss_KL: 0.05303\n",
      "[207/600] [100/331] Loss_D: 0.58379, Loss_G: 3.68356, Loss_KL: 0.07170\n",
      "[207/600] [200/331] Loss_D: 0.77080, Loss_G: 3.13961, Loss_KL: 0.03045\n",
      "[207/600] [300/331] Loss_D: 0.56171, Loss_G: 2.81022, Loss_KL: 0.06451\n",
      "[207/600] Loss_D: 0.60085, Loss_G: 2.73394, Loss_KL: 0.06531\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[208/600] [0/331] Loss_D: 0.72311, Loss_G: 2.31121, Loss_KL: 0.06707\n",
      "[208/600] [100/331] Loss_D: 0.36151, Loss_G: 2.94793, Loss_KL: 0.04791\n",
      "[208/600] [200/331] Loss_D: 0.51550, Loss_G: 2.50471, Loss_KL: 0.07755\n",
      "[208/600] [300/331] Loss_D: 0.45882, Loss_G: 3.01503, Loss_KL: 0.08060\n",
      "[208/600] Loss_D: 0.57048, Loss_G: 2.75295, Loss_KL: 0.06506\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[209/600] [0/331] Loss_D: 0.50158, Loss_G: 3.21053, Loss_KL: 0.09644\n",
      "[209/600] [100/331] Loss_D: 0.43144, Loss_G: 4.12669, Loss_KL: 0.07736\n",
      "[209/600] [200/331] Loss_D: 0.38908, Loss_G: 3.05663, Loss_KL: 0.06467\n",
      "[209/600] [300/331] Loss_D: 0.78635, Loss_G: 3.21946, Loss_KL: 0.05463\n",
      "[209/600] Loss_D: 0.56420, Loss_G: 2.85784, Loss_KL: 0.06628\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[210/600] [0/331] Loss_D: 0.75563, Loss_G: 2.74932, Loss_KL: 0.06055\n",
      "[210/600] [100/331] Loss_D: 0.71511, Loss_G: 2.37040, Loss_KL: 0.06206\n",
      "[210/600] [200/331] Loss_D: 0.77930, Loss_G: 2.46309, Loss_KL: 0.05543\n",
      "[210/600] [300/331] Loss_D: 0.64857, Loss_G: 3.14078, Loss_KL: 0.07421\n",
      "[210/600] Loss_D: 0.57985, Loss_G: 2.71734, Loss_KL: 0.06817\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[211/600] [0/331] Loss_D: 0.67292, Loss_G: 2.36864, Loss_KL: 0.08811\n",
      "[211/600] [100/331] Loss_D: 0.72452, Loss_G: 3.52015, Loss_KL: 0.05632\n",
      "[211/600] [200/331] Loss_D: 0.52787, Loss_G: 2.47591, Loss_KL: 0.05099\n",
      "[211/600] [300/331] Loss_D: 0.46228, Loss_G: 3.36706, Loss_KL: 0.06637\n",
      "[211/600] Loss_D: 0.59427, Loss_G: 2.70537, Loss_KL: 0.06784\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[212/600] [0/331] Loss_D: 0.34702, Loss_G: 3.25936, Loss_KL: 0.06432\n",
      "[212/600] [100/331] Loss_D: 0.78248, Loss_G: 2.92698, Loss_KL: 0.07632\n",
      "[212/600] [200/331] Loss_D: 0.62640, Loss_G: 2.99558, Loss_KL: 0.07335\n",
      "[212/600] [300/331] Loss_D: 0.34384, Loss_G: 2.82274, Loss_KL: 0.03989\n",
      "[212/600] Loss_D: 0.56814, Loss_G: 2.82525, Loss_KL: 0.06928\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[213/600] [0/331] Loss_D: 0.58795, Loss_G: 2.62027, Loss_KL: 0.06050\n",
      "[213/600] [100/331] Loss_D: 0.23018, Loss_G: 3.45058, Loss_KL: 0.07203\n",
      "[213/600] [200/331] Loss_D: 0.76631, Loss_G: 2.77879, Loss_KL: 0.07503\n",
      "[213/600] [300/331] Loss_D: 0.63873, Loss_G: 2.51052, Loss_KL: 0.06033\n",
      "[213/600] Loss_D: 0.58797, Loss_G: 2.73599, Loss_KL: 0.06780\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[214/600] [0/331] Loss_D: 0.67737, Loss_G: 2.93366, Loss_KL: 0.06649\n",
      "[214/600] [100/331] Loss_D: 0.75498, Loss_G: 2.17721, Loss_KL: 0.07664\n",
      "[214/600] [200/331] Loss_D: 0.27240, Loss_G: 2.85327, Loss_KL: 0.06682\n",
      "[214/600] [300/331] Loss_D: 0.40931, Loss_G: 2.44659, Loss_KL: 0.04806\n",
      "[214/600] Loss_D: 0.56486, Loss_G: 2.74128, Loss_KL: 0.06791\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[215/600] [0/331] Loss_D: 0.73121, Loss_G: 2.58294, Loss_KL: 0.05527\n",
      "[215/600] [100/331] Loss_D: 0.79596, Loss_G: 3.17356, Loss_KL: 0.07028\n",
      "[215/600] [200/331] Loss_D: 0.66247, Loss_G: 3.45089, Loss_KL: 0.06399\n",
      "[215/600] [300/331] Loss_D: 0.73862, Loss_G: 2.86294, Loss_KL: 0.09790\n",
      "[215/600] Loss_D: 0.56960, Loss_G: 2.83860, Loss_KL: 0.06882\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[216/600] [0/331] Loss_D: 0.63381, Loss_G: 2.53630, Loss_KL: 0.05423\n",
      "[216/600] [100/331] Loss_D: 0.77971, Loss_G: 2.79007, Loss_KL: 0.08449\n",
      "[216/600] [200/331] Loss_D: 0.68640, Loss_G: 3.04196, Loss_KL: 0.07199\n",
      "[216/600] [300/331] Loss_D: 0.74895, Loss_G: 2.75658, Loss_KL: 0.09344\n",
      "[216/600] Loss_D: 0.56666, Loss_G: 2.68288, Loss_KL: 0.06682\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[217/600] [0/331] Loss_D: 0.72085, Loss_G: 3.73218, Loss_KL: 0.05965\n",
      "[217/600] [100/331] Loss_D: 0.52120, Loss_G: 3.15647, Loss_KL: 0.06489\n",
      "[217/600] [200/331] Loss_D: 0.76995, Loss_G: 2.89129, Loss_KL: 0.04973\n",
      "[217/600] [300/331] Loss_D: 0.81252, Loss_G: 2.26729, Loss_KL: 0.07918\n",
      "[217/600] Loss_D: 0.58951, Loss_G: 2.78499, Loss_KL: 0.06478\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[218/600] [0/331] Loss_D: 0.69434, Loss_G: 2.57115, Loss_KL: 0.08085\n",
      "[218/600] [100/331] Loss_D: 0.41664, Loss_G: 2.60042, Loss_KL: 0.06743\n",
      "[218/600] [200/331] Loss_D: 0.73377, Loss_G: 2.59664, Loss_KL: 0.05465\n",
      "[218/600] [300/331] Loss_D: 0.67817, Loss_G: 2.54875, Loss_KL: 0.05482\n",
      "[218/600] Loss_D: 0.55465, Loss_G: 2.60674, Loss_KL: 0.06362\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[219/600] [0/331] Loss_D: 0.65676, Loss_G: 3.01989, Loss_KL: 0.05169\n",
      "[219/600] [100/331] Loss_D: 0.27869, Loss_G: 2.75191, Loss_KL: 0.06111\n",
      "[219/600] [200/331] Loss_D: 0.70911, Loss_G: 2.54557, Loss_KL: 0.04120\n",
      "[219/600] [300/331] Loss_D: 0.70608, Loss_G: 2.53746, Loss_KL: 0.06336\n",
      "[219/600] Loss_D: 0.54524, Loss_G: 2.69239, Loss_KL: 0.05882\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[220/600] [0/331] Loss_D: 0.39020, Loss_G: 2.65320, Loss_KL: 0.09113\n",
      "[220/600] [100/331] Loss_D: 0.41456, Loss_G: 2.60194, Loss_KL: 0.05445\n",
      "[220/600] [200/331] Loss_D: 0.65468, Loss_G: 2.52831, Loss_KL: 0.06367\n",
      "[220/600] [300/331] Loss_D: 0.28982, Loss_G: 2.54315, Loss_KL: 0.05130\n",
      "[220/600] Loss_D: 0.55264, Loss_G: 2.68672, Loss_KL: 0.05809\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[221/600] [0/331] Loss_D: 0.16946, Loss_G: 2.65420, Loss_KL: 0.05228\n",
      "[221/600] [100/331] Loss_D: 0.54895, Loss_G: 2.11752, Loss_KL: 0.04860\n",
      "[221/600] [200/331] Loss_D: 0.29227, Loss_G: 2.29859, Loss_KL: 0.05963\n",
      "[221/600] [300/331] Loss_D: 0.28879, Loss_G: 2.42672, Loss_KL: 0.06504\n",
      "[221/600] Loss_D: 0.56449, Loss_G: 2.56743, Loss_KL: 0.05539\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[222/600] [0/331] Loss_D: 0.79330, Loss_G: 2.33837, Loss_KL: 0.05159\n",
      "[222/600] [100/331] Loss_D: 0.64121, Loss_G: 2.49291, Loss_KL: 0.06371\n",
      "[222/600] [200/331] Loss_D: 0.64645, Loss_G: 2.64988, Loss_KL: 0.03787\n",
      "[222/600] [300/331] Loss_D: 0.56378, Loss_G: 2.58128, Loss_KL: 0.07157\n",
      "[222/600] Loss_D: 0.54158, Loss_G: 2.64383, Loss_KL: 0.05344\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[223/600] [0/331] Loss_D: 0.47938, Loss_G: 3.07494, Loss_KL: 0.05293\n",
      "[223/600] [100/331] Loss_D: 0.40542, Loss_G: 2.70753, Loss_KL: 0.05391\n",
      "[223/600] [200/331] Loss_D: 0.81399, Loss_G: 3.38993, Loss_KL: 0.04375\n",
      "[223/600] [300/331] Loss_D: 0.38859, Loss_G: 3.14136, Loss_KL: 0.05013\n",
      "[223/600] Loss_D: 0.56148, Loss_G: 2.84215, Loss_KL: 0.05094\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[224/600] [0/331] Loss_D: 0.38740, Loss_G: 2.80252, Loss_KL: 0.05486\n",
      "[224/600] [100/331] Loss_D: 0.45790, Loss_G: 3.19141, Loss_KL: 0.07499\n",
      "[224/600] [200/331] Loss_D: 0.29712, Loss_G: 2.79395, Loss_KL: 0.03287\n",
      "[224/600] [300/331] Loss_D: 0.79420, Loss_G: 3.19045, Loss_KL: 0.04883\n",
      "[224/600] Loss_D: 0.57319, Loss_G: 2.85659, Loss_KL: 0.05410\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[225/600] [0/331] Loss_D: 0.22057, Loss_G: 2.74701, Loss_KL: 0.03846\n",
      "[225/600] [100/331] Loss_D: 0.71785, Loss_G: 2.33314, Loss_KL: 0.04089\n",
      "[225/600] [200/331] Loss_D: 0.32189, Loss_G: 3.60914, Loss_KL: 0.06714\n",
      "[225/600] [300/331] Loss_D: 0.64792, Loss_G: 2.23881, Loss_KL: 0.05016\n",
      "[225/600] Loss_D: 0.56738, Loss_G: 2.76258, Loss_KL: 0.05517\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[226/600] [0/331] Loss_D: 0.75882, Loss_G: 2.83315, Loss_KL: 0.06627\n",
      "[226/600] [100/331] Loss_D: 0.74340, Loss_G: 2.39182, Loss_KL: 0.07821\n",
      "[226/600] [200/331] Loss_D: 0.30981, Loss_G: 2.54861, Loss_KL: 0.04728\n",
      "[226/600] [300/331] Loss_D: 0.56592, Loss_G: 3.37275, Loss_KL: 0.06606\n",
      "[226/600] Loss_D: 0.57837, Loss_G: 2.75514, Loss_KL: 0.05536\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[227/600] [0/331] Loss_D: 0.81350, Loss_G: 2.42361, Loss_KL: 0.05027\n",
      "[227/600] [100/331] Loss_D: 0.58245, Loss_G: 2.86958, Loss_KL: 0.04615\n",
      "[227/600] [200/331] Loss_D: 0.30302, Loss_G: 3.06829, Loss_KL: 0.05168\n",
      "[227/600] [300/331] Loss_D: 0.71578, Loss_G: 2.35464, Loss_KL: 0.07286\n",
      "[227/600] Loss_D: 0.57913, Loss_G: 2.76604, Loss_KL: 0.05494\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[228/600] [0/331] Loss_D: 0.34108, Loss_G: 3.09834, Loss_KL: 0.04038\n",
      "[228/600] [100/331] Loss_D: 0.48386, Loss_G: 2.57108, Loss_KL: 0.03576\n",
      "[228/600] [200/331] Loss_D: 0.77551, Loss_G: 2.56410, Loss_KL: 0.08548\n",
      "[228/600] [300/331] Loss_D: 0.70296, Loss_G: 2.11384, Loss_KL: 0.04905\n",
      "[228/600] Loss_D: 0.57643, Loss_G: 2.71342, Loss_KL: 0.05671\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[229/600] [0/331] Loss_D: 0.47704, Loss_G: 3.04721, Loss_KL: 0.06183\n",
      "[229/600] [100/331] Loss_D: 0.38945, Loss_G: 2.85294, Loss_KL: 0.05801\n",
      "[229/600] [200/331] Loss_D: 0.39634, Loss_G: 2.80455, Loss_KL: 0.04295\n",
      "[229/600] [300/331] Loss_D: 0.72289, Loss_G: 2.84123, Loss_KL: 0.05515\n",
      "[229/600] Loss_D: 0.57797, Loss_G: 2.71892, Loss_KL: 0.05835\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[230/600] [0/331] Loss_D: 0.66744, Loss_G: 2.90424, Loss_KL: 0.07305\n",
      "[230/600] [100/331] Loss_D: 0.73203, Loss_G: 2.78067, Loss_KL: 0.03332\n",
      "[230/600] [200/331] Loss_D: 0.66031, Loss_G: 3.28765, Loss_KL: 0.05437\n",
      "[230/600] [300/331] Loss_D: 0.65086, Loss_G: 3.00190, Loss_KL: 0.04304\n",
      "[230/600] Loss_D: 0.57939, Loss_G: 2.78358, Loss_KL: 0.05714\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[231/600] [0/331] Loss_D: 0.79170, Loss_G: 2.27928, Loss_KL: 0.07081\n",
      "[231/600] [100/331] Loss_D: 0.63447, Loss_G: 2.53008, Loss_KL: 0.05683\n",
      "[231/600] [200/331] Loss_D: 0.40211, Loss_G: 2.96917, Loss_KL: 0.07481\n",
      "[231/600] [300/331] Loss_D: 0.67514, Loss_G: 3.25565, Loss_KL: 0.05408\n",
      "[231/600] Loss_D: 0.57910, Loss_G: 2.78415, Loss_KL: 0.05704\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[232/600] [0/331] Loss_D: 0.83707, Loss_G: 1.79620, Loss_KL: 0.06516\n",
      "[232/600] [100/331] Loss_D: 0.47993, Loss_G: 3.06061, Loss_KL: 0.05034\n",
      "[232/600] [200/331] Loss_D: 0.61845, Loss_G: 2.65245, Loss_KL: 0.03560\n",
      "[232/600] [300/331] Loss_D: 0.72785, Loss_G: 2.82845, Loss_KL: 0.07015\n",
      "[232/600] Loss_D: 0.57490, Loss_G: 2.73318, Loss_KL: 0.05875\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[233/600] [0/331] Loss_D: 0.41822, Loss_G: 2.85514, Loss_KL: 0.05616\n",
      "[233/600] [100/331] Loss_D: 0.31529, Loss_G: 2.94765, Loss_KL: 0.04637\n",
      "[233/600] [200/331] Loss_D: 0.33703, Loss_G: 3.54075, Loss_KL: 0.05135\n",
      "[233/600] [300/331] Loss_D: 0.64521, Loss_G: 3.16729, Loss_KL: 0.07241\n",
      "[233/600] Loss_D: 0.57206, Loss_G: 2.76797, Loss_KL: 0.05796\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[234/600] [0/331] Loss_D: 0.32879, Loss_G: 2.91850, Loss_KL: 0.04905\n",
      "[234/600] [100/331] Loss_D: 0.65964, Loss_G: 2.58145, Loss_KL: 0.05104\n",
      "[234/600] [200/331] Loss_D: 0.68522, Loss_G: 2.66018, Loss_KL: 0.06266\n",
      "[234/600] [300/331] Loss_D: 0.24273, Loss_G: 3.36373, Loss_KL: 0.03882\n",
      "[234/600] Loss_D: 0.57604, Loss_G: 2.71475, Loss_KL: 0.05704\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[235/600] [0/331] Loss_D: 0.41603, Loss_G: 3.31241, Loss_KL: 0.06960\n",
      "[235/600] [100/331] Loss_D: 0.74163, Loss_G: 3.13870, Loss_KL: 0.08094\n",
      "[235/600] [200/331] Loss_D: 0.67006, Loss_G: 2.26327, Loss_KL: 0.07720\n",
      "[235/600] [300/331] Loss_D: 0.49101, Loss_G: 2.39374, Loss_KL: 0.04361\n",
      "[235/600] Loss_D: 0.59988, Loss_G: 2.65294, Loss_KL: 0.05762\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[236/600] [0/331] Loss_D: 0.57373, Loss_G: 3.07025, Loss_KL: 0.04591\n",
      "[236/600] [100/331] Loss_D: 0.58393, Loss_G: 1.96386, Loss_KL: 0.04891\n",
      "[236/600] [200/331] Loss_D: 0.44446, Loss_G: 2.88831, Loss_KL: 0.04133\n",
      "[236/600] [300/331] Loss_D: 0.66526, Loss_G: 3.01891, Loss_KL: 0.07758\n",
      "[236/600] Loss_D: 0.57057, Loss_G: 2.71700, Loss_KL: 0.05875\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[237/600] [0/331] Loss_D: 0.38117, Loss_G: 2.68310, Loss_KL: 0.06254\n",
      "[237/600] [100/331] Loss_D: 0.42514, Loss_G: 3.15443, Loss_KL: 0.05555\n",
      "[237/600] [200/331] Loss_D: 0.82247, Loss_G: 2.59396, Loss_KL: 0.04971\n",
      "[237/600] [300/331] Loss_D: 0.42602, Loss_G: 2.87511, Loss_KL: 0.09091\n",
      "[237/600] Loss_D: 0.58154, Loss_G: 2.71668, Loss_KL: 0.05722\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[238/600] [0/331] Loss_D: 0.65732, Loss_G: 2.62262, Loss_KL: 0.08284\n",
      "[238/600] [100/331] Loss_D: 0.76946, Loss_G: 2.68039, Loss_KL: 0.04974\n",
      "[238/600] [200/331] Loss_D: 0.57947, Loss_G: 3.26068, Loss_KL: 0.04940\n",
      "[238/600] [300/331] Loss_D: 0.69910, Loss_G: 2.38018, Loss_KL: 0.04814\n",
      "[238/600] Loss_D: 0.55594, Loss_G: 2.74550, Loss_KL: 0.05585\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[239/600] [0/331] Loss_D: 0.74046, Loss_G: 2.38020, Loss_KL: 0.03941\n",
      "[239/600] [100/331] Loss_D: 0.76162, Loss_G: 3.13218, Loss_KL: 0.05762\n",
      "[239/600] [200/331] Loss_D: 0.66858, Loss_G: 2.70780, Loss_KL: 0.04033\n",
      "[239/600] [300/331] Loss_D: 0.41712, Loss_G: 3.18220, Loss_KL: 0.06591\n",
      "[239/600] Loss_D: 0.56037, Loss_G: 2.73374, Loss_KL: 0.05600\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[240/600] [0/331] Loss_D: 0.54718, Loss_G: 2.70418, Loss_KL: 0.05439\n",
      "[240/600] [100/331] Loss_D: 0.69237, Loss_G: 2.88524, Loss_KL: 0.05104\n",
      "[240/600] [200/331] Loss_D: 0.39468, Loss_G: 2.66743, Loss_KL: 0.05507\n",
      "[240/600] [300/331] Loss_D: 0.58053, Loss_G: 3.15566, Loss_KL: 0.07487\n",
      "[240/600] Loss_D: 0.56992, Loss_G: 2.76485, Loss_KL: 0.05852\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[241/600] [0/331] Loss_D: 0.48716, Loss_G: 2.34041, Loss_KL: 0.05002\n",
      "[241/600] [100/331] Loss_D: 0.25575, Loss_G: 3.27267, Loss_KL: 0.04398\n",
      "[241/600] [200/331] Loss_D: 0.64670, Loss_G: 3.03739, Loss_KL: 0.06870\n",
      "[241/600] [300/331] Loss_D: 0.70823, Loss_G: 2.62596, Loss_KL: 0.06559\n",
      "[241/600] Loss_D: 0.58421, Loss_G: 2.71606, Loss_KL: 0.05921\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[242/600] [0/331] Loss_D: 0.41039, Loss_G: 2.67866, Loss_KL: 0.06166\n",
      "[242/600] [100/331] Loss_D: 0.50529, Loss_G: 2.60846, Loss_KL: 0.04815\n",
      "[242/600] [200/331] Loss_D: 0.70489, Loss_G: 2.78410, Loss_KL: 0.04758\n",
      "[242/600] [300/331] Loss_D: 0.74042, Loss_G: 2.96405, Loss_KL: 0.06501\n",
      "[242/600] Loss_D: 0.60537, Loss_G: 2.66239, Loss_KL: 0.05966\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[243/600] [0/331] Loss_D: 0.75862, Loss_G: 2.61676, Loss_KL: 0.07099\n",
      "[243/600] [100/331] Loss_D: 0.71029, Loss_G: 2.42392, Loss_KL: 0.05336\n",
      "[243/600] [200/331] Loss_D: 0.81493, Loss_G: 2.82835, Loss_KL: 0.08172\n",
      "[243/600] [300/331] Loss_D: 0.41324, Loss_G: 2.56199, Loss_KL: 0.06086\n",
      "[243/600] Loss_D: 0.57374, Loss_G: 2.72506, Loss_KL: 0.06035\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[244/600] [0/331] Loss_D: 0.54131, Loss_G: 2.53315, Loss_KL: 0.03927\n",
      "[244/600] [100/331] Loss_D: 0.38452, Loss_G: 3.19882, Loss_KL: 0.04706\n",
      "[244/600] [200/331] Loss_D: 0.81790, Loss_G: 2.36634, Loss_KL: 0.07757\n",
      "[244/600] [300/331] Loss_D: 0.70140, Loss_G: 2.53548, Loss_KL: 0.06099\n",
      "[244/600] Loss_D: 0.58610, Loss_G: 2.70676, Loss_KL: 0.05919\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[245/600] [0/331] Loss_D: 0.59673, Loss_G: 2.47434, Loss_KL: 0.06729\n",
      "[245/600] [100/331] Loss_D: 0.47069, Loss_G: 2.97826, Loss_KL: 0.06802\n",
      "[245/600] [200/331] Loss_D: 0.32861, Loss_G: 2.76448, Loss_KL: 0.05007\n",
      "[245/600] [300/331] Loss_D: 0.61126, Loss_G: 2.61600, Loss_KL: 0.06363\n",
      "[245/600] Loss_D: 0.56259, Loss_G: 2.75379, Loss_KL: 0.05881\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[246/600] [0/331] Loss_D: 0.74490, Loss_G: 2.72300, Loss_KL: 0.08624\n",
      "[246/600] [100/331] Loss_D: 0.77384, Loss_G: 1.51950, Loss_KL: 0.06127\n",
      "[246/600] [200/331] Loss_D: 0.73513, Loss_G: 2.66524, Loss_KL: 0.04886\n",
      "[246/600] [300/331] Loss_D: 0.56563, Loss_G: 2.51960, Loss_KL: 0.05616\n",
      "[246/600] Loss_D: 0.58646, Loss_G: 2.72126, Loss_KL: 0.05914\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[247/600] [0/331] Loss_D: 0.77185, Loss_G: 2.40369, Loss_KL: 0.05333\n",
      "[247/600] [100/331] Loss_D: 0.74202, Loss_G: 2.41032, Loss_KL: 0.04629\n",
      "[247/600] [200/331] Loss_D: 0.34197, Loss_G: 2.74806, Loss_KL: 0.04817\n",
      "[247/600] [300/331] Loss_D: 0.67910, Loss_G: 2.81090, Loss_KL: 0.06785\n",
      "[247/600] Loss_D: 0.56471, Loss_G: 2.75294, Loss_KL: 0.05982\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[248/600] [0/331] Loss_D: 0.63594, Loss_G: 2.70533, Loss_KL: 0.06598\n",
      "[248/600] [100/331] Loss_D: 0.79237, Loss_G: 2.82358, Loss_KL: 0.05875\n",
      "[248/600] [200/331] Loss_D: 0.51719, Loss_G: 3.14522, Loss_KL: 0.07694\n",
      "[248/600] [300/331] Loss_D: 0.44578, Loss_G: 2.80759, Loss_KL: 0.07282\n",
      "[248/600] Loss_D: 0.58786, Loss_G: 2.72017, Loss_KL: 0.06002\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[249/600] [0/331] Loss_D: 0.68323, Loss_G: 2.13826, Loss_KL: 0.04448\n",
      "[249/600] [100/331] Loss_D: 0.59053, Loss_G: 3.12115, Loss_KL: 0.05527\n",
      "[249/600] [200/331] Loss_D: 0.39908, Loss_G: 2.56151, Loss_KL: 0.05821\n",
      "[249/600] [300/331] Loss_D: 0.70644, Loss_G: 2.61627, Loss_KL: 0.04770\n",
      "[249/600] Loss_D: 0.55563, Loss_G: 2.73707, Loss_KL: 0.06082\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[250/600] [0/331] Loss_D: 0.50758, Loss_G: 2.75180, Loss_KL: 0.04392\n",
      "[250/600] [100/331] Loss_D: 0.74523, Loss_G: 3.01483, Loss_KL: 0.03833\n",
      "[250/600] [200/331] Loss_D: 0.64418, Loss_G: 2.75225, Loss_KL: 0.06057\n",
      "[250/600] [300/331] Loss_D: 0.35537, Loss_G: 3.02975, Loss_KL: 0.06491\n",
      "[250/600] Loss_D: 0.56750, Loss_G: 2.65655, Loss_KL: 0.05863\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[251/600] [0/331] Loss_D: 0.74223, Loss_G: 2.45141, Loss_KL: 0.06945\n",
      "[251/600] [100/331] Loss_D: 0.61439, Loss_G: 2.02412, Loss_KL: 0.05076\n",
      "[251/600] [200/331] Loss_D: 0.60800, Loss_G: 2.44057, Loss_KL: 0.06002\n",
      "[251/600] [300/331] Loss_D: 0.21261, Loss_G: 3.41962, Loss_KL: 0.07438\n",
      "[251/600] Loss_D: 0.57020, Loss_G: 2.75622, Loss_KL: 0.05743\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[252/600] [0/331] Loss_D: 0.70671, Loss_G: 3.12911, Loss_KL: 0.05799\n",
      "[252/600] [100/331] Loss_D: 0.58877, Loss_G: 2.39342, Loss_KL: 0.05671\n",
      "[252/600] [200/331] Loss_D: 0.63320, Loss_G: 2.67248, Loss_KL: 0.03955\n",
      "[252/600] [300/331] Loss_D: 0.63200, Loss_G: 2.90275, Loss_KL: 0.05296\n",
      "[252/600] Loss_D: 0.57340, Loss_G: 2.80110, Loss_KL: 0.05703\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[253/600] [0/331] Loss_D: 0.38206, Loss_G: 2.96435, Loss_KL: 0.06081\n",
      "[253/600] [100/331] Loss_D: 0.63514, Loss_G: 3.06128, Loss_KL: 0.07994\n",
      "[253/600] [200/331] Loss_D: 0.61940, Loss_G: 3.31862, Loss_KL: 0.03879\n",
      "[253/600] [300/331] Loss_D: 0.70973, Loss_G: 2.45121, Loss_KL: 0.07057\n",
      "[253/600] Loss_D: 0.58574, Loss_G: 2.69139, Loss_KL: 0.05588\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[254/600] [0/331] Loss_D: 0.68755, Loss_G: 2.36745, Loss_KL: 0.06041\n",
      "[254/600] [100/331] Loss_D: 0.81091, Loss_G: 2.43596, Loss_KL: 0.05982\n",
      "[254/600] [200/331] Loss_D: 0.75553, Loss_G: 2.60798, Loss_KL: 0.05804\n",
      "[254/600] [300/331] Loss_D: 0.71374, Loss_G: 2.60426, Loss_KL: 0.04257\n",
      "[254/600] Loss_D: 0.55161, Loss_G: 2.67254, Loss_KL: 0.05631\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[255/600] [0/331] Loss_D: 0.70136, Loss_G: 2.75816, Loss_KL: 0.06175\n",
      "[255/600] [100/331] Loss_D: 0.78641, Loss_G: 2.55454, Loss_KL: 0.07031\n",
      "[255/600] [200/331] Loss_D: 0.66022, Loss_G: 2.91900, Loss_KL: 0.03785\n",
      "[255/600] [300/331] Loss_D: 0.70160, Loss_G: 3.21267, Loss_KL: 0.07022\n",
      "[255/600] Loss_D: 0.56591, Loss_G: 2.65425, Loss_KL: 0.05437\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[256/600] [0/331] Loss_D: 0.59565, Loss_G: 2.76345, Loss_KL: 0.04017\n",
      "[256/600] [100/331] Loss_D: 0.48942, Loss_G: 2.59710, Loss_KL: 0.04792\n",
      "[256/600] [200/331] Loss_D: 0.70708, Loss_G: 2.80342, Loss_KL: 0.05688\n",
      "[256/600] [300/331] Loss_D: 0.63494, Loss_G: 2.58495, Loss_KL: 0.05420\n",
      "[256/600] Loss_D: 0.54943, Loss_G: 2.63922, Loss_KL: 0.05242\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[257/600] [0/331] Loss_D: 0.61915, Loss_G: 2.58395, Loss_KL: 0.05730\n",
      "[257/600] [100/331] Loss_D: 0.58093, Loss_G: 2.46731, Loss_KL: 0.05830\n",
      "[257/600] [200/331] Loss_D: 0.39782, Loss_G: 2.62173, Loss_KL: 0.06458\n",
      "[257/600] [300/331] Loss_D: 0.71608, Loss_G: 2.55475, Loss_KL: 0.05138\n",
      "[257/600] Loss_D: 0.54727, Loss_G: 2.60292, Loss_KL: 0.05061\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[258/600] [0/331] Loss_D: 0.68730, Loss_G: 2.08278, Loss_KL: 0.05976\n",
      "[258/600] [100/331] Loss_D: 0.36713, Loss_G: 2.39761, Loss_KL: 0.05608\n",
      "[258/600] [200/331] Loss_D: 0.82107, Loss_G: 2.42601, Loss_KL: 0.03342\n",
      "[258/600] [300/331] Loss_D: 0.69572, Loss_G: 2.53001, Loss_KL: 0.02816\n",
      "[258/600] Loss_D: 0.53868, Loss_G: 2.56597, Loss_KL: 0.04836\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[259/600] [0/331] Loss_D: 0.67011, Loss_G: 2.86578, Loss_KL: 0.03690\n",
      "[259/600] [100/331] Loss_D: 0.39085, Loss_G: 2.68631, Loss_KL: 0.04082\n",
      "[259/600] [200/331] Loss_D: 0.61962, Loss_G: 2.58350, Loss_KL: 0.04083\n",
      "[259/600] [300/331] Loss_D: 0.72131, Loss_G: 2.49627, Loss_KL: 0.02463\n",
      "[259/600] Loss_D: 0.56647, Loss_G: 2.59984, Loss_KL: 0.04570\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[260/600] [0/331] Loss_D: 0.38322, Loss_G: 2.59245, Loss_KL: 0.04453\n",
      "[260/600] [100/331] Loss_D: 0.65322, Loss_G: 2.45421, Loss_KL: 0.04374\n",
      "[260/600] [200/331] Loss_D: 0.55478, Loss_G: 2.55410, Loss_KL: 0.05371\n",
      "[260/600] [300/331] Loss_D: 0.64557, Loss_G: 2.66818, Loss_KL: 0.05473\n",
      "[260/600] Loss_D: 0.55968, Loss_G: 2.60830, Loss_KL: 0.04340\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[261/600] [0/331] Loss_D: 0.36503, Loss_G: 2.52475, Loss_KL: 0.03221\n",
      "[261/600] [100/331] Loss_D: 0.66822, Loss_G: 2.40115, Loss_KL: 0.02147\n",
      "[261/600] [200/331] Loss_D: 0.67584, Loss_G: 2.18647, Loss_KL: 0.03805\n",
      "[261/600] [300/331] Loss_D: 0.75485, Loss_G: 3.16880, Loss_KL: 0.07440\n",
      "[261/600] Loss_D: 0.58130, Loss_G: 2.85703, Loss_KL: 0.04309\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[262/600] [0/331] Loss_D: 0.70911, Loss_G: 3.26548, Loss_KL: 0.03380\n",
      "[262/600] [100/331] Loss_D: 0.48614, Loss_G: 3.39341, Loss_KL: 0.04308\n",
      "[262/600] [200/331] Loss_D: 0.65190, Loss_G: 2.70231, Loss_KL: 0.03441\n",
      "[262/600] [300/331] Loss_D: 0.68714, Loss_G: 2.37955, Loss_KL: 0.04836\n",
      "[262/600] Loss_D: 0.55039, Loss_G: 2.84706, Loss_KL: 0.04416\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[263/600] [0/331] Loss_D: 0.60505, Loss_G: 2.94805, Loss_KL: 0.03358\n",
      "[263/600] [100/331] Loss_D: 0.67928, Loss_G: 2.79507, Loss_KL: 0.05886\n",
      "[263/600] [200/331] Loss_D: 0.68960, Loss_G: 2.50738, Loss_KL: 0.05161\n",
      "[263/600] [300/331] Loss_D: 0.51482, Loss_G: 2.53239, Loss_KL: 0.04916\n",
      "[263/600] Loss_D: 0.55852, Loss_G: 2.77851, Loss_KL: 0.04506\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[264/600] [0/331] Loss_D: 0.56890, Loss_G: 2.47840, Loss_KL: 0.04211\n",
      "[264/600] [100/331] Loss_D: 0.64742, Loss_G: 2.61910, Loss_KL: 0.04809\n",
      "[264/600] [200/331] Loss_D: 0.29891, Loss_G: 2.88497, Loss_KL: 0.02616\n",
      "[264/600] [300/331] Loss_D: 0.56104, Loss_G: 2.60456, Loss_KL: 0.03347\n",
      "[264/600] Loss_D: 0.55800, Loss_G: 2.69108, Loss_KL: 0.04495\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[265/600] [0/331] Loss_D: 0.44763, Loss_G: 2.34599, Loss_KL: 0.03650\n",
      "[265/600] [100/331] Loss_D: 0.48801, Loss_G: 2.55873, Loss_KL: 0.03848\n",
      "[265/600] [200/331] Loss_D: 0.32624, Loss_G: 2.70433, Loss_KL: 0.04116\n",
      "[265/600] [300/331] Loss_D: 0.67675, Loss_G: 2.71354, Loss_KL: 0.04943\n",
      "[265/600] Loss_D: 0.55903, Loss_G: 2.67464, Loss_KL: 0.04600\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[266/600] [0/331] Loss_D: 0.55276, Loss_G: 2.43384, Loss_KL: 0.03489\n",
      "[266/600] [100/331] Loss_D: 0.69972, Loss_G: 2.41592, Loss_KL: 0.03144\n",
      "[266/600] [200/331] Loss_D: 0.68363, Loss_G: 2.47545, Loss_KL: 0.04385\n",
      "[266/600] [300/331] Loss_D: 0.83024, Loss_G: 2.63182, Loss_KL: 0.04104\n",
      "[266/600] Loss_D: 0.55633, Loss_G: 2.69805, Loss_KL: 0.04420\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[267/600] [0/331] Loss_D: 0.70319, Loss_G: 2.48262, Loss_KL: 0.04972\n",
      "[267/600] [100/331] Loss_D: 0.67732, Loss_G: 2.67323, Loss_KL: 0.02937\n",
      "[267/600] [200/331] Loss_D: 0.47798, Loss_G: 2.72690, Loss_KL: 0.02597\n",
      "[267/600] [300/331] Loss_D: 0.84468, Loss_G: 2.38652, Loss_KL: 0.02897\n",
      "[267/600] Loss_D: 0.55522, Loss_G: 2.66781, Loss_KL: 0.04394\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[268/600] [0/331] Loss_D: 0.67939, Loss_G: 2.56816, Loss_KL: 0.02990\n",
      "[268/600] [100/331] Loss_D: 0.60856, Loss_G: 2.69270, Loss_KL: 0.03411\n",
      "[268/600] [200/331] Loss_D: 0.44408, Loss_G: 2.69152, Loss_KL: 0.03190\n",
      "[268/600] [300/331] Loss_D: 0.43169, Loss_G: 2.45296, Loss_KL: 0.04981\n",
      "[268/600] Loss_D: 0.54864, Loss_G: 2.71314, Loss_KL: 0.04234\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[269/600] [0/331] Loss_D: 0.55654, Loss_G: 2.58205, Loss_KL: 0.03890\n",
      "[269/600] [100/331] Loss_D: 0.66051, Loss_G: 3.03945, Loss_KL: 0.04310\n",
      "[269/600] [200/331] Loss_D: 0.70564, Loss_G: 2.47267, Loss_KL: 0.03287\n",
      "[269/600] [300/331] Loss_D: 0.36705, Loss_G: 2.54232, Loss_KL: 0.03838\n",
      "[269/600] Loss_D: 0.54601, Loss_G: 2.68269, Loss_KL: 0.04184\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[270/600] [0/331] Loss_D: 0.75815, Loss_G: 2.82566, Loss_KL: 0.03837\n",
      "[270/600] [100/331] Loss_D: 0.64505, Loss_G: 2.73922, Loss_KL: 0.06335\n",
      "[270/600] [200/331] Loss_D: 0.44033, Loss_G: 2.43674, Loss_KL: 0.03694\n",
      "[270/600] [300/331] Loss_D: 0.35322, Loss_G: 2.98048, Loss_KL: 0.03507\n",
      "[270/600] Loss_D: 0.54758, Loss_G: 2.72853, Loss_KL: 0.04066\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[271/600] [0/331] Loss_D: 0.77905, Loss_G: 3.33161, Loss_KL: 0.04850\n",
      "[271/600] [100/331] Loss_D: 0.44826, Loss_G: 2.56903, Loss_KL: 0.06048\n",
      "[271/600] [200/331] Loss_D: 0.25426, Loss_G: 2.83356, Loss_KL: 0.03961\n",
      "[271/600] [300/331] Loss_D: 0.67271, Loss_G: 2.84230, Loss_KL: 0.02931\n",
      "[271/600] Loss_D: 0.55966, Loss_G: 2.69784, Loss_KL: 0.03701\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[272/600] [0/331] Loss_D: 0.70302, Loss_G: 2.62993, Loss_KL: 0.01655\n",
      "[272/600] [100/331] Loss_D: 0.67065, Loss_G: 2.98366, Loss_KL: 0.05893\n",
      "[272/600] [200/331] Loss_D: 0.43430, Loss_G: 2.80947, Loss_KL: 0.04588\n",
      "[272/600] [300/331] Loss_D: 0.68296, Loss_G: 2.65189, Loss_KL: 0.03261\n",
      "[272/600] Loss_D: 0.55889, Loss_G: 2.76321, Loss_KL: 0.03594\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[273/600] [0/331] Loss_D: 0.62265, Loss_G: 2.85942, Loss_KL: 0.03554\n",
      "[273/600] [100/331] Loss_D: 0.46318, Loss_G: 2.59864, Loss_KL: 0.06415\n",
      "[273/600] [200/331] Loss_D: 0.67753, Loss_G: 2.66510, Loss_KL: 0.02739\n",
      "[273/600] [300/331] Loss_D: 0.36237, Loss_G: 2.60452, Loss_KL: 0.04030\n",
      "[273/600] Loss_D: 0.56440, Loss_G: 2.72186, Loss_KL: 0.03671\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[274/600] [0/331] Loss_D: 0.55648, Loss_G: 2.54318, Loss_KL: 0.04129\n",
      "[274/600] [100/331] Loss_D: 0.75111, Loss_G: 2.40229, Loss_KL: 0.07143\n",
      "[274/600] [200/331] Loss_D: 0.60797, Loss_G: 2.44575, Loss_KL: 0.05883\n",
      "[274/600] [300/331] Loss_D: 0.37023, Loss_G: 2.82630, Loss_KL: 0.03323\n",
      "[274/600] Loss_D: 0.55733, Loss_G: 2.66018, Loss_KL: 0.03497\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[275/600] [0/331] Loss_D: 0.35151, Loss_G: 2.61857, Loss_KL: 0.03481\n",
      "[275/600] [100/331] Loss_D: 0.51343, Loss_G: 2.90828, Loss_KL: 0.02295\n",
      "[275/600] [200/331] Loss_D: 0.23313, Loss_G: 2.63613, Loss_KL: 0.02551\n",
      "[275/600] [300/331] Loss_D: 0.52123, Loss_G: 2.72101, Loss_KL: 0.06225\n",
      "[275/600] Loss_D: 0.54736, Loss_G: 2.69747, Loss_KL: 0.03533\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[276/600] [0/331] Loss_D: 0.45520, Loss_G: 2.80259, Loss_KL: 0.03719\n",
      "[276/600] [100/331] Loss_D: 0.57819, Loss_G: 2.79727, Loss_KL: 0.02063\n",
      "[276/600] [200/331] Loss_D: 0.64656, Loss_G: 2.58812, Loss_KL: 0.02761\n",
      "[276/600] [300/331] Loss_D: 0.38829, Loss_G: 2.85913, Loss_KL: 0.04120\n",
      "[276/600] Loss_D: 0.55392, Loss_G: 2.67999, Loss_KL: 0.03694\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[277/600] [0/331] Loss_D: 0.54172, Loss_G: 2.58162, Loss_KL: 0.03251\n",
      "[277/600] [100/331] Loss_D: 0.66624, Loss_G: 2.79632, Loss_KL: 0.02751\n",
      "[277/600] [200/331] Loss_D: 0.75004, Loss_G: 2.85207, Loss_KL: 0.01575\n",
      "[277/600] [300/331] Loss_D: 0.66445, Loss_G: 2.53061, Loss_KL: 0.03179\n",
      "[277/600] Loss_D: 0.56289, Loss_G: 2.61871, Loss_KL: 0.03270\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[278/600] [0/331] Loss_D: 0.69344, Loss_G: 2.41931, Loss_KL: 0.01778\n",
      "[278/600] [100/331] Loss_D: 0.76175, Loss_G: 2.73798, Loss_KL: 0.03208\n",
      "[278/600] [200/331] Loss_D: 0.39093, Loss_G: 2.51844, Loss_KL: 0.03781\n",
      "[278/600] [300/331] Loss_D: 0.28207, Loss_G: 2.57670, Loss_KL: 0.03740\n",
      "[278/600] Loss_D: 0.55824, Loss_G: 2.61054, Loss_KL: 0.03075\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[279/600] [0/331] Loss_D: 0.64629, Loss_G: 2.45827, Loss_KL: 0.04189\n",
      "[279/600] [100/331] Loss_D: 0.31297, Loss_G: 2.93610, Loss_KL: 0.02162\n",
      "[279/600] [200/331] Loss_D: 0.66209, Loss_G: 2.58937, Loss_KL: 0.02630\n",
      "[279/600] [300/331] Loss_D: 0.66627, Loss_G: 2.45917, Loss_KL: 0.02043\n",
      "[279/600] Loss_D: 0.55361, Loss_G: 2.64320, Loss_KL: 0.02921\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[280/600] [0/331] Loss_D: 0.32283, Loss_G: 2.82669, Loss_KL: 0.03134\n",
      "[280/600] [100/331] Loss_D: 0.74279, Loss_G: 2.65190, Loss_KL: 0.01870\n",
      "[280/600] [200/331] Loss_D: 0.69850, Loss_G: 2.89174, Loss_KL: 0.02122\n",
      "[280/600] [300/331] Loss_D: 0.38809, Loss_G: 2.70046, Loss_KL: 0.02617\n",
      "[280/600] Loss_D: 0.55152, Loss_G: 2.64921, Loss_KL: 0.02983\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Save G1/D1 models\n",
      "[281/600] [0/331] Loss_D: 0.51828, Loss_G: 2.64575, Loss_KL: 0.05552\n",
      "[281/600] [100/331] Loss_D: 0.65279, Loss_G: 2.54871, Loss_KL: 0.05237\n",
      "[281/600] [200/331] Loss_D: 0.54353, Loss_G: 2.75640, Loss_KL: 0.02740\n",
      "[281/600] [300/331] Loss_D: 0.63642, Loss_G: 2.55778, Loss_KL: 0.03000\n",
      "[281/600] Loss_D: 0.54472, Loss_G: 2.70385, Loss_KL: 0.02783\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[282/600] [0/331] Loss_D: 0.71723, Loss_G: 2.47818, Loss_KL: 0.02366\n",
      "[282/600] [100/331] Loss_D: 0.67626, Loss_G: 2.87470, Loss_KL: 0.02747\n",
      "[282/600] [200/331] Loss_D: 0.30105, Loss_G: 2.52620, Loss_KL: 0.02000\n",
      "[282/600] [300/331] Loss_D: 0.60784, Loss_G: 2.43372, Loss_KL: 0.04268\n",
      "[282/600] Loss_D: 0.56108, Loss_G: 2.64192, Loss_KL: 0.02838\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[283/600] [0/331] Loss_D: 0.22676, Loss_G: 2.67395, Loss_KL: 0.03229\n",
      "[283/600] [100/331] Loss_D: 0.72855, Loss_G: 2.53707, Loss_KL: 0.04205\n",
      "[283/600] [200/331] Loss_D: 0.74608, Loss_G: 2.50846, Loss_KL: 0.06148\n",
      "[283/600] [300/331] Loss_D: 0.50776, Loss_G: 2.76469, Loss_KL: 0.01090\n",
      "[283/600] Loss_D: 0.54684, Loss_G: 2.64941, Loss_KL: 0.02800\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[284/600] [0/331] Loss_D: 0.63079, Loss_G: 2.62019, Loss_KL: 0.01926\n",
      "[284/600] [100/331] Loss_D: 0.72150, Loss_G: 2.50267, Loss_KL: 0.02482\n",
      "[284/600] [200/331] Loss_D: 0.62539, Loss_G: 2.69691, Loss_KL: 0.03156\n",
      "[284/600] [300/331] Loss_D: 0.43978, Loss_G: 2.55055, Loss_KL: 0.02641\n",
      "[284/600] Loss_D: 0.56388, Loss_G: 2.64595, Loss_KL: 0.02734\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[285/600] [0/331] Loss_D: 0.23639, Loss_G: 2.55061, Loss_KL: 0.03051\n",
      "[285/600] [100/331] Loss_D: 0.73105, Loss_G: 2.47886, Loss_KL: 0.01834\n",
      "[285/600] [200/331] Loss_D: 0.61143, Loss_G: 2.74584, Loss_KL: 0.02523\n",
      "[285/600] [300/331] Loss_D: 0.58214, Loss_G: 3.22666, Loss_KL: 0.02520\n",
      "[285/600] Loss_D: 0.55709, Loss_G: 2.56985, Loss_KL: 0.02580\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[286/600] [0/331] Loss_D: 0.61446, Loss_G: 2.25378, Loss_KL: 0.01528\n",
      "[286/600] [100/331] Loss_D: 0.64104, Loss_G: 2.57065, Loss_KL: 0.01827\n",
      "[286/600] [200/331] Loss_D: 0.69368, Loss_G: 2.56288, Loss_KL: 0.01779\n",
      "[286/600] [300/331] Loss_D: 0.70623, Loss_G: 2.27391, Loss_KL: 0.01783\n",
      "[286/600] Loss_D: 0.56471, Loss_G: 2.55259, Loss_KL: 0.02243\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[287/600] [0/331] Loss_D: 0.47272, Loss_G: 2.76116, Loss_KL: 0.02365\n",
      "[287/600] [100/331] Loss_D: 0.63907, Loss_G: 2.72157, Loss_KL: 0.01326\n",
      "[287/600] [200/331] Loss_D: 0.56413, Loss_G: 2.59034, Loss_KL: 0.01808\n",
      "[287/600] [300/331] Loss_D: 0.49926, Loss_G: 2.47083, Loss_KL: 0.01475\n",
      "[287/600] Loss_D: 0.55172, Loss_G: 2.67602, Loss_KL: 0.02046\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[288/600] [0/331] Loss_D: 0.24218, Loss_G: 2.68788, Loss_KL: 0.01761\n",
      "[288/600] [100/331] Loss_D: 0.74423, Loss_G: 2.65057, Loss_KL: 0.01860\n",
      "[288/600] [200/331] Loss_D: 0.46104, Loss_G: 2.62032, Loss_KL: 0.01603\n",
      "[288/600] [300/331] Loss_D: 0.63402, Loss_G: 2.40233, Loss_KL: 0.02241\n",
      "[288/600] Loss_D: 0.55472, Loss_G: 2.66484, Loss_KL: 0.02281\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[289/600] [0/331] Loss_D: 0.72532, Loss_G: 2.47688, Loss_KL: 0.03059\n",
      "[289/600] [100/331] Loss_D: 0.41487, Loss_G: 2.46978, Loss_KL: 0.02288\n",
      "[289/600] [200/331] Loss_D: 0.24800, Loss_G: 3.05713, Loss_KL: 0.01551\n",
      "[289/600] [300/331] Loss_D: 0.23201, Loss_G: 2.68940, Loss_KL: 0.03681\n",
      "[289/600] Loss_D: 0.56024, Loss_G: 2.61298, Loss_KL: 0.02168\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[290/600] [0/331] Loss_D: 0.46599, Loss_G: 2.46121, Loss_KL: 0.01090\n",
      "[290/600] [100/331] Loss_D: 0.32562, Loss_G: 2.72842, Loss_KL: 0.01098\n",
      "[290/600] [200/331] Loss_D: 0.65127, Loss_G: 2.56591, Loss_KL: 0.03779\n",
      "[290/600] [300/331] Loss_D: 0.66579, Loss_G: 2.41177, Loss_KL: 0.01976\n",
      "[290/600] Loss_D: 0.55549, Loss_G: 2.66322, Loss_KL: 0.02223\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[291/600] [0/331] Loss_D: 0.70694, Loss_G: 2.31133, Loss_KL: 0.02149\n",
      "[291/600] [100/331] Loss_D: 0.48055, Loss_G: 2.84479, Loss_KL: 0.01851\n",
      "[291/600] [200/331] Loss_D: 0.74200, Loss_G: 2.61447, Loss_KL: 0.00506\n",
      "[291/600] [300/331] Loss_D: 0.59476, Loss_G: 2.89981, Loss_KL: 0.02760\n",
      "[291/600] Loss_D: 0.54144, Loss_G: 2.66851, Loss_KL: 0.02427\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[292/600] [0/331] Loss_D: 0.29692, Loss_G: 2.58976, Loss_KL: 0.01811\n",
      "[292/600] [100/331] Loss_D: 0.47680, Loss_G: 2.60561, Loss_KL: 0.01592\n",
      "[292/600] [200/331] Loss_D: 0.35045, Loss_G: 2.58796, Loss_KL: 0.02883\n",
      "[292/600] [300/331] Loss_D: 0.75687, Loss_G: 2.42061, Loss_KL: 0.00902\n",
      "[292/600] Loss_D: 0.56345, Loss_G: 2.61534, Loss_KL: 0.02478\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[293/600] [0/331] Loss_D: 0.68855, Loss_G: 2.47255, Loss_KL: 0.01880\n",
      "[293/600] [100/331] Loss_D: 0.43642, Loss_G: 2.54833, Loss_KL: 0.03880\n",
      "[293/600] [200/331] Loss_D: 0.70257, Loss_G: 2.73531, Loss_KL: 0.02897\n",
      "[293/600] [300/331] Loss_D: 0.29934, Loss_G: 2.76033, Loss_KL: 0.01166\n",
      "[293/600] Loss_D: 0.55159, Loss_G: 2.65093, Loss_KL: 0.02684\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[294/600] [0/331] Loss_D: 0.33799, Loss_G: 2.80038, Loss_KL: 0.03959\n",
      "[294/600] [100/331] Loss_D: 0.64220, Loss_G: 2.64207, Loss_KL: 0.02417\n",
      "[294/600] [200/331] Loss_D: 0.64229, Loss_G: 2.48005, Loss_KL: 0.02919\n",
      "[294/600] [300/331] Loss_D: 0.48552, Loss_G: 2.86368, Loss_KL: 0.03506\n",
      "[294/600] Loss_D: 0.54351, Loss_G: 2.68139, Loss_KL: 0.02738\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[295/600] [0/331] Loss_D: 0.45563, Loss_G: 2.87888, Loss_KL: 0.02164\n",
      "[295/600] [100/331] Loss_D: 0.57906, Loss_G: 2.42863, Loss_KL: 0.01360\n",
      "[295/600] [200/331] Loss_D: 0.25869, Loss_G: 2.61817, Loss_KL: 0.00999\n",
      "[295/600] [300/331] Loss_D: 0.67561, Loss_G: 2.49739, Loss_KL: 0.02403\n",
      "[295/600] Loss_D: 0.55959, Loss_G: 2.61835, Loss_KL: 0.02515\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[296/600] [0/331] Loss_D: 0.49367, Loss_G: 2.83965, Loss_KL: 0.04042\n",
      "[296/600] [100/331] Loss_D: 0.46313, Loss_G: 2.57701, Loss_KL: 0.01857\n",
      "[296/600] [200/331] Loss_D: 0.48265, Loss_G: 2.44767, Loss_KL: 0.02731\n",
      "[296/600] [300/331] Loss_D: 0.24758, Loss_G: 2.54238, Loss_KL: 0.01591\n",
      "[296/600] Loss_D: 0.56224, Loss_G: 2.59173, Loss_KL: 0.02452\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[297/600] [0/331] Loss_D: 0.69650, Loss_G: 2.48977, Loss_KL: 0.02316\n",
      "[297/600] [100/331] Loss_D: 0.45934, Loss_G: 2.64714, Loss_KL: 0.01713\n",
      "[297/600] [200/331] Loss_D: 0.39871, Loss_G: 2.73390, Loss_KL: 0.02869\n",
      "[297/600] [300/331] Loss_D: 0.72367, Loss_G: 2.44150, Loss_KL: 0.03779\n",
      "[297/600] Loss_D: 0.55601, Loss_G: 2.61898, Loss_KL: 0.02270\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[298/600] [0/331] Loss_D: 0.61431, Loss_G: 2.45710, Loss_KL: 0.02012\n",
      "[298/600] [100/331] Loss_D: 0.70535, Loss_G: 2.79091, Loss_KL: 0.02595\n",
      "[298/600] [200/331] Loss_D: 0.64628, Loss_G: 2.49510, Loss_KL: 0.02253\n",
      "[298/600] [300/331] Loss_D: 0.64221, Loss_G: 2.38491, Loss_KL: 0.01867\n",
      "[298/600] Loss_D: 0.55926, Loss_G: 2.60393, Loss_KL: 0.02249\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[299/600] [0/331] Loss_D: 0.30057, Loss_G: 2.74275, Loss_KL: 0.02479\n",
      "[299/600] [100/331] Loss_D: 0.71351, Loss_G: 2.68912, Loss_KL: 0.02048\n",
      "[299/600] [200/331] Loss_D: 0.49669, Loss_G: 3.11671, Loss_KL: 0.02446\n",
      "[299/600] [300/331] Loss_D: 0.53586, Loss_G: 2.64930, Loss_KL: 0.01065\n",
      "[299/600] Loss_D: 0.53945, Loss_G: 2.68692, Loss_KL: 0.02141\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "Adjusting learning rate of group 0 to 5.0000e-05.\n",
      "[300/600] [0/331] Loss_D: 0.82750, Loss_G: 2.57945, Loss_KL: 0.01190\n",
      "[300/600] [100/331] Loss_D: 0.69634, Loss_G: 2.54120, Loss_KL: 0.01778\n",
      "[300/600] [200/331] Loss_D: 0.62468, Loss_G: 2.43386, Loss_KL: 0.01694\n",
      "[300/600] [300/331] Loss_D: 0.56789, Loss_G: 2.43334, Loss_KL: 0.00650\n",
      "[300/600] Loss_D: 0.56792, Loss_G: 2.55072, Loss_KL: 0.01889\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[301/600] [0/331] Loss_D: 0.35587, Loss_G: 2.94487, Loss_KL: 0.03618\n",
      "[301/600] [100/331] Loss_D: 0.55443, Loss_G: 2.65019, Loss_KL: 0.00852\n",
      "[301/600] [200/331] Loss_D: 0.47244, Loss_G: 2.48182, Loss_KL: 0.01571\n",
      "[301/600] [300/331] Loss_D: 0.46592, Loss_G: 2.51547, Loss_KL: 0.00949\n",
      "[301/600] Loss_D: 0.55457, Loss_G: 2.51917, Loss_KL: 0.01746\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[302/600] [0/331] Loss_D: 0.68945, Loss_G: 2.87421, Loss_KL: 0.00734\n",
      "[302/600] [100/331] Loss_D: 0.52580, Loss_G: 2.67205, Loss_KL: 0.01561\n",
      "[302/600] [200/331] Loss_D: 0.68928, Loss_G: 2.90952, Loss_KL: 0.02299\n",
      "[302/600] [300/331] Loss_D: 0.32024, Loss_G: 2.60483, Loss_KL: 0.01342\n",
      "[302/600] Loss_D: 0.55759, Loss_G: 2.58833, Loss_KL: 0.01718\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[303/600] [0/331] Loss_D: 0.62284, Loss_G: 2.44763, Loss_KL: 0.04749\n",
      "[303/600] [100/331] Loss_D: 0.74772, Loss_G: 2.70741, Loss_KL: 0.02752\n",
      "[303/600] [200/331] Loss_D: 0.34004, Loss_G: 2.20742, Loss_KL: 0.01149\n",
      "[303/600] [300/331] Loss_D: 0.52544, Loss_G: 2.80719, Loss_KL: 0.01097\n",
      "[303/600] Loss_D: 0.56397, Loss_G: 2.68952, Loss_KL: 0.01805\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[304/600] [0/331] Loss_D: 0.42916, Loss_G: 2.34465, Loss_KL: 0.03542\n",
      "[304/600] [100/331] Loss_D: 0.51375, Loss_G: 2.34615, Loss_KL: 0.01364\n",
      "[304/600] [200/331] Loss_D: 0.23018, Loss_G: 2.67330, Loss_KL: 0.01679\n",
      "[304/600] [300/331] Loss_D: 0.68190, Loss_G: 2.47483, Loss_KL: 0.02456\n",
      "[304/600] Loss_D: 0.55314, Loss_G: 2.62428, Loss_KL: 0.01816\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[305/600] [0/331] Loss_D: 0.70544, Loss_G: 2.52343, Loss_KL: 0.02121\n",
      "[305/600] [100/331] Loss_D: 0.76736, Loss_G: 2.47929, Loss_KL: 0.02240\n",
      "[305/600] [200/331] Loss_D: 0.78801, Loss_G: 2.54106, Loss_KL: 0.01353\n",
      "[305/600] [300/331] Loss_D: 0.42695, Loss_G: 2.88674, Loss_KL: 0.02255\n",
      "[305/600] Loss_D: 0.54473, Loss_G: 2.62170, Loss_KL: 0.01809\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[306/600] [0/331] Loss_D: 0.49500, Loss_G: 2.50896, Loss_KL: 0.01679\n",
      "[306/600] [100/331] Loss_D: 0.42908, Loss_G: 2.74376, Loss_KL: 0.00937\n",
      "[306/600] [200/331] Loss_D: 0.67037, Loss_G: 2.48965, Loss_KL: 0.01354\n",
      "[306/600] [300/331] Loss_D: 0.73130, Loss_G: 2.84146, Loss_KL: 0.01847\n",
      "[306/600] Loss_D: 0.55269, Loss_G: 2.63226, Loss_KL: 0.01837\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[307/600] [0/331] Loss_D: 0.38050, Loss_G: 2.46029, Loss_KL: 0.01432\n",
      "[307/600] [100/331] Loss_D: 0.58348, Loss_G: 2.47684, Loss_KL: 0.01479\n",
      "[307/600] [200/331] Loss_D: 0.67285, Loss_G: 2.57208, Loss_KL: 0.02793\n",
      "[307/600] [300/331] Loss_D: 0.74822, Loss_G: 2.77444, Loss_KL: 0.01453\n",
      "[307/600] Loss_D: 0.53743, Loss_G: 2.63709, Loss_KL: 0.01842\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[308/600] [0/331] Loss_D: 0.49176, Loss_G: 2.84802, Loss_KL: 0.01706\n",
      "[308/600] [100/331] Loss_D: 0.74697, Loss_G: 2.43620, Loss_KL: 0.03767\n",
      "[308/600] [200/331] Loss_D: 0.31236, Loss_G: 2.46446, Loss_KL: 0.03899\n",
      "[308/600] [300/331] Loss_D: 0.62147, Loss_G: 2.69430, Loss_KL: 0.00976\n",
      "[308/600] Loss_D: 0.56340, Loss_G: 2.60711, Loss_KL: 0.01763\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[309/600] [0/331] Loss_D: 0.48136, Loss_G: 2.45939, Loss_KL: 0.00866\n",
      "[309/600] [100/331] Loss_D: 0.35149, Loss_G: 2.61104, Loss_KL: 0.02615\n",
      "[309/600] [200/331] Loss_D: 0.44239, Loss_G: 2.63853, Loss_KL: 0.02495\n",
      "[309/600] [300/331] Loss_D: 0.32398, Loss_G: 2.40347, Loss_KL: 0.02755\n",
      "[309/600] Loss_D: 0.55499, Loss_G: 2.61134, Loss_KL: 0.01668\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[310/600] [0/331] Loss_D: 0.69519, Loss_G: 2.56473, Loss_KL: 0.01250\n",
      "[310/600] [100/331] Loss_D: 0.70335, Loss_G: 2.46254, Loss_KL: 0.01509\n",
      "[310/600] [200/331] Loss_D: 0.53813, Loss_G: 2.54247, Loss_KL: 0.01027\n",
      "[310/600] [300/331] Loss_D: 0.64287, Loss_G: 2.67240, Loss_KL: 0.00764\n",
      "[310/600] Loss_D: 0.54270, Loss_G: 2.64757, Loss_KL: 0.01715\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[311/600] [0/331] Loss_D: 0.66554, Loss_G: 2.82429, Loss_KL: 0.00751\n",
      "[311/600] [100/331] Loss_D: 0.33330, Loss_G: 2.65616, Loss_KL: 0.01285\n",
      "[311/600] [200/331] Loss_D: 0.55541, Loss_G: 2.63675, Loss_KL: 0.02991\n",
      "[311/600] [300/331] Loss_D: 0.67471, Loss_G: 2.68357, Loss_KL: 0.01619\n",
      "[311/600] Loss_D: 0.53245, Loss_G: 2.67087, Loss_KL: 0.01693\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[312/600] [0/331] Loss_D: 0.66562, Loss_G: 2.42771, Loss_KL: 0.00736\n",
      "[312/600] [100/331] Loss_D: 0.22472, Loss_G: 2.80435, Loss_KL: 0.02423\n",
      "[312/600] [200/331] Loss_D: 0.67424, Loss_G: 2.46165, Loss_KL: 0.01321\n",
      "[312/600] [300/331] Loss_D: 0.83616, Loss_G: 2.75487, Loss_KL: 0.01768\n",
      "[312/600] Loss_D: 0.53908, Loss_G: 2.64055, Loss_KL: 0.01639\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[313/600] [0/331] Loss_D: 0.49197, Loss_G: 2.63206, Loss_KL: 0.01100\n",
      "[313/600] [100/331] Loss_D: 0.39039, Loss_G: 2.86114, Loss_KL: 0.02237\n",
      "[313/600] [200/331] Loss_D: 0.42788, Loss_G: 2.57883, Loss_KL: 0.03253\n",
      "[313/600] [300/331] Loss_D: 0.69687, Loss_G: 2.41743, Loss_KL: 0.01663\n",
      "[313/600] Loss_D: 0.55748, Loss_G: 2.62245, Loss_KL: 0.01592\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[314/600] [0/331] Loss_D: 0.62056, Loss_G: 2.45028, Loss_KL: 0.01678\n",
      "[314/600] [100/331] Loss_D: 0.31763, Loss_G: 2.70431, Loss_KL: 0.02754\n",
      "[314/600] [200/331] Loss_D: 0.30876, Loss_G: 2.53959, Loss_KL: 0.02661\n",
      "[314/600] [300/331] Loss_D: 0.32713, Loss_G: 2.57119, Loss_KL: 0.01702\n",
      "[314/600] Loss_D: 0.56029, Loss_G: 2.61436, Loss_KL: 0.01625\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[315/600] [0/331] Loss_D: 0.66586, Loss_G: 2.60133, Loss_KL: 0.02735\n",
      "[315/600] [100/331] Loss_D: 0.71199, Loss_G: 2.46645, Loss_KL: 0.02257\n",
      "[315/600] [200/331] Loss_D: 0.47492, Loss_G: 2.82908, Loss_KL: 0.00796\n",
      "[315/600] [300/331] Loss_D: 0.23317, Loss_G: 2.96611, Loss_KL: 0.00964\n",
      "[315/600] Loss_D: 0.53702, Loss_G: 2.66884, Loss_KL: 0.01558\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[316/600] [0/331] Loss_D: 0.66129, Loss_G: 2.58873, Loss_KL: 0.02994\n",
      "[316/600] [100/331] Loss_D: 0.30548, Loss_G: 2.81761, Loss_KL: 0.00842\n",
      "[316/600] [200/331] Loss_D: 0.59588, Loss_G: 2.42063, Loss_KL: 0.02529\n",
      "[316/600] [300/331] Loss_D: 0.72764, Loss_G: 2.50574, Loss_KL: 0.01077\n",
      "[316/600] Loss_D: 0.55561, Loss_G: 2.59187, Loss_KL: 0.01501\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[317/600] [0/331] Loss_D: 0.68624, Loss_G: 2.72422, Loss_KL: 0.01660\n",
      "[317/600] [100/331] Loss_D: 0.42351, Loss_G: 2.69089, Loss_KL: 0.01486\n",
      "[317/600] [200/331] Loss_D: 0.73476, Loss_G: 2.54076, Loss_KL: 0.00916\n",
      "[317/600] [300/331] Loss_D: 0.44340, Loss_G: 2.67960, Loss_KL: 0.01741\n",
      "[317/600] Loss_D: 0.54139, Loss_G: 2.66144, Loss_KL: 0.01566\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[318/600] [0/331] Loss_D: 0.68273, Loss_G: 2.33977, Loss_KL: 0.02213\n",
      "[318/600] [100/331] Loss_D: 0.32836, Loss_G: 2.86914, Loss_KL: 0.01757\n",
      "[318/600] [200/331] Loss_D: 0.63932, Loss_G: 2.80907, Loss_KL: 0.01083\n",
      "[318/600] [300/331] Loss_D: 0.13020, Loss_G: 2.61679, Loss_KL: 0.01774\n",
      "[318/600] Loss_D: 0.54898, Loss_G: 2.63664, Loss_KL: 0.01578\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[319/600] [0/331] Loss_D: 0.48041, Loss_G: 2.46894, Loss_KL: 0.01415\n",
      "[319/600] [100/331] Loss_D: 0.61226, Loss_G: 2.58932, Loss_KL: 0.00984\n",
      "[319/600] [200/331] Loss_D: 0.38677, Loss_G: 2.48445, Loss_KL: 0.01183\n",
      "[319/600] [300/331] Loss_D: 0.70605, Loss_G: 2.40771, Loss_KL: 0.01402\n",
      "[319/600] Loss_D: 0.55208, Loss_G: 2.60282, Loss_KL: 0.01502\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[320/600] [0/331] Loss_D: 0.75662, Loss_G: 2.40341, Loss_KL: 0.02293\n",
      "[320/600] [100/331] Loss_D: 0.63786, Loss_G: 2.64288, Loss_KL: 0.00859\n",
      "[320/600] [200/331] Loss_D: 0.69082, Loss_G: 2.41543, Loss_KL: 0.01465\n",
      "[320/600] [300/331] Loss_D: 0.43075, Loss_G: 2.56237, Loss_KL: 0.01089\n",
      "[320/600] Loss_D: 0.56072, Loss_G: 2.58885, Loss_KL: 0.01505\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Save G1/D1 models\n",
      "[321/600] [0/331] Loss_D: 0.73396, Loss_G: 2.50818, Loss_KL: 0.01315\n",
      "[321/600] [100/331] Loss_D: 0.69344, Loss_G: 2.58550, Loss_KL: 0.00850\n",
      "[321/600] [200/331] Loss_D: 0.66147, Loss_G: 2.66767, Loss_KL: 0.02059\n",
      "[321/600] [300/331] Loss_D: 0.30695, Loss_G: 2.56079, Loss_KL: 0.02546\n",
      "[321/600] Loss_D: 0.56452, Loss_G: 2.59255, Loss_KL: 0.01558\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[322/600] [0/331] Loss_D: 0.51564, Loss_G: 2.55956, Loss_KL: 0.01359\n",
      "[322/600] [100/331] Loss_D: 0.22120, Loss_G: 2.78422, Loss_KL: 0.03570\n",
      "[322/600] [200/331] Loss_D: 0.62231, Loss_G: 2.50042, Loss_KL: 0.00863\n",
      "[322/600] [300/331] Loss_D: 0.59861, Loss_G: 2.56315, Loss_KL: 0.01141\n",
      "[322/600] Loss_D: 0.56543, Loss_G: 2.59317, Loss_KL: 0.01512\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[323/600] [0/331] Loss_D: 0.24228, Loss_G: 2.46193, Loss_KL: 0.00882\n",
      "[323/600] [100/331] Loss_D: 0.21654, Loss_G: 2.67656, Loss_KL: 0.01148\n",
      "[323/600] [200/331] Loss_D: 0.57107, Loss_G: 2.38982, Loss_KL: 0.01112\n",
      "[323/600] [300/331] Loss_D: 0.33966, Loss_G: 2.71590, Loss_KL: 0.01425\n",
      "[323/600] Loss_D: 0.54558, Loss_G: 2.67209, Loss_KL: 0.01545\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[324/600] [0/331] Loss_D: 0.59425, Loss_G: 2.64067, Loss_KL: 0.02439\n",
      "[324/600] [100/331] Loss_D: 0.63150, Loss_G: 2.72092, Loss_KL: 0.01700\n",
      "[324/600] [200/331] Loss_D: 0.27330, Loss_G: 2.58738, Loss_KL: 0.01262\n",
      "[324/600] [300/331] Loss_D: 0.31440, Loss_G: 2.57424, Loss_KL: 0.02097\n",
      "[324/600] Loss_D: 0.54975, Loss_G: 2.62465, Loss_KL: 0.01586\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[325/600] [0/331] Loss_D: 0.65247, Loss_G: 2.82655, Loss_KL: 0.01137\n",
      "[325/600] [100/331] Loss_D: 0.25756, Loss_G: 2.86946, Loss_KL: 0.01188\n",
      "[325/600] [200/331] Loss_D: 0.73516, Loss_G: 2.78349, Loss_KL: 0.01315\n",
      "[325/600] [300/331] Loss_D: 0.70086, Loss_G: 2.52208, Loss_KL: 0.01038\n",
      "[325/600] Loss_D: 0.55403, Loss_G: 2.62952, Loss_KL: 0.01575\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[326/600] [0/331] Loss_D: 0.72158, Loss_G: 2.70102, Loss_KL: 0.01677\n",
      "[326/600] [100/331] Loss_D: 0.51250, Loss_G: 2.50551, Loss_KL: 0.01621\n",
      "[326/600] [200/331] Loss_D: 0.47536, Loss_G: 2.54295, Loss_KL: 0.00682\n",
      "[326/600] [300/331] Loss_D: 0.33396, Loss_G: 2.63069, Loss_KL: 0.02088\n",
      "[326/600] Loss_D: 0.53330, Loss_G: 2.66494, Loss_KL: 0.01477\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[327/600] [0/331] Loss_D: 0.70699, Loss_G: 2.77857, Loss_KL: 0.01347\n",
      "[327/600] [100/331] Loss_D: 0.33719, Loss_G: 2.50109, Loss_KL: 0.02642\n",
      "[327/600] [200/331] Loss_D: 0.18225, Loss_G: 2.84117, Loss_KL: 0.00793\n",
      "[327/600] [300/331] Loss_D: 0.30060, Loss_G: 2.85757, Loss_KL: 0.02024\n",
      "[327/600] Loss_D: 0.54831, Loss_G: 2.63171, Loss_KL: 0.01560\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[328/600] [0/331] Loss_D: 0.66762, Loss_G: 2.50742, Loss_KL: 0.00740\n",
      "[328/600] [100/331] Loss_D: 0.27747, Loss_G: 2.38439, Loss_KL: 0.01840\n",
      "[328/600] [200/331] Loss_D: 0.35750, Loss_G: 2.50904, Loss_KL: 0.01868\n",
      "[328/600] [300/331] Loss_D: 0.31789, Loss_G: 2.56126, Loss_KL: 0.01367\n",
      "[328/600] Loss_D: 0.56402, Loss_G: 2.58003, Loss_KL: 0.01388\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[329/600] [0/331] Loss_D: 0.68326, Loss_G: 2.45857, Loss_KL: 0.03221\n",
      "[329/600] [100/331] Loss_D: 0.61663, Loss_G: 2.66943, Loss_KL: 0.00707\n",
      "[329/600] [200/331] Loss_D: 0.30560, Loss_G: 2.51995, Loss_KL: 0.02852\n",
      "[329/600] [300/331] Loss_D: 0.29916, Loss_G: 2.69981, Loss_KL: 0.01053\n",
      "[329/600] Loss_D: 0.54717, Loss_G: 2.61768, Loss_KL: 0.01472\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[330/600] [0/331] Loss_D: 0.74016, Loss_G: 2.65730, Loss_KL: 0.01426\n",
      "[330/600] [100/331] Loss_D: 0.38668, Loss_G: 2.47778, Loss_KL: 0.01736\n",
      "[330/600] [200/331] Loss_D: 0.68067, Loss_G: 2.34638, Loss_KL: 0.01103\n",
      "[330/600] [300/331] Loss_D: 0.34913, Loss_G: 2.74270, Loss_KL: 0.02042\n",
      "[330/600] Loss_D: 0.54884, Loss_G: 2.60837, Loss_KL: 0.01452\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[331/600] [0/331] Loss_D: 0.74073, Loss_G: 2.50000, Loss_KL: 0.01831\n",
      "[331/600] [100/331] Loss_D: 0.60863, Loss_G: 2.81632, Loss_KL: 0.02356\n",
      "[331/600] [200/331] Loss_D: 0.68551, Loss_G: 2.51501, Loss_KL: 0.00839\n",
      "[331/600] [300/331] Loss_D: 0.57128, Loss_G: 2.62304, Loss_KL: 0.01013\n",
      "[331/600] Loss_D: 0.54916, Loss_G: 2.65144, Loss_KL: 0.01530\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[332/600] [0/331] Loss_D: 0.71034, Loss_G: 2.53900, Loss_KL: 0.01944\n",
      "[332/600] [100/331] Loss_D: 0.68963, Loss_G: 2.52237, Loss_KL: 0.01224\n",
      "[332/600] [200/331] Loss_D: 0.41138, Loss_G: 2.48944, Loss_KL: 0.01007\n",
      "[332/600] [300/331] Loss_D: 0.66245, Loss_G: 2.72047, Loss_KL: 0.01768\n",
      "[332/600] Loss_D: 0.54772, Loss_G: 2.60862, Loss_KL: 0.01426\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[333/600] [0/331] Loss_D: 0.63845, Loss_G: 2.80213, Loss_KL: 0.00598\n",
      "[333/600] [100/331] Loss_D: 0.67111, Loss_G: 2.69537, Loss_KL: 0.02276\n",
      "[333/600] [200/331] Loss_D: 0.41769, Loss_G: 2.69352, Loss_KL: 0.02446\n",
      "[333/600] [300/331] Loss_D: 0.72276, Loss_G: 2.61722, Loss_KL: 0.00619\n",
      "[333/600] Loss_D: 0.53415, Loss_G: 2.64004, Loss_KL: 0.01436\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[334/600] [0/331] Loss_D: 0.73936, Loss_G: 2.95546, Loss_KL: 0.02531\n",
      "[334/600] [100/331] Loss_D: 0.71297, Loss_G: 2.53340, Loss_KL: 0.01507\n",
      "[334/600] [200/331] Loss_D: 0.59738, Loss_G: 2.72254, Loss_KL: 0.01383\n",
      "[334/600] [300/331] Loss_D: 0.49074, Loss_G: 2.98814, Loss_KL: 0.01758\n",
      "[334/600] Loss_D: 0.54624, Loss_G: 2.65558, Loss_KL: 0.01408\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[335/600] [0/331] Loss_D: 0.32668, Loss_G: 2.49613, Loss_KL: 0.02483\n",
      "[335/600] [100/331] Loss_D: 0.38596, Loss_G: 2.75740, Loss_KL: 0.00961\n",
      "[335/600] [200/331] Loss_D: 0.63118, Loss_G: 2.85385, Loss_KL: 0.02947\n",
      "[335/600] [300/331] Loss_D: 0.58463, Loss_G: 2.40819, Loss_KL: 0.00782\n",
      "[335/600] Loss_D: 0.56204, Loss_G: 2.56370, Loss_KL: 0.01320\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[336/600] [0/331] Loss_D: 0.40780, Loss_G: 2.57798, Loss_KL: 0.01543\n",
      "[336/600] [100/331] Loss_D: 0.71200, Loss_G: 2.48281, Loss_KL: 0.01768\n",
      "[336/600] [200/331] Loss_D: 0.53313, Loss_G: 2.55388, Loss_KL: 0.02035\n",
      "[336/600] [300/331] Loss_D: 0.65459, Loss_G: 2.56507, Loss_KL: 0.01663\n",
      "[336/600] Loss_D: 0.55388, Loss_G: 2.59613, Loss_KL: 0.01371\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "Adjusting learning rate of group 0 to 2.5000e-05.\n",
      "[337/600] [0/331] Loss_D: 0.74398, Loss_G: 2.75116, Loss_KL: 0.01436\n",
      "[337/600] [100/331] Loss_D: 0.73498, Loss_G: 2.60778, Loss_KL: 0.00382\n",
      "[337/600] [200/331] Loss_D: 0.47779, Loss_G: 2.55596, Loss_KL: 0.00979\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-bf4fd3192232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4dddc83b21fe>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(stage, batch_size, trainloader)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# Update generator network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             errG_fake = compute_generator_loss(netD, criterion, fake_images, real_images, \n\u001b[0m\u001b[1;32m    139\u001b[0m                                                real_labels, text_embeddings, gpus)\n\u001b[1;32m    140\u001b[0m             \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/common/users/ppk31/CS543_DL_Proj/utils.py\u001b[0m in \u001b[0;36mcompute_generator_loss\u001b[0;34m(netD, criterion, fake_imgs, real_images, real_labels, conditions, gpus)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconditions\u001b[0m \u001b[0;31m# conditions.detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mfake_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfake_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# fake pairs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mdata_parallel\u001b[0;34m(module, inputs, device_ids, output_device, dim, module_kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0mused_device_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mused_device_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py\u001b[0m in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0m_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_tup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/koko/system/anaconda/envs/python39/lib/python3.9/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(stage, batch_size, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "             Layer (type)                       Output Shape         Param #     Tr. Param #\n",
      "=============================================================================================\n",
      "   Augmented_Projection-1     [1, 24576], [1, 128], [1, 128]       5,914,880       5,914,880\n",
      "               Upsample-2                     [1, 768, 8, 8]      10,619,136      10,619,136\n",
      "               Upsample-3                   [1, 384, 16, 16]       2,655,360       2,655,360\n",
      "               Upsample-4                   [1, 192, 32, 32]         664,128         664,128\n",
      "               Upsample-5                    [1, 96, 64, 64]         166,176         166,176\n",
      "                 Conv2d-6                     [1, 3, 64, 64]           2,595           2,595\n",
      "                   Tanh-7                     [1, 3, 64, 64]               0               0\n",
      "=============================================================================================\n",
      "Total params: 20,022,275\n",
      "Trainable params: 20,022,275\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = Generator1(stage=1)\n",
    "embed = torch.zeros((1,1024))\n",
    "z = torch.zeros((1,100))\n",
    "print(summary(g, embed, z, show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized stage2 Generator\n",
      "-------------------------------------------------------------------------------------------\n",
      "             Layer (type)                     Output Shape         Param #     Tr. Param #\n",
      "===========================================================================================\n",
      "             Downsample-1                 [1, 192, 64, 64]           5,376           5,376\n",
      "             Downsample-2                 [1, 384, 32, 32]       1,180,800       1,180,800\n",
      "             Downsample-3                 [1, 768, 16, 16]       4,720,896       4,720,896\n",
      "   Augmented_Projection-4     [1, 128], [1, 128], [1, 128]         262,400         262,400\n",
      "             Downsample-5                 [1, 768, 16, 16]       6,195,456       6,195,456\n",
      "               ResBlock-6                 [1, 768, 16, 16]      10,621,440      10,621,440\n",
      "               ResBlock-7                 [1, 768, 16, 16]      10,621,440      10,621,440\n",
      "               ResBlock-8                 [1, 768, 16, 16]      10,621,440      10,621,440\n",
      "               ResBlock-9                 [1, 768, 16, 16]      10,621,440      10,621,440\n",
      "              Upsample-10                 [1, 384, 32, 32]       2,655,360       2,655,360\n",
      "              Upsample-11                 [1, 192, 64, 64]         664,128         664,128\n",
      "              Upsample-12                [1, 96, 128, 128]         166,176         166,176\n",
      "              Upsample-13                [1, 48, 256, 256]          41,616          41,616\n",
      "                Conv2d-14                 [1, 3, 256, 256]           1,299           1,299\n",
      "                  Tanh-15                 [1, 3, 256, 256]               0               0\n",
      "===========================================================================================\n",
      "Total params: 58,379,267\n",
      "Trainable params: 58,379,267\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "g = Generator2(stage=2)\n",
    "embed = torch.zeros((1,1024))\n",
    "s1out = torch.zeros((1,3,64,64))\n",
    "print(summary(g, embed, s1out, show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized, stage 1 discriminator\n",
      "------------------------------------------------------------------------\n",
      "      Layer (type)         Output Shape         Param #     Tr. Param #\n",
      "========================================================================\n",
      "      Downsample-1      [1, 64, 32, 32]           3,136           3,136\n",
      "      Downsample-2     [1, 128, 16, 16]         131,456         131,456\n",
      "      Downsample-3       [1, 256, 8, 8]         525,056         525,056\n",
      "      Downsample-4       [1, 512, 4, 4]       2,098,688       2,098,688\n",
      "========================================================================\n",
      "Total params: 2,758,336\n",
      "Trainable params: 2,758,336\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "d1 = Discriminator(stage=1)\n",
    "embed = torch.zeros((1,1024))\n",
    "s1out = torch.zeros((1,3,64,64))\n",
    "print(summary(d1, s1out, show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1            [1, 128]         131,200         131,200\n",
      "            ReLU-2            [1, 128]               0               0\n",
      "      Downsample-3      [1, 512, 4, 4]         328,704         328,704\n",
      "      Downsample-4        [1, 1, 1, 1]           8,193           8,193\n",
      "         Sigmoid-5        [1, 1, 1, 1]               0               0\n",
      "=======================================================================\n",
      "Total params: 468,097\n",
      "Trainable params: 468,097\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "feat =  torch.zeros((1,512,4,4))\n",
    "print(summary(d1.cond_discriminator_logits, feat, embed, show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_in = torch.zeros((1,384,16,16))\n",
    "_out = nn.ConvTranspose2d(384, 384//2, kernel_size=4, stride=2, padding=1, bias=False)(_in)\n",
    "_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator2(\n",
       "  (aug_project): Augmented_Projection(\n",
       "    (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (downblocks): Sequential(\n",
       "    (0): Downsample(\n",
       "      (op): Conv2d(3, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (activtn): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Downsample(\n",
       "      (op): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): Downsample(\n",
       "      (op): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (combined): Sequential(\n",
       "    (0): Downsample(\n",
       "      (op): Conv2d(896, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): ResBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (x_residual): Identity()\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): ResBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (x_residual): Identity()\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): ResBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (x_residual): Identity()\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): ResBlock(\n",
       "      (conv1): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (conv2): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (x_residual): Identity()\n",
       "      (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Upsample(\n",
       "      (conv): Conv2d(768, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): ReLU()\n",
       "    )\n",
       "    (5): Upsample(\n",
       "      (conv): Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): ReLU()\n",
       "    )\n",
       "    (6): Upsample(\n",
       "      (conv): Conv2d(192, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): ReLU()\n",
       "    )\n",
       "    (7): Upsample(\n",
       "      (conv): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batchnorm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activtn): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Conv2d(48, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
